%Para este capítulo se usará la abreviatura "dual".
\chapter{Dualidad}
\label{dual}
En este capítulo introduciremos los llamados ``problemas duales'', estudiando sus propiedades y relaciones con sus respectivos ``problemas primales''. Sacaremos jugo a estas propiedades cual Jíbaro a la cabeza de un enemigo.
\section{Formulación canónica del problema dual}
Sea un problema $P$ de programación lineal en forma canónica de maximización. Sintéticamente
\begin{equation*}
	\begin{array}{c}
		\max c^tx\\
		\text{Sujeto a:}\qquad Ax\leq b,\qquad x\geq 0
	\end{array}
\end{equation*}
se define el \tbi[problema!dual]{problema dual} $D$ asociado a $P$ como
\begin{equation*}
	\begin{array}{c}
		\min b^tu\\
		\text{Sujeto a:}\qquad A^tu\geq c,\qquad u\geq 0
	\end{array}
\end{equation*}
usualmente nos referiremos al problema $P$ como el \tbi[problema!primal]{problema primal}.
\begin{obs}[Involutividad]
	La primera cosa que salta a la vista es que la dualidad es involutiva, es decir, el problema dual asociado al problema dual es el problema primal (esto lo veremos en el ejemplo \ref{dual_exa_min}), que es de vital importancia.
\end{obs}
Aunque esta definición de problema dual parezca estricta, por solo poder aplicarse a los problemas en forma canónica de maximización, en realidad no lo es tanto, ya que podemos pasar de una forma canónica a otra. Veamos, por ejemplo, cuál es el problema dual asociado a un problema en forma canónica de minimización.
\begin{exa}[Minimización]
	\label{dual_exa_min}
	Teniendo en cuenta que $\min f = -\max -f$ (se deja al lector la comprobación) tenemos que, dado el problema en forma canónica de minimización
	\begin{equation*}
		\begin{array}{c}
			\min c^tx\\
			\text{Sujeto a:}\qquad Ax\geq b,\qquad x\geq 0
		\end{array}
	\end{equation*}
	su transformación a forma canónica de maximización es
	\begin{equation*}
		\begin{array}{c}
			-\max -c^tx\\
			\text{Sujeto a:}\qquad -Ax\leq -b,\qquad x\geq 0
		\end{array}
	\end{equation*}
	donde lo único que se ha hecho es multiplicar por $-1$ todas las restricciones así como la función objetivo. En esta situación el problema dual ya está bien definido, y es
	\begin{equation*}
		\begin{array}{c}
			-\min -b^tu\\
			\text{Sujeto a:}\qquad -A^tu\geq -c,\qquad u\geq 0
		\end{array}
	\end{equation*}
	planteando esto de nuevo en forma canónica de maximización obtenemos
	\begin{equation*}
		\begin{array}{c}
			\max b^tu\\
			\text{Sujeto a:}\qquad A^tu\leq c,\qquad u\geq 0
		\end{array}
	\end{equation*}
	con lo que ya tenemos el problema dual asociado al problema original.
\end{exa}
Nótese que no está muy claro cómo definir el problema dual asociado a un problema en forma estándar (mucho menos en otras formas más exóticas). A esta cuestión nos decicaremos en secciones posteriores, justo después de ver por qué merece la pena plantearse estos problemas.
\section{Relaciones de dualidad}
En esta sección veremos cómo se relacionan un problema y su dual en términos de sus soluciones, viendo así parte de la utilidad de estudiar la dualidad. Comencemos con el siguiente resultado elemental.
\begin{lem}[Cotas]
	\label{dual_lem_cotas}
	Toda solución factible del problema dual proporciona una cota superior del valor óptimo de la función objetivo del problema primal, y viceversa. Es decir
	\begin{equation*}
		c^tx\leq b^tu
	\end{equation*}
\end{lem}
\begin{proof}
	Sea $x\in\R^n$ una solución factible del problema primal. Asimismo consideremos $u\in\R^m$ una solución factible del problema dual.
	
	Tenemos que $c^tx=\sum_{j=1}^nc_jx_j$. Teniendo en cuenta la definición del problema dual ($A^tu\geq c$) tenemos que $c^tx\leq \sum_{j=1}^n(\sum_{i=1}^{m}a_{ij}u_i)x_j$. Intercambiando los sumatorios obtenemos la expresión $c^tx\leq\sum_{i=1}^{m}(\sum_{j=1}^{n}a_{ij}x_j)u_i$. Teniendo en cuenta la definición del problema primal $(Ax\leq b)$ concluimos que $c^tx\leq \sum_{i=1}^{m}bu_i=b^tu$. Como queríamos demostrar.
\end{proof}
\begin{cor}[Dualidad débil]
	\label{dual_cor_dualidadDebil}
	Si dos soluciones factibles $x$ y $u$, del problema primal y dual respectivamente, verifican que $c^tx=b^tu$, entonces, tanto $x$ como $u$ son soluciones óptimas de sus respectivos problemas.
\end{cor}
El siguiente resultado es conocido en la literatura con el nombre de ``teorema de dualidad'' o ``teorema de dualidad fuerte''. Es el recíproco del corolario \ref{dual_cor_dualidadDebil}. Veámoslo.
\begin{theo}[Dualidad fuerte]
	\label{dual_teo_dualidadFuerte}
	Si el problema primal tiene solución óptima $x$, entonces el problema dual tiene solución óptima $u$, y se verifica que $c^tx=b^tu$.
\end{theo}
\begin{proof}
	Podemos suponer si pérdida de generalidad que la solución óptima $x$ se ha obtenido mediante el algoritmo del Símplex. En tal, caso llamaremos $B$ a su base asociada. Asimismo consideremos el vector fila $\lambda^t:=c_B^tB^{-1}$.
	
	Como $x$ es solución óptima de un problema de maximización encontrada mediante el algoritmo del Símplex se deberá verificar que todos los costes reducidos son no positivos, es decir, $\overline{c_j}\leq 0$.
	
	Nótese que para que se pueda ejecutar el algoritmo del Símplex sobre el problema primal se deberán añadir $m$ variables de holgura (una por restricción), de modo que la matriz del problema pasaría a ser $(A|I_m)$.
	
	Estudiemos con un poco de cariño los costes reducidos.
	\begin{equation*}
		\overline{c_j}=c_j-c_B^tB^{-1}a_j=c_j-\lambda^ta_j\leq 0\sii \lambda^ta_j\geq c_j\sii \sum_{i=1}^{m}a_{ij}\lambda_i\geq c_j
	\end{equation*}
	La última equivalencia nos dice que el vector $\lambda\in\R^m$ cumple con las restricciones del problema dual. Para asegurarse de que es solución factible de este basta con comprobar la no negatividad, para ello consideramos los costes reducidos asociados a las variables de holgura ($\overline{c_j}$ con $j\in\{n+1,\dots,n+m\}$), que se caracterizan por el hecho de que $c_{n+i}=0$ y $a_{n+i}=e_i$ con $i\in\{1,\dots,m\}$.
	\begin{equation*}
		\overline{c_{n+i}}=-\lambda^te_i=-\lambda_i\leq 0\sii\lambda_i\geq 0 
	\end{equation*}
	con lo que se concluye que $\lambda$ es solución factible dual. Además
	\begin{equation*}
		c^tx=\sum_{i=1}^{n}c_ix_i=c_B^t\overline{x_B}=c_B^tB^{-1}b=\lambda^tb=b^t\lambda
	\end{equation*}
	por el coralario \ref{dual_cor_dualidadDebil} tenemos que $\lambda$ es solución óptima del problema dual.
\end{proof}
\begin{obs}[Construcción]
	Nótese que a partir de la solución óptima del primal obtenida por el algoritmo del Símplex obtenemos automáticamente la solución óptima del dual, en concreto, si la solución óptima del primal está asociada a la base $B$, la solución óptima del dual es $(c_B^tB^{-1})^t=-(\overline{c_{n+1}},\dots,\overline{c_{n+m}})^t$. Luego podemos decir que la solución del problema dual está literalmente en la tabla del Símplex del primal.
\end{obs}
Antes de continuar es necesario hacer un pequeño inciso.
\begin{obs}[Teorema dual]
	\label{dual_obs_dual}
	Existe un ``teorema dual'' al teorema de dualidad fuerte \ref{dual_teo_dualidadFuerte}, cuyo enunciado es el mismo, pero permutando las palabras ``primal'' y ``dual''. La demostración es totalmente análoga y se deja al lector.
	
	Esto viene a significar que si alguno de los dos problemas tiene solución óptima, el otro también la tendrá.
\end{obs}

Otro resultado bastante fuerte que relaciona las soluciones de los problemas primal y dual es el llamado ``teorema de la holgura complementaria'', que da condiciones necesarias y suficientes para que dos soluciones (una del primal y otra del dual) sean óptimas simultáneamente.
\begin{theo}[Teorema de la holgura complementaria]
	\label{dual_teo_holgura}
	Sean $x$ e $u$ soluciones factibles de primal y dual respectivamente. Se verifica que $x$ e $u$ son soluciones óptimas de sus respectivos problemas si y solo si se cumplen
	\begin{enumerate}
		\item Para cada restricción del problema primal (suponemos la $i$--ésima con $i\in\{1,\dots,m\}$) se verifica que $\sum_{j=1}^{n}a_{ij}x_j=b_i$ o bien $u_i=0$.
		\item Para cada restricción del problema dual (suponemos la $j$--ésima con $j\in\{1,\dots,n\}$) se verifica que $\sum_{i=1}^{m}a_{ij}u_i=c_j$ o bien $x_j=0$.
	\end{enumerate}
\end{theo}
\begin{proof}
	Como por el lema \ref{dual_lem_cotas} sabemos que $c^tx\leq b^tu$, lo cual implica que tanto el problema primal como el problema dual tienen solución óptima (por no ser ni infactibles ni no acotados, véase teorema \ref{fund_teo_fund}).
	
	Dicho esto, el teorema \ref{dual_teo_dualidadFuerte} combinado con el corolario \ref{dual_cor_dualidadDebil} nos asegura que ser soluciones óptimas simultáneas es equivalente a que se verifique la igualdad $c^tx=b^tu$. Busquemos condiciones necesarias y suficientes para que esto se cumpla.
	
	Podemos resumir el contenido fundamental del lema \ref{dual_lem_cotas} en la siguiente cadena de desigualdades.
	\begin{equation*}
		c^tx=\sum_{j=1}^{n}c_jx_j\leq \sum_{j=1}^{n}\left(\sum_{i=1}^{m}a_{ij}u_i\right)x_j=\sum_{i=1}^{m}\left(\sum_{j=1}^{n}a_{ij}x_j\right)u_i\leq \sum_{i=1}^{m}b_iu_i=b^tu
	\end{equation*}
	Una condición suficiente para que se de la igualdad en la primera desigualdad de la cadena es que $c_jx_j=(\sum_{i=1}^{m}a_{ij}u_i)x_j$ para $j\in\{1,\dots,n\}$, lo cual se da si y solo si $c_j=\sum_{i=1}^{m}a_{ij}u_i$ o bien $x_j=0$. Encontramos una condición suciente análoga para la segunda desigualdad de la cadena.
	
	La condición que hemos impuesto también es necesaria, ya que $c_j\leq\sum_{i=1}^{m}a_{ij}u_i$ y $x_j\geq 0$ para todo $j\in\{1,\dots,n\}$, de este modo es fácil ver que si se diera la desigualdad estricta para un $j$ en el cual $x_j>0$ se produciría una desigualdad estricta también en el sumatorio. Análogamente se hace con la otra desigualdad.
\end{proof}
Aunque este último teorema pueda parece más inútil que un cubo de tela, no es así, pues, como vemos a continuación, sirve para comprobar si una solución (no necesariamente punto extremo) es óptima o no sin necesidad de aplicar el algoritmo del Símplex. Veamos este resultado en forma de una observación y un ejemplo.
\begin{obs}[Test de optimalidad]
	Una solución factible $x$ del problema primal es óptima si y solo si existe un $u\in\R^m$ que verifica las siguientes condiciones
	\begin{enumerate}
		\item Si $\sum_{j=1}^{n}a_{ij}x_j<b_i$ entonces $u_i=0$ y si $x_j>0$ entonces $\sum_{i=1}^{m}a_{ij}u_i=c_j$.
		\item Se verifica que $\sum_{i=1}^{m}a_{ij}u_i\geq c_j$ y $u_i\geq 0$ para todo $j\in\{1,\dots,m\}$.
	\end{enumerate}
	La corrección de este ``test'' se deduce trivialmente de los teoremas \ref{dual_teo_dualidadFuerte} y \ref{dual_teo_holgura} junto con el corolario \ref{dual_cor_dualidadDebil}. Se recomienda encarecidamente al lector poner a prueba esta observación en la práctica.
\end{obs}
Para finalizar la sección presentemos una tabla resumen que recopila casi todo lo aprendido.
\begin{table}[H]
	\centering
	\begin{tabular}{c|c|c|c|}
		\multicolumn{1}{l|}{} & Infactible & Sol. óptima & Sol. no acotada \\ \hline
		Infactible & \textbf{Posible} & \cellcolor[HTML]{C0C0C0}{\color[HTML]{FFCCC9} \textbf{}} & \textbf{Posible} \\ \hline
		Sol. óptima & \cellcolor[HTML]{C0C0C0}\textbf{} & \textbf{Posible} & \cellcolor[HTML]{C0C0C0}\textbf{} \\ \hline
		Sol. no acotada & \textbf{Posible} & \cellcolor[HTML]{C0C0C0}\textbf{} & \cellcolor[HTML]{C0C0C0}\textbf{} \\ \hline
	\end{tabular}
	\caption{Relaciones de dualidad.}
	\label{dual_tab_tabla}
\end{table}
Como se puede observar, la tabla es totalmente simétrica, motivo por el cual no se ha especificado a en ningún letrero si nos referimos al problema dual o al problema primal.

La demostración de que la tabla es verídica es muy sencilla, siendo la demostración de la veracidad de la fila central el contenido de la observación \ref{dual_obs_dual}. Por su parte, la demostración correspondiente a la última fila se basa en el lema \ref{dual_lem_cotas}. Es muy sencilla y se deja al lector. En cuanto a la primera fila, se pueden encontrar ejemplos de los dos casos posibles, mientras que el caso otro caso es imposible por la observación \ref{dual_obs_dual}.
\section{Otras formulaciones}
Vamos a extender la noción ``problema dual asociado'' a problemas que no necesariamente están presentados en forma canónica.
\subsection{Formulación estándar}
Dado un problema de programación lineal en forma estándar
\begin{equation*}
\begin{array}{c}
\min c^tx\\
\text{Sujeto a:}\qquad Ax= b,\qquad x\geq 0
\end{array}
\end{equation*}
vamos a ponerlo en forma canónica de minimización usando el siguiente truco. Consideramos el problema equivalente
\begin{equation*}
\begin{array}{c}
\min c^tx\\
\text{Sujeto a:}\qquad Ax\leq b\quad \&\quad Ax\geq b,\qquad x\geq 0
\end{array}
\end{equation*}
que a su vez transformamos en
\begin{equation*}
\begin{array}{c}
\min c^tx\\
\text{Sujeto a:}\qquad -Ax\geq -b\quad \&\quad Ax\geq b,\qquad x\geq 0
\end{array}
\end{equation*}
que ya está en forma canónica, por simplificar un poco las cosas explicitamos que la matriz de este problema es $(A^t|-A^t)^t$ y su vector de términos independientes $(b^t|-b^t)^t$. De esta forma ya podemos calcular el problema dual con la definición usual. Este es
\begin{equation*}
\begin{array}{c}
\max (b^t|-b^t)^tu\\
\text{Sujeto a:}\qquad(A^t|-A^t)u\leq c\qquad u\geq 0
\end{array}
\end{equation*}
Si llamamos $u_1$ a las $m$ componentes de $u$ asociadas a la submatriz $A^t$, mientras llamamos $u_2$ a las componentes asociadas a la submatriz restante tenemos que el problema dual tiene por restricción $A^tu_1-A^tu_2\leq c$, que es equivalente a la restricción $A^t(u_1-u_2)\leq c$. Algo similar se hace con la función objetivo.

Llamando $w:=u_1-u_2$ tenemos que el problema dual es
\begin{equation*}
\begin{array}{c}
\max b^tw\\
\text{Sujeto a:}\qquad A^tw\leq c
\end{array}
\end{equation*}
Nótese que como $u_1,u_2\geq 0$, $w$ no tiene ninguna restricción en cuanto al signo.

Como la involutividad se sigue cumpliendo, ya tenemos una forma de ``dualizar'' problemas de maximización con variables no restringidas. Si uno no quiere gastar memoria en esto, basta con reproducir el procedimiento realizado y leerlo de abajo a arriba.

Un buen ejercicio consistiría en repetir el apartado entero cambiando el problema inicial a dualizar por un problema en forma estándar pero de maximización.
\subsection{Formulación general}
En este apartado vamos a ver cómo dualizar cualquier problema de programación lineal (independientemente de su formulación), para lo cual seguiremos una estrategia totalmente análoga a la del apartado anterior.

Uno de los problemas más generales que se nos puede plantear es el que tiene restricciones de todo tipo, es decir
 \begin{equation*}
 \begin{array}{c}
 \max c^tx\\
 \text{Sujeto a:}\qquad A_1x\leq b\quad \&\quad A_2x=b\quad\&\quad A_3x\geq b,\qquad x\geq 0
 \end{array}
 \end{equation*}
 Si lo transformamos en un problema en forma canónica obtenemos (¡compruébese!)
 \begin{equation*}
 \begin{array}{c}
 \max c^tx\\
 \text{Sujeto a:}\qquad (A_1^t|A_2^t|-A_2^t|-A_3^t)^tx\leq (b_1^t|b_2^t|b_3^t|b_4^t)^t\qquad x\geq 0
 \end{array}
 \end{equation*}
 por tanto su dual será (háganse las cuentas)
  \begin{equation*}
  \begin{array}{c}
  \min (b_1^t|b_2^t|b_3^t|b_4^t)(u_1^t|u_2^t|u_3^t|u_4^t)^t\\
  \text{Sujeto a:}\qquad (A_1^t|A_2^t|-A_2^t|-A_3^t)(u_1^t|u_2^t|u_3^t|u_4^t)^t\geq c\qquad u\geq 0
  \end{array}
  \end{equation*}
  Por ende la restricción del problema dual es equivalente a $A_1^tu_1+A_2^t(u_2-u_3)+A_3^t(-u_4)\geq c$. Llamando $w_1:=u_2-u_3$ y $w_2:=-u_4$ obtenemos que el problema dual tiene a $A_1^tu_1+A_2^tw_1+A_3^tw_2$ por restricción, donde $w_1$ son variables no restrigidas y $w_2$ son variables no positivas. Algo similar se hace con la función objetivo.
  
  Podemos realizar el mismo proceso con un problema de minimización, y en general con cualquier problema, ya que las anomalías de tener variables no restringidas o no positivas se trataron en el capítulo \ref{fund}, de modo que podemos transformar cualquier problema en uno del tipo tratado en este apartado o su análogo de minimización.
\section{Algoritmo dual}
Resolver problemas de programación lineal en forma canónica de minimización con vector de términos independientes $b\geq 0$ utilizando el algoritmo de Símplex provoca un desperdicio de memoria que da pena verlo.

Esto es debido a que al realizar la transformación del problema $Ax\geq b$ a forma estándar nos encontramos con que la matriz asociada al problema pasa a ser $(A|-I_m)$, lo cual arroja una base trivial del problema, sin embargo, esta base no nos sirve, ya que no está asociada a ningún punto extremo. En efecto $(-I_m)^{-1}b=-I_mb=-b\leq 0$.

Esto nos obliga a usar algún procedimiento de inicialización como el método de las dos fases o el método de las penalizaciones, lo cual supone añadir $m$ columnas más a la tabla, lo cual da rabia, habiendo estado tan cerca de conseguir una base trivial sin hacer nada.

Es de esta frustración que nace el llamado ``algoritmo dual'' en los años $90$, que resuelve estas situaciones más ágilmente, exprimiendo las propiedades de dualidad vistas hasta ahora.
\subsection{Fundamentación teórica}
En este apartado desarrollaremos otro algoritmo para resolver problemas de programación lineal en forma estándar que, como se comentó antes, en ciertas situaciones es más eficiente.

Sea el problema de programación lineal en forma estándar
\begin{equation*}
\begin{array}{c}
\min c^tx\\
\text{Sujeto a:}\qquad Ax= b,\qquad x\geq 0
\end{array}
\end{equation*}
cuyo dual, convenientemente reescrito es
\begin{equation*}
\begin{array}{c}
\max \lambda^tb\\
\text{Sujeto a:}\qquad \lambda^tA\leq c^t
\end{array}
\end{equation*}
Supongamos que disponemos de una base inicial $B$ de la matriz $A$ de forma que todos los costes reducidos asociados al problema primal son positivos, es decir
\begin{equation*}
	\overline{c_j}=c_j-c_B^tB^{-1}a_j\geq 0
\end{equation*}
De ser esto así, si consideramos el vector $\lambda^t:=c_B^tB^{-1}$ es sencillo deducir que $\lambda^t$ es una solución factible del problema dual. En efecto
\begin{multline*}
	\lambda^tA=c_B^tB^{-1}(B|N)=c_B^tB^{-1}(a_1\cdots a_m|a_{m+1}\cdots a_n)=\\=(c_B^tB^{-1}a_1\cdots c_B^tB^{-1}a_m|c_B^tB^{-1}a_{m+1}\cdots c_B^tB^{-1}a_n)=\\=(c_1-\overline{c_1}\cdots c_m-\overline{c_m}|c_{m+1}-\overline{c_{m+1}}\cdots c_n-\overline{c_n})\leq (c_1\cdots c_n)=c^t
\end{multline*}
Nótese que como $\overline{c_j}=0$ cuando $a_j$ es una columna de la base se tiene que $\lambda^ta_j=c_j$ para las columnas asociadas a $B$, mientras que $\lambda^ta_j\leq c_j$ para las columnas no asociadas a $B$.

Dicho esto, consideremos el vector $\overline{x}:=((B^{-1}b)^t|0)^t$, que es solución del problema primal. Nótese que en ningún momento decimos que $\overline{x}$ sea solución \tb{factible}, ya que no sabemos si $\overline{x}\geq 0$. No obstante, sería maravilloso que esto ocurriera, como vemos a continuación.
\begin{lem}[Test de optimalidad]
	\label{dual_lem_testOptim}
	Si $\overline{x}\geq 0$ entonces $\overline{x}$ es solución óptima del problema primal.
\end{lem}
\begin{proof}
	Es evidente ya que en tal caso $c^t\overline{x}=c_B^tB^{-1}b=\lambda^tb$. Es decir, ambas soluciones factibles tienen el mismo valor al ser pasados por sendas funciones objetivo. El corolario \ref{dual_cor_dualidadDebil} nos da el resto del resultado.
\end{proof}
No obstante, esto no suele ser lo habitual. Así pues, la estrategia a seguir cuando tengamos que $\overline{x}$ no es solución factible será cambiar de base, es decir, construir a partir de $B$ una nueva base $B'$ sobre la que continuar probando suerte. Nótese que es una filosofía similar a la que sigue el algoritmo del Símplex primal. Supongamos pues que $\overline{x_l}<0$.

En este caso consideraremos el vector $\lambda_\varepsilon^t:=\lambda^t-\varepsilon u_l^t$ donde $u_l^t$ es la $l$--ésima fila de $B^{-1}$. A lo largo del apartado determinaremos un $\varepsilon>0$ adecuado, de forma que $\lambda_\varepsilon'$ sea una solución factible del problema dual y además exista una base $B'$ de $A$ de forma que $\lambda_\varepsilon^t=c_{B'}^tB'^{-1}$, y, por ende, todos los costes reducidos asociados a $B'$ sean no negativos (compruébese), pudiéndose volver a ejecutar todo el proceso de nuevo.
\begin{obs}[Igualdades útiles]
	Es sencillo comprobar que $u_l^ta_j=y_{lj}$. Esto se debe a que, por definición $B^{-1}a_j=y_j$. Si expresamos la matriz $B^{-1}$ descompuesta en filas obtenemos la expresión $(u_1\cdots u_m)^ta_j=y_j$, en concreto, $y_{lj}=u_l^ta_j$. Análogamente se demuestra que $u_l^tb=\overline{x_l}$. Usaremos estas igualdades a menudo.
\end{obs}

Para que $\lambda_\varepsilon^t$ sea solución factible dual se debe cumplir que $\lambda_\varepsilon^tA\leq c^t$, o equivalentemente, que $\lambda_\varepsilon^ta_j\leq c_j$ para todo $j\in\{1,\dots,n\}$. Vayamos caso por caso para ver qué $\varepsilon$ sería bueno escoger.
\begin{itemize}
	\item Para $j\in\{1,\dots,m\}\setminus\{l\}$ tenemos que
	\begin{equation*}
		\lambda_\varepsilon^ta_j=(\lambda^t-\varepsilon u_l^t)a_j=\lambda^ta_j-\cancel{\varepsilon y_{lj}}=\lambda^ta_j=c_j
	\end{equation*}
	luego para este caso el $\varepsilon$ que cojamos da un poco igual.
	\item Por su parte para $j=l$ tenemos
	\begin{equation*}
		\lambda_\varepsilon^ta_l=\lambda^ta_l-\varepsilon \cancel{y_{ll}}=\lambda^ta_l-\varepsilon=c_j-\varepsilon\leq c_j
	\end{equation*}
	concluyendo que para este caso también da un poco igual.
	\item Por último consideramos el caso $j\in\{m+1,\dots,n\}$
	\begin{equation*}
		\lambda_\varepsilon^ta_j=\lambda^ta_j-y_{lj}\varepsilon\leq c_j-y_{lj}\varepsilon
	\end{equation*}
	este caso ya es más delicado, ya que si $y_{lj}\geq0$ podemos coger el $\varepsilon$ que queramos, no obstante, si este caso no se da, debemos coger $\varepsilon$ con mucho cuidado.
\end{itemize}
Al hilo de esto, veamos qué ocurre si todos los $y_{lj}$ son no negativos.
\begin{lem}[Test de infactibilidad]
	\label{dual_lem_testFacti}
	Si $y_{lj}\geq 0$ para todo $j\in\{m+1,\dots,n\}$ entonces el problema primal es infactible.
\end{lem}
\begin{proof}
	Si consideramos el vector $\lambda_\varepsilon^t$ tenemos que es una solución factible del problema dual, ya que \begin{equation*}
		\lambda_\varepsilon^t A = \lambda_\varepsilon^t (a_1\cdots a_m|a_{m+1}\cdots a_n)=(\lambda_\varepsilon^t a_1\cdots \lambda_\varepsilon^t a_m|\lambda_\varepsilon^t a_{m+1}\cdots\lambda_\varepsilon^t a_n)
	\end{equation*}
	y resulta que $\lambda_\varepsilon^ta_j=\lambda^ta_j-\varepsilon y_{lj}\leq \lambda^ta_j\leq c_j$. Esta desigualdad también se da para los $j\in\{1,\dots,m\}$ ya que $y_{lj}\in\{0,1\}$ para esos índices.
	
	Además, se tiene que $\lambda_\varepsilon^tb=\lambda^tb-\varepsilon u_l^tb=\lambda^tb-\varepsilon\overline{x_l}$. Como $\overline{x_l}<0$ es claro que $\lim_{\varepsilon\to\infty}\lambda_\varepsilon^tb=\infty$, siendo así el problema dual no acotado, y, por tanto (ver la tabla \ref{dual_tab_tabla}), el problema primal será infactible.
\end{proof}
Llegados a este punto únicamente nos queda suponer que hay algún $a_j$ con $y_{lj}<0$ (con $j\in\{m+1,\dots,n\}$). Veamos qué $\varepsilon$ coger. Como ya vimos $\lambda_\varepsilon^ta_j=\lambda^ta_j-y_{lj}\varepsilon$. Desarrollando a partir de aquí tenemos que
\begin{equation*}
	\lambda^ta_j-y_{lj}\varepsilon\leq c_j\sii -\frac{c_j-\lambda^ta_j}{y_{lj}}\geq \varepsilon\sii \varepsilon\leq -\frac{\overline{c_j}}{y_{lj}}
\end{equation*}
En previsión de que haya varios $y_{lj}<0$ tomaremos $\varepsilon$ como
\begin{equation*}
	\varepsilon:=-\frac{\overline{c_k}}{y_{lk}}:=\min\left\{-\frac{\overline{c_j}}{y_{lj}}\tq y_{lj}<0\right\}
\end{equation*}
De esta forma tenemos que $\lambda_\varepsilon^t$ es una solución factible dual, que en particular cumple que $\lambda_\varepsilon^ta_j=c_j$ para $j\in({1,\dots,m}\setminus\{l\})\cup\{k\}$. Esta propiedad es crucial, ya que si consideramos el conjunto de columnas $B':=(B\setminus\{a_l\})\cup\{a_k\}$ (que es una base, se comprueba como en la demostración del teorema \ref{fund_teo_mejoraSimplex}) tenemos
\begin{equation*}
	\lambda_\varepsilon^tB'=c_{B'}^t\sii \lambda_\varepsilon^t=c_{B'}^tB'^{-1}
\end{equation*}
Luego hemos ganado, y además, de una forma objetivamente bonita.

Falta sin embargo comprobar que con este cambio de base estamos acercándonos más a la solución óptima. Una forma de verlo, usando dualidad, es que estamos mejorando la solución del problema dual, y, en efecto
\begin{equation*}
	\lambda_\varepsilon^tb=\lambda^tb-\varepsilon\overline{x_l}\geq \lambda^tb
\end{equation*}
Otro detalle a tener en cuenta es que puede ocurrir que haya varias componenetes negativas de $\overline{x}$. En este caso ¿cuál elegimos para salir de la base? La respuesta es que da un poco igual, no obstante, podemos adoptar un análogo a la vieja y confiable regla de Dantzig, es decir, coger la más pequeña de las componentes negativas.
\begin{obs}[Bucles]
	Lamentablemente, en el algoritmo dual también pueden producirse bucles, en concreto en la situación en la que tomamos $\varepsilon=0$. Para solucionar estas incómodas situaciones se recomienda usar la regla de Bland. Habría que demostrar que esta regla sigue siendo efectiva en este contexto, lo cual se deja al lector ya que la demostración es bastante simétrica.
\end{obs}
Dicho lo cual, podemos pasar a resumirlo todo en un bonito algoritmo.
\begin{algorithm}[H]
	\begin{algorithmic}[1]
		\REQUIRE Base $B$ de $A$ con $\overline{c}\geq 0$.
		\ENSURE Solución óptima del problema.
		\STATE Aplicar el lema \ref{dual_lem_testOptim} para detectar la optimalidad de $\overline{x}$.
		\IF {$\overline{x}$ es solución óptima}
		\RETURN $\overline{x}$.
		\ELSE 
		\STATE Aplicar el lema \ref{dual_lem_testFacti} para detectar la infactibilidad.
		\IF{el problema es no factible}
		\RETURN infactible
		\ELSE
		\STATE Cambiar de base según lo explicado anteriormente.
		\RETURN $B'$, la nueva base.
		\ENDIF
		\ENDIF	
	\end{algorithmic}
	\caption{Primera aproximacíon al algoritmo del Símplex dual.}\label{dual_alg_dual}
\end{algorithm}
Nótese que podemos aplicar el algoritmo del Símplex dual sobre una tabla del Símplex normal y corriente, realizandose los cambios de base de la misma manera, lo cual, de nuevo, es una bendición de los dioses. Queda, no obstante, ver cómo nos las apañamos para obtener la primera base $B$, cosa que veremos en la siguiente sección.
\begin{obs}[Maximización]
	Se puede realizar una adaptación similar a la que ya se hizo con el problema primal para que el algoritmo dual sea también capaz de resolver problemas de maximización. Se deja como ejercicio al lector.
\end{obs}
\subsection{Método de la restricción artificial}
En este apartado veremos una técnica para conseguir la base inicial necesaria para ejecuta el algoritmo dual. Supongamos que ya disponemos de una base $B$ de tal forma que hay algún coste reducido negativo.
\begin{obs}[Suposiciones]
	Suponer que se tiene una base inicial $B$ no es mucho suponer, pues este suele ser el caso de los problemas en los que merece la pena aplicar el algoritmo dual (forma canónica de minimización).
\end{obs}
Supuesto esto, nos panteamos el problema, al que llamaremos \tbi[problema!artificialmente restringido]{problema artificialmente restringido}, o símplemente problema restringido, que se define como
\begin{equation*}
\begin{array}{c}
\min c^tx\\
\text{Sujeto a:}\qquad Ax\leq b\quad \&\quad \sum_{j=m+1}^{n}x_j\leq M,\qquad x\geq 0
\end{array}
\end{equation*}
donde $M$ es un número lo suficientemente grande. Una vez transformado el problema restringido a forma estándar, la matriz del problema restringido tendrá una fila y una columna más correspondientes a la nueva restricción y a su correspondiente variable de holgura.

La columna añadida es linealmente independiente a las demás (compruébese), por lo que deberá ser incluída en la base.

Hecho esto, realizamos el cambio de base consistente en expulsar a la columna correspondiente a la variable de holgura asociada a la restricción artificial (a la cual le asignamos el índice $0$) y admitir la columna con menor coste reducido (supondremos que es la columna $a_k$).
\begin{lem}[Base inicial]
	La base obtenida tras es el cambio hace que el vector de costes reducidos sea no negativo.
\end{lem}
\begin{proof}
	En efecto, la matriz asociada al problema original es $A=(B|N)$, siendo la matriz asociada al problema restringido
	\begin{equation*}
	\overline{A}=\left(\begin{array}{c|c|c}
	1&0&1\\
	\hline
	0&B&N
	\end{array}\right)
	\end{equation*}
	siendo la base $\overline{B}$ del problema restringido la matriz $\overline{A}$ excluyendo la última columna por bloques.
	
	Veamos cómo es la tabla del símplex asociada al problema restringido. Para lo cual, deberemos calcular la inversa de $\overline{B}$. Con un sencillo cálculo llegamos a que
	\begin{equation*}
	\overline{B}^{-1}=\left(\begin{array}{c|c}
	1&0\\
	\hline
	0&B^{-1}
	\end{array}\right)
	\end{equation*}
	de forma que la tabla del Símplex asociada al problema restringido es
	\begin{equation*}
	\overline{B}^{-1}\overline{A}=
	\left(\begin{array}{c|c|c}
	1&0&1\\
	\hline
	0&I_m&B^{-1}N
	\end{array}\right)
	\end{equation*}
	de aquí concluimos que los escalares $y_{0j}=1$ para $j\in\{m+1,\dots,n\}$, dicho esto, de las fórmulas de cambio de base se deduce que los costes reducidos a la base $\overline{B}$ son $\overline{c_j'}=\overline{c_j}-\overline{c_k}$ (compruébese).
	
	Como $\overline{c_k}=\min\{\overline{c_j}\tq \overline{c_j}<0\}$ se tiene que todos los costes reducidos son no negativos.
\end{proof}
A partir de aquí podemos aplicar el algoritmo del Símplex dual para resolver el problema restringido. Dicho algoritmo puede acabar de dos formas distintas.
\begin{enumerate}
	\item \label{dual_enum_caso1}El problema restringido es infactible.
	\item \label{dual_enum_caso2}El problema restringido tiene solución óptima.
	\begin{enumerate}
		\item \label{dual_enum_caso2a}La columna $a_0$ está en la última base considerada por el algoritmo.
		\item \label{dual_enum_caso2b}La columna $a_0$ no está en la última base considerada por el algoritmo.
	\end{enumerate}
\end{enumerate}
Veamos cuáles son las consecuencias de cada uno de estos finales para el problema original.

Supongamos que se da el caso \ref{dual_enum_caso1}. En tal caso tenemos el siguiente lema.
\begin{lem}[Test de factibilidad]
	\label{dual_lem_facti}
	El problema original es factible si y solo si el problema restringido lo es.
\end{lem}
\begin{proof}
	Es claro que si el problema restringido es factible, también lo es el original, ya que tiene menos restricciones.
	
	Recíprocamente, si el problema original es factible, digamos que $\overline{x}$ es una solución factible del problema original (sin pérdida de generalidad un punto extremo), como escogimos $M$ lo suficientemente grande, es claro que $\overline{x}$ también verifica la restricción adicional.
\end{proof}
\begin{obs}[Valor de $M$]
	La demostración anterior puede parecer tramposa, pero realmente no lo es, ya que, como hicimos con el método de las penalizaciones para el algoritmo del Símplex primal, para cada problema podemos escoger un $M$ finito de forma que todos los teoremas funcionen bien.
	
	La existencia de dicho $M$ finito ha de ser demostrada, buscando cotas para el mismo cada vez que aludimos a su ``grandeza'' para demostrar algo.
	
	Por poner un ejemplo, en el caso de \ref{dual_lem_facti} necesitamos que
	\begin{equation*}
		M>\max\left\{\sum_{j=m+1}^{n}\overline{x_j}\tq \overline{x}\text{ punto extremo}\right\}\qedhere
	\end{equation*}
\end{obs}
Continuando con el estudio de la cauística, si se diera la situación \ref{dual_enum_caso2a} se puede demostrar (no lo haremos) que el problema original tiene solución óptima, y, de hecho, si el algoritmo determina que la solución óptima del problema restringido se alcanza en el punto $(x_0,\dots,x_n)$, la solución óptima del problema original se alcanzará en $(x_1,\dots,x_n)$.

Por último, si se cumpliera \ref{dual_enum_caso2b}, cabe distinguir dos opciones
\begin{itemize}
	\item El valor de la función objetivo depende de $M$.
	\item El valor de la función objetivo no depende de $M$.
\end{itemize}
\begin{obs}[Dependencia de la $M$]
	\label{dual_obs_M}
	Las alternativas que acabamos de exponer no pueden considerarse en el caso \ref{dual_enum_caso2a}, ya que el caso de que el valor de la función objetivo dependa de $M$ es imposible. En efecto, si consideramos que la última base examinada es $B':=B\cup\{a_0\}$ (donde $B$ es un conjunto de columnas linealmente independientes) tendríamos que $c^tx=c_{B'}^t\overline{x_{B'}}=(0|c_{B})^tB'^{-1}(M|b^t)^t$, si miramos esto con un poco de cariño tenemos que
	\begin{equation*}
	\begin{array}{ccc}
	
	B':=\left(\begin{array}{c|c}
	1&0\\
	\hline
	0&B
	\end{array}\right),\quad
	&
	B'^{-1}=\left(\begin{array}{c|c}
	1&0\\
	\hline
	0&B^{-1}
	\end{array}\right),\quad
	&
	B'^{-1}(M|b^t)^t=(M|(B^{-1}b)^t)^t
	\end{array}
	\end{equation*}
	de forma que $c^tx$ no depende de $M$ en ningún caso, nótese que se ha usado fuertemente que $c_0=0$, por lo que esta demostración no sirve para los casos en los que $a_0$ no está en la base.
\end{obs}
Finalizado el inciso, supongamos que el valor de la función objetivo depende de la $M$. En tal caso, puede demostrarse, aunque no lo haremos aquí, que el problema original es no acotado.

Por último, si la función objetivo no depende de la $M$, puede demostrarse (no lo haremos) que el problema orginal alcanza la solución óptima, y además, en un número infinito de puntos.