%Para este capítulo se usará la abreviatura "simp".
\chapter{Algoritmo del Símplex}
\label{simp}
Este es un capítulo dedicado a aclarar las lagunas que presenta el algoritmo \ref{fund_alg_simplex}, desarrollando ya un algoritmo completo que tenga en cuenta todas las contigencias habidas y por haber, el algoritmo del Símplex.
\section{Cambios de base}
Llegados a este punto nos preguntamos qué datos necesita el algoritmo \ref{fund_alg_simplex} para ejecutarse. Para obtener la respuesta basta con mirar con cuidado los teoremas en los que se basa, con lo que concluimos que son necesarios
\begin{itemize}
	\item Las componentes asociadas a la base (o componentes básicas) del punto extremo. Para calcular qué vector sale de la base al mejorar el punto extremo (teorema \ref{fund_teo_mejoraSimplex}) y para devolverlas cuando se detecta la optimalidad.
	\item El vector de costes reducidos asociado a la base del punto extremo para aplicar los tests de optimalidad y no acotación. Además de para decidir qué vector entra a la base para mejorar el punto extremo.
	\item Los vectores auxiliares $y_j$ para aplicar los tests de no acotación (y en su caso devolver la dirección en la que la función objetivo decrece indefinidamente) y para calcular qué vector sale de la base al mejorar el punto extremo.
	\item El valor de la función objetivo al ser evaluada en el punto extremo, para devolverlo cuando se halle la solución óptima.
\end{itemize}
Cuando cambiamos de punto extremo, el teorema de mejora nos da explícitamnte la nueva base asociada al nuevo punto. Este nuevo punto tendrá otras componentes asociadas a la base e inducirá nuevos vectores de costes reducidos y vectores axiliares que necesitan ser calculados para ejecutar la nueva iteración.

La forma inocente de calcular todas estas cosas pasa por invertir la base asociada al nuevo punto extremo, lo cual es pecado mortal, pues es un trabajo computacionalmente muy pesado (del orden de $\mc{O}(n^3)$).

En lugar de eso lo que haremos será calcular todos los nuevos valores necesarios a partir de los anteriores. A ver que esto es posible y cómo nos dedicamos en esta sección.
\subsection{Vectores auxiliares y componentes básicas}
En primer lugar nos planteamos la ecuación \eqref{fund_eq_problema} respecto de la base asociada al punto extremo original, $B$, y la nueva base $B'=(B\setminus a_l)\cup a_k$.
\begin{gather}
	x_B=\overline{x_B}-B^{-1}Nx_N\label{simp_eq_general1}\\
	x_{B'}=\overline{x_{B'}}-B'^{-1}N'x_{N'}\label{simp_eq_general2}
\end{gather}
\begin{obs}[Vectores auxiliares]
	Nótese que, por las propiedades del producto de matrices se tiene que
	\begin{gather*}
		B^{-1}N=B^{-1}(a_{m+1}\cdots a_n)=(B^{-1}a_{m+1}\cdots B^{-1}a_n)=(y_{m+1}\cdots y_n)\\
		B'^{-1}N'=(y'_{m+1}\cdots y'_{k-1}|y'_l|y'_{k+1}\cdots y'_n)\qedhere
	\end{gather*}
\end{obs}
Planteando las ecuaciones \eqref{simp_eq_general1} y \eqref{simp_eq_general2} componente a componente nos econtramos con
\begin{gather}
	x_s=\overline{x_s}-\sum_{\substack{j=m+1\\j\not=k}}^{n}y_{sj}x_j-y_{sk}x_k\label{simp_eq_componentes1}\\
	x_s=\overline{x_s'}-\sum_{\substack{j=m+1\\j\not=k}}^{n}y'_{sj}x_j-y'_{sl}x_l\label{simp_eq_componentes2}
\end{gather}
Una vez organizado nuestro espacio de trabajo como lo está ahora, vayamos por partes. Por un lado consideramos la ecuación \eqref{simp_eq_componentes1} para $s=l$. Despejando $x_k$ de esta ecuación obtenemos (¡compruébese!)
\begin{equation}
\label{simp_eq_xk}
	x_k=\frac{\overline{x_l}}{y_{lk}}-\sum_{\substack{j=m+1\\j\not=k}}^{n}\frac{y_{lj}}{y_{lk}}x_j-\frac{1}{y_{lk}}x_l
\end{equation}
Nótese que el despeje que hemos hecho es válido, ya que por el teorema \ref{fund_teo_mejoraSimplex} tenemos garantizado que $y_{lk}>0$. Ahora, si sustituimos el valor de $x_k$ dado por la ecuación \eqref{simp_eq_xk} en la ecuación \eqref{simp_eq_componentes1} obtenemos
\begin{equation*}
	x_s=\overline{x_s}-\frac{y_{sk}\overline{x_l}}{y_{lk}}+\sum_{\substack{j=m+1\\j\not=k}}^{n}\left(\frac{y_{sk}y_{lj}}{y_{lk}}-y_{sj}\right)x_j+\frac{y_{sk}}{y_{lk}}x_l
\end{equation*}
esta ecuación y la ecuación \eqref{simp_eq_componentes2} son dos ecuaciones lineales equivalentes. Por tanto, son la una múltiplo de la otra, no obtante, al tener $x_s$ el mismo coeficiente en ambas ecuaciones, estas deben ser iguales. De esto se desprende, comparando ambas ecuaciones
\begin{equation}
\label{simp_eq_aux1}
	\begin{array}{ccc}
		\displaystyle{\overline{x_s'}=\overline{x_s}-\frac{y_{sk}\overline{x_l}}{y_{lk}}}&\qquad
		\displaystyle{y_{sj}'=y_{sj}-\frac{y_{sk}y_{lj}}{y_{lk}}}&\qquad\displaystyle{y_{sl}'}=-\frac{y_{sk}}{y_{kl}}
	\end{array}
\end{equation}
para $s\in\{1,\dots,m\}\setminus\{l\}$ y con $j\in\{m+1,\dots,n\}\setminus\{k\}$. En el caso $s=l$ estas expresiones también son válidas (aunque no nos importa mucho).

Nos queda pues el trabajo de hallar extresiones para $\overline{x_k'}$, la coordenada $k$--ésima del nuevo punto extremo, e $y_{kj}'$ con $j\in\{m+1,\dots n\}\setminus\{k\}$ (las columnas de $N'$, excepto la columna $l$, que ya se quedó calculada) ya que $y_j'$ donde $j$ es un índice correspondiente a las columnas de $B'$ es $e_j$ (véase definición \ref{fund_defi_vectorAux}).

Cosideramos ahora la ecuación \eqref{simp_eq_componentes2} en el caso $s=k$. Dicha ecuación y \eqref{simp_eq_xk} son ecuaciones lineales equivalentes, de hecho iguales (siguiendo el razonamiento anterior). Por ende basta compararlas para obtener
\begin{equation}
\label{simp_eq_aux2}
	\begin{array}{ccc}
	\displaystyle{\overline{x_k'}=\frac{\overline{x_l}}{y_{lk}}} \qquad&\displaystyle{ y_{kj}'=\frac{y_{lj}}{y_{lk}}}\qquad&\displaystyle{y_{kl}'=\frac{1}{y_{lk}}}
	\end{array}
\end{equation}
\subsection{Costes reducidos y función objetivo}
Echando mano de la ecuación \eqref{fund_eq_funcionObj} respecto de las bases $B$ y $B'$ obtenemos
\begin{gather}
	c^tx=c_B^t\overline{x_B}+\sum_{\substack{j=m+1\\j\not=k}}^{n}\overline{c_j}x_j+\overline{c_k}x_k\label{simp_eq_obj1}\\
	c^tx=c_{B'}^t\overline{x_{B'}}+\sum_{\substack{j=m+1\\j\not=k}}^{n}\overline{c_j'}x_j+\overline{c_l'}x_l\label{simp_eq_obj2}
\end{gather}
Sustituyendo el valor de $x_k$ de \eqref{simp_eq_xk} en \eqref{simp_eq_obj1} obtenemos
\begin{equation*}
	c^tx=c_B^t\overline{x_B}+\overline{c_k}\frac{\overline{x_l}}{y_{lk}}+\sum_{\substack{j=m+1\\j\not=k}}^{n}\left(\overline{c_j}-\overline{c_k}\frac{y_{lj}}{y_{lk}}\right)x_j-\overline{c_k}\frac{1}{y_{lk}}x_l
\end{equation*}
tanto esta ecuación como la \eqref{simp_eq_obj2} son expresiones analíticas de la misma función lineal respecto de las mismas bases (que no tienen nada que ver con las bases asociadas a la matriz del problema). Esto quiere decir que ambas expresiones son iguales, luego comparándolas obtenemos
\begin{equation}
\label{simp_eq_reducidos}
	\begin{array}{ccc}
	\displaystyle{c_{B'}^t\overline{x_{B'}}=c_B^t\overline{x_B}+\overline{c_k}\frac{\overline{x_l}}{y_{lk}}}\qquad&\displaystyle{\overline{c_j'}=\overline{c_j}-\overline{c_k}\frac{y_{lj}}{y_{lk}}}\qquad&
	\displaystyle{\overline{c_l'}=-\overline{c_k}\frac{1}{y_{lk}}}\end{array}
\end{equation}
De esta forma, las ecuaciones \eqref{simp_eq_reducidos}, \eqref{simp_eq_aux1} y \eqref{simp_eq_aux2} nos proporcionan las fórmulas que andábamos buscando.
\section{Tabla del Símplex. Pivotajes}
En esta sección introducimos la llamada \tbi[tabla!del símplex]{tabla del símplex}, que no es más que una forma elegante de implementar las fórmulas de cambio de base deducidas en la sección anterior. Además, evita su memorización.

Sea un problema de programación lineal en forma estándar del que suponemos conocido un punto extremo de su región factible, es decir, una base $B$.

Consideremos el sistema de ecuaciones lineales asociado a dicho problema y multipliquémoslo a ambos lados por $B^{-1}$.
\begin{multline}
	\label{simp_eq_tabla}
	Ax=b\sii (B|N)(x_B^t|x_N^t)^t=b\sii B^{-1}(B|N)(x_B^t|x_N^t)^t=B^{-1}b\sii\\
	\sii (B^{-1}B|B^{-1}N)(x_B^t|x_N^t)^t=\overline{x_B}\sii (I_m|y_{m+1}\cdots y_n)(x_B^t|x_N^t)^t=\overline{x_B}
\end{multline}
La igualdad final de la ecuación \eqref{simp_eq_tabla} puede representarse como una tabla de la siguiente manera
\begin{table}[H]
	\centering
	\begin{tabular}{c|cccccccccc|c}
		$B$      & $x_1$    & $\cdots$ & $x_l$    & $\cdots$ & $x_m$    & $x_{m+1}$   & $\cdots$ & $x_k$    & $\cdots$ & $x_n$    & $\overline{x_B}$ \\ \hline
		$x_1$    & $1$      & $\cdots$ & $0$      & $\cdots$ & $0$      & $y_{1,m+1}$ & $\cdots$ & $y_{1k}$ & $\cdots$ & $y_{1n}$ & $\overline{x_1}$ \\
		$\cdots$ & $\cdots$ &          & $\cdots$ &          & $\cdots$ & $\cdots$    &          & $\cdots$ &          & $\cdots$ & $\cdots$         \\
		$x_l$    & $0$      & $\cdots$ & $1$      & $\cdots$ & $0$      & $y_{l,m+1}$ & $\cdots$ & $y_{lk}$ & $\cdots$ & $y_{ln}$ & $\overline{x_l}$ \\
		$\cdots$ & $\cdots$ &          & $\cdots$ &          & $\cdots$ & $\cdots$    &          & $\cdots$ &          & $\cdots$ & $\cdots$         \\
		$x_m$    & $0$      & $\cdots$ & $0$      & $\cdots$ & $1$      & $y_{m,m+1}$ & $\cdots$ & $y_{mk}$ & $\cdots$ & $y_{mn}$ & $\overline{x_m}$ \\ \hline
	\end{tabular}
	\label{simp_tab_simplex1}
	\caption{Primera aproximación a la tabla del Símplex.}
\end{table}
La virtud de representar un problema de programación lineal de esta forma es que los cambios de base son ``automáticos'' en cierto sentido. Supongamos que queremos cambiar de punto extremo de forma que la nueva base es $B'=(B\setminus a_l)\cup a_k$. Siguiendo las fórmulas de la sección anterior, la tabla asociada a la nueva base sería
\begin{table}[H]
	\centering
	\scalebox{0.9}[0.9]{
	\begin{tabular}{c|cccccccccc|c}
		$B'$     & $x_1$    & $\vdots$ & $x_l$                    & $\vdots$ & $x_m$    & $x_{m+1}$                                  & $\vdots$ & $x_k$    & $\vdots$ & $x_n$                                & $\overline{x_{B'}}$                                  \\ \hline
		$x_1$    & $1$      & $\vdots$ & $-\frac{y_{1k}}{y_{lk}}$ & $\vdots$ & $0$      & $y_{1,m+1}-y_{1k}\frac{y_{l,m+1}}{y_{lk}}$ & $\vdots$ & $0$      & $\vdots$ & $y_{1n}-y_{1k}\frac{y_{ln}}{y_{lk}}$ & $\overline{x_1}-y_{1k}\frac{\overline{x_l}}{y_{lk}}$ \\
		$\vdots$ & $\vdots$ &          & $\vdots$                 &          & $\vdots$ & $\vdots$                                   &          & $\vdots$ &          & $\vdots$                             & $\vdots$                                             \\
		$x_k$    & $0$      & $\vdots$ & $\frac{1}{y_{lk}}$       & $\vdots$ & $0$      & $\frac{y_{l,m+1}}{y_{lk}}$                 & $\vdots$ & $1$      & $\vdots$ & $\frac{y_{ln}}{y_{lk}}$              & $\frac{\overline{x_l}}{y_{lk}}$                      \\
		$\vdots$ & $\vdots$ &          & $\vdots$                 &          & $\vdots$ & $\vdots$                                   &          & $\vdots$ &          & $\vdots$                             & $\vdots$                                             \\
		$x_m$    & $0$      & $\vdots$ & $-\frac{y_{mk}}{y_{lk}}$ & $\vdots$ & $1$      & $y_{m,m+1}-y_{mk}\frac{y_{l,m+1}}{y_{lk}}$ & $\vdots$ & $0$      & $\vdots$ & $y_{mn}-y_{mk}\frac{y_{ln}}{y_{kl}}$ & $\overline{x_m}-y_{mk}\frac{\overline{x_l}}{y_{lk}}$ \\ \hline
	\end{tabular}
	}
	\caption{Tabla del problema respecto de la base $B'$.}
\end{table}
Si nos fijamos, la nueva tabla puede verse como el resultado de aplicar el algoritmo de Gauss--Jordan en la columna $k$--ésima, es decir, considerando la tabla como una matriz, usar las transformaciones elementales de matrices para convertir el elemento $y_{lk}$ en un $1$ y ``hacer ceros'' en el resto de la columna. Esto es maravilloso porque puede implementarse en ordenador de forma casi trivial.

La mala noticia es que la tabla \ref{simp_tab_simplex1} no tiene todos los datos necesarios para que el algoritmo se ejecute. En concreto, faltan los costes reducidos y el valor de la función objetivo.

La gran noticia sin embargo (y prueba irrefutable de que los dioses son benévolos de cuando en cuando) es que podemos completar la tabla \ref{simp_tab_simplex1} de manera que lleve todos los datos necesarios y además la implementacón de los cambios de base no varíe en absoluto. En efecto, si consideramos la tabla
\begin{table}[H]
	\centering
	\begin{tabular}{c|cccccccccc|c}
		$B$                                 & $x_1$                   & $\cdots$                     & $x_l$                   & $\cdots$                     & $x_m$                   & $x_{m+1}$                                & $\cdots$                     & $x_k$                                & $\cdots$                    & $x_n$                                 & $\overline{x_B}$                               \\ \hline
		$x_1$                               & $1$                     & $\cdots$                     & $0$                     & $\cdots$                     & $0$                     & $y_{1,m+1}$                              & $\cdots$                     & $y_{1k}$                             & $\cdots$                    & $y_{1n}$                              & $\overline{x_1}$                               \\
		$\cdots$                            & $\cdots$                &                              & $\cdots$                &                              & $\cdots$                & $\cdots$                                 &                              & $\cdots$                             &                             & $\cdots$                              & $\cdots$                                       \\
		$x_l$                               & $0$                     & $\cdots$                     & $1$                     & $\cdots$                     & $0$                     & $y_{l,m+1}$                              & $\cdots$                     & $y_{lk}$                             & $\cdots$                    & $y_{ln}$                              & $\overline{x_l}$                               \\
		$\cdots$                            & $\cdots$                &                              & $\cdots$                &                              & $\cdots$                & $\cdots$                                 &                              & $\cdots$                             &                             & $\cdots$                              & $\cdots$                                       \\
		$x_m$                               & $0$                     & $\cdots$                     & $0$                     & $\cdots$                     & $1$                     & $y_{m,m+1}$                              & $\cdots$                     & $y_{mk}$                             & $\cdots$                    & $y_{mn}$                              & $\overline{x_m}$                               \\ \hline
		\multicolumn{1}{c|}{$\overline{c}$} & \multicolumn{1}{c}{$0$} & \multicolumn{1}{c}{$\cdots$} & \multicolumn{1}{c}{$0$} & \multicolumn{1}{c}{$\cdots$} & \multicolumn{1}{c}{$0$} & \multicolumn{1}{c}{$\overline{c_{m+1}}$} & \multicolumn{1}{c}{$\cdots$} & \multicolumn{1}{c}{$\overline{c_k}$} & \multicolumn{1}{c}{$\cdots$} & \multicolumn{1}{c|}{$\overline{c_n}$} & \multicolumn{1}{c}{$-(c_{B}^t\overline{x_B})$}
	\end{tabular}
	\caption{Tabla del Símplex.}
\end{table}
la tabla asociada a la base $B'$ será
\begin{table}[H]
	\centering
	\scalebox{0.9}[0.9]{
	\begin{tabular}{c|cccccccccc|c}
		$B'$           & $x_1$    & $\vdots$ & $x_l$                             & $\vdots$ & $x_m$    & $x_{m+1}$                                                   & $\vdots$ & $x_k$    & $\vdots$ & $x_n$                                                & $\overline{x_{B'}}$                                                      \\ \hline
		$x_1$          & $1$      & $\vdots$ & $-\frac{y_{1k}}{y_{lk}}$          & $\vdots$ & $0$      & $y_{1,m+1}-y_{1k}\frac{y_{l,m+1}}{y_{lk}}$                  & $\vdots$ & $0$      & $\vdots$ & $y_{1n}-y_{1k}\frac{y_{ln}}{y_{lk}}$                 & $\overline{x_1}-y_{1k}\frac{\overline{x_l}}{y_{lk}}$                     \\
		$\vdots$       & $\vdots$ &          & $\vdots$                          &          & $\vdots$ & $\vdots$                                                    &          & $\vdots$ &          & $\vdots$                                             & $\vdots$                                                                 \\
		$x_k$          & $0$      & $\vdots$ & $\frac{1}{y_{lk}}$                & $\vdots$ & $0$      & $\frac{y_{l,m+1}}{y_{lk}}$                                  & $\vdots$ & $1$      & $\vdots$ & $\frac{y_{ln}}{y_{lk}}$                              & $\frac{\overline{x_l}}{y_{lk}}$                                          \\
		$\vdots$       & $\vdots$ &          & $\vdots$                          &          & $\vdots$ & $\vdots$                                                    &          & $\vdots$ &          & $\vdots$                                             & $\vdots$                                                                 \\
		$x_m$          & $0$      & $\vdots$ & $-\frac{y_{mk}}{y_{lk}}$          & $\vdots$ & $1$      & $y_{m,m+1}-y_{mk}\frac{y_{l,m+1}}{y_{lk}}$                  & $\vdots$ & $0$      & $\vdots$ & $y_{mn}-y_{mk}\frac{y_{ln}}{y_{kl}}$                 & $\overline{x_m}-y_{mk}\frac{\overline{x_l}}{y_{lk}}$                     \\ \hline
		$\overline{c'}$ & $0$      & $\vdots$ & $-\overline{c_k}\frac{1}{y_{lk}}$ & $\vdots$ & $0$      & $\overline{c_{m+1}}-\overline{c_k}\frac{y_{l,m+1}}{y_{lk}}$ & $\vdots$ & $0$      & $\vdots$ & $\overline{c_n}-\overline{c_k}\frac{y_{ln}}{y_{lk}}$ & $-c_{B'}^t\overline{x_{B'}}-\overline{c_k}\frac{\overline{x_l}}{y_{lk}}$
	\end{tabular}}
	\caption{Tabla del problema respecto de la base $B'$.}
\end{table}
Como se observa, la tabla del Símplex tiene en su última fila el vector de costes reducidos y el \tb{opuesto} al valor que toma la función objetivo en el punto extremo asociado a la base que corresponda. La razón para guardar el opuesto y no el valor a secas es que si guardamos el opuesto podemos seguir haciendo los cambios de base como si aplicaramos parcialmente el algoritmo de Gauss-Jordan, en caso contrario no podríamos hacerlo.

\begin{obs}[Notación]
	Es habitual encontrarse con que la casilla correspondiente a la función objetivo en la tabla del Símplex contiene la expresión $z-(c_B^t\overline{x_B})$. Esto no supone ninguna variación en absolutamente nada, es símplemente notación, añadir una $z$ al principio, sin más.
\end{obs}

A la operación de cambiar de base una tabla se la denmina \tbi{pivotaje} y al elemento $y_{lk}$ se le denomina \tbi{pivote}.

\subsection{Criterios de entrada y salida de la base}
El algoritmo \ref{fund_alg_simplex} deja al azar qué hacer para elegir la columna que entra en la base cuando hay varias columnas con coste reducido negativo. Hay diversas heurísticas y técnicas para resolver estos empates a la hora de la implementación, no obstante, la que usaremos nosotros es la llamada \tbi[regla!de Dantzig]{regla de Dantzig}, que consiste en coger la columna asociada al coste reducido más pequeño, deshaciendo los empates al azar.

En cualquier caso, como hemos podido demostrar, aplicar esta regla o no da un poco igual, por tanto se dejó al azar con cierta parte de razón. No obstante, a la hora de la implementación si es muy recomendable seguir algún criterio, pues elegir la columna de entrada al tuntún puede llevarnos a hacer más pivotajes de la cuenta.

El algoritmo \ref{fund_alg_simplex} sí que nos da un criterio de salida de la base, ya que nos pide amablemente que apliquemos el teorema \ref{fund_teo_mejoraSimplex}, y este teorema nos dice que escojamos la columna que minimice el cociente $\frac{\overline{x_i}}{y_{ik}}$ con $y_{ik}>0$ siendo $k$ el índice de la columna que va a entrar en la base.

A este criterio se le conoce como \tbi[regla!de la razón mínima]{regla de la razón mínima}. El problema de esta regla es que en ocasiones pueden producirse empates, que, aunque en la mayoría de los casos podrían resolverse al azar, en ciertas circunstancias pueden ocasionar graves problemas. Veremos como subsanar estas contingencias más adelante.
\section{Punto extremo inicial}
Aunque el trabajo duro ya está hecho, todavía quedan preguntas por resolver, por ejemplo, cómo obtener el punto extremo inicial (si lo hay, ya que el problema podría ser infactible) y todos los datos necesarios asociados a este de forma eficiente.
\subsection{Método de las dos fases}
Encontrar bases de matrices es un trabajo compuracionalmente costoso, en el caso peor, que no haya base alguna, es del orden de $\mc{O}\binom{n}{m}$ (eso si las comprobaciones intermedias tuvieran coste constante, que no lo tienen). Razón por la cual necesitamos algún tipo de sustitutivo a este método de fuerza bruta.

Lo que exponemos a continuación es conocido como \tbi[método!de las dos fases]{método de las dos fases} y únicamente es aplicable a problemas en forma estándar con $b\geq 0$. Esto no supone ninguna pérdida de generalidad ya que siempre podemos multiplicar la restricción correspondiente a un $b_i<0$ por $-1$, obteniendo una restricción equivalente y, por tanto, un problema equivalente.

A un problema en las hipótesis anteriormente expuestas que tenga a $A$ por matriz de coeficientes de las restricciones, se le asocia otro problema al que llamaremos \tbi[problema!artificial]{problema artificial}. Dicho problema es el siguiente
\begin{equation*}
	\begin{array}{c}
		\min w(x^t|x_a^t)^t:=\sum_{i=1}^{m}x_i^a\\
		\text{Sujeto a:}\qquad (A|I_m)(x^t|x_a^t)^t=b,\qquad x\geq 0,\ x_a\geq 0
	\end{array}
\end{equation*}
es decir, el problema resultante de cosiderar las restricciones
\begin{equation*}
	a_{i1}x_1+\dots+a_{in}x_n+x_i^a=b_i
\end{equation*}
para $i\in\{1,\dots,m\}$, cuya matriz asociada es $(A|I_m)$. A las nuevas variables $x_i^a$ se las conoce como \tbi[variable!artificial]{variables artificiales}. Además, la función objetivo se sustituye por $w\equiv \sum_{i=1}^{m}x_i^a$.

Entre las buenas propiedades de este nuevo problema es que tiene una base trivial, la identidad, situada convenientemente en las últimas columnas (lo cual el ordenador agradece).

Pero la cosa no queda aquí, ya que el problema artificial no es un problema no acotado. En efecto, por las restricciones de no negatividad, la función objetivo está acotada inferiormente por $0$. Además, el problema artificial es factible, bastanto considerar el vector $(0|b^t)^t\in\R^{n+m}$ como solución factible. Esta es la razón por la que exigimos $b\geq 0$.

Con esto, por el teorema fundamental de la programación lineal (teorema \ref{fund_teo_fund}), sabemos que el problema artificial tiene solución óptima. Por si esto fuera poco, obtenemos casi gratis un test de factibilidad para el problema original.
\begin{lem}[Test de factibilidad]
	\label{simp_lem_testFacti}
	El problema original es factible si y solo si el problema artificial posee una solución óptima $(\overline{x}^t|\overline{x_a}^t)^t$ con $\overline{x_a}=0$.
\end{lem}
\begin{proof}
	Demostraremos la implicación a la derecha, para descubrir después que si miramos la demostración con amor, y del revés, la otra implicación también está demostrada.
	
	Si el problema original es factible, entonces habrá un $x\geq 0$ tal que $Ax=b$. En tal caso es claro que el vector $(x^t|0)^t$ es solución factible del problema artificial (¡compruébese!). Como resulta que $w(x^t|0)^t=0$, por la acotación inferior de $w$ tenemos que $(x^t|0)^t$ es solución óptima del problema artificial con $\overline{x_a}=0$.
\end{proof}
\begin{obs}[Sutileza]
	Si el problema artificial posee una solución óptima $x^*$ con $\overline{x_a}=0$, todas sus soluciones óptimas $x'$ tendrán $\overline{x_a}=0$, ya que, en caso contrario $w(x^*)<w(x')$, lo cual es absurdo. 
\end{obs}
Dicho esto, es claro que, dado un problema de programación lineal dentro de nuestras hipótesis, podemos considerar el problema artificial asociado y resolverlo mediante el algoritmo del Símplex que ya tenemos desarrollado, siendo el rellenado de la tabla inicial (y la determinación del primer punto extremo) trivial, ya que $B=I_m$, por tanto la tabla del Símplex es la correspondiente a la matriz $(A|I_m|b)$ añadiendo como última fila el vector de costes reducidos y el opuesto del valor de la función objetivo.

Cabe destacar que el cálculo del vector de costes reducidos es bastante trivial computacionalmente ya que no hay que invertir ninguna base, símplemente $\overline{w_j}=w_j-w_B^ta_j$.

Una vez hallada la solución óptima al problema artificial, podremos usar el lema \ref{simp_lem_testFacti} para decidir sobre la factibilidad o no del problema original. En caso de que este resulte factible, la base asociada a la solución óptima encontrada será base del problema original (ya que todas las columnas de la base están asociadas a columnas de la matriz original). Con esto, ya podríamos aplicar el algoritmo del Símplex para resolver el problema original.

No obstante, cabría preguntarse si es computacionalmente costoso rellenar la tabla del simplex del problema original respecto de la base $B$ dada (es decir, ¿hay que invertir $B$?). La respuesta es no, ya que necesitamos calcular la matriz
\begin{equation*}
	B^{-1}(A|b)=(B^{-1}a_1\cdots B^{-1}a_n|B^{-1}b)
\end{equation*}
y la matriz asociada a la tabla asociada a la solución óptima del problema artificial es
\begin{equation*}
	B^{-1}(A|I_m|b)=(B^{-1}a_1\cdots B^{-1}a_n|B^{-1}e_1\cdots B^{-1}e_m|B^{-1}b)=(B^{-1}A|B^{-1}|B^{-1}b)
\end{equation*}
luego basta con tomar la misma tabla suprimiendo las columnas asociadas a las variables artificiales.

Puede ocurrir, sin embargo, que, una vez finalizada la primera fase, haya variables artificiales entre las básicas. Si dichas variables toman un valor positivo, por el lema~\ref{simp_lem_testFacti} sabemos que el problema original es infactible. Si, en cambio, tienen valor nulo, el problema es factible. En tal caso, no es deseable que sean básicas, dado que el problema original carece de estas variables artificiales. Luego debemos sacarlas de la base antes de pasar a la segunda fase. Pueden darse dos casos. Supongamos que $\overline{x_{ia}}=0$ es una de las variables artificiales básicas. Definimos $I_a$ como el conjunto de índices de las variables artificiales básicas e $I_o$ como el conjunto de índices de las variables originales básicas. Análogamente para las variables no básicas, $J_a, J_o$. Entonces:
\begin{itemize}
	\item Si existe algún índice $j\in J_o$ tal que $y_{iaj}$ es no nulo, entonces, pivotando en $y_{iaj}$, obtenemos la misma solución factible con una base distinta que no incluye dicha variable artificial.
	
	En efecto, atendiendo a las expresiones~\eqref{simp_eq_aux1},~\eqref{simp_eq_aux2} y~\eqref{simp_eq_reducidos} queda
	\begin{equation}
		\overline{x_s'}=\overline{x_s}-\frac{y_{sj}}{y_{iaj}}\overline{x_{ia}}=\overline{x_s} \qquad \overline{x_j'}=\frac{\overline{x_{ia}}}{y_{iaj}}=0 \qquad c^t_{B'}\overline{x_{B'}}=c^t_B\overline{x_B}+\overline{c_j}\frac{\overline{x_{ia}}}{y_{iaj}}\overline{c_s'}=c^t_B\overline{x_B},
	\end{equation}
	con lo que la solución es la misma que en la iteración anterior. Nótese que no es necesario que $y_{iaj}$ sea positivo.
	
	\item Si $y_{iaj}$ es nulo para todo $j\in J_o$, entonces la fila $ia$-ésima puede ser suprimida por ser redundante. Veamos esto en un lema.
\end{itemize}
\begin{lem}
	Si en la solución resultante de la primera fase existe una variable artificial en la base $\overline{x^a_k}=0$ verificando, además, que $y_{kj}=0$ para todo $j\in J_o$, entonces el sistema de ecuaciones $Ax=b$ es redundante y la ecuación correspondiente a esta variables se puede suprimir.
\end{lem}
\begin{proof}
	Sea $a^i$, para $i\in\{1,\cdot,m\}$ la fila $i$-ésima de la matriz $A$. El sistema de ecuaciones $Ax=b$ es redundante si, y solo si, existe una combinación lineal no trivial entre las filas de la matriz $A$:
	\begin{equation*}
		\sum_{i=1}^m\lambda_ia^i=0\in\R^m,
	\end{equation*}
	siendo $\lambda\in\R^m$ no nulo y verificando, para que el sistema de ecuaciones sea compatible,
	\begin{equation*}
		\sum_{i=1}^m\lambda_ib_i=0.
	\end{equation*}
	Sea $B$ la base asociada a la solución de la primera fase. Definimos entonces
	\begin{equation*}
		\lambda^t=e_k^tB^{-1},
	\end{equation*}
	siendo así el vector de la fila $k$-ésima de la inversa de la base. Comprobemos que este $\lambda$ cumple las condiciones para que el sistema sea redundante, terminando así la prueba.
	
	Para empezar, $\lambda$ es no nulo, ya que coincide con la fila de una matriz invertible.
	
	Por otro lado, 
	\begin{equation*}
		\sum_{i=1}^m\lambda_ia^i=\lambda^t A=e^t_kB^{-1}A=e^t_k(\cdots,e_i,\cdots,y_j,\cdots),
	\end{equation*}
	donde $e_i$ son vectores unitarios para todo $i\in I_o$, mientras que $y_j=B^{-1}a_j$ para todo $j\in J_a$. Al ser $i\not=k$ para cualquier $i\in I_o$, se tiene que $e^t_ke_i=0$. Además, como por hipótesis $y_{kj}=0$, también $e^t_ky_j=0$ para todo $j\in J_o$. Luego se cumple que es nulo.
	
	Por último, 
	\begin{equation*}
		\sum_{i=1}^m\lambda_ib_i=\lambda^t b=e^t_kB^{-1}b=e^t_k\overline{x_B}=\overline{x_k^a}=0.
	\end{equation*}
\end{proof}
Una vez aplicado el caso correspondiente a cada una de las variables artificiales básicas, obtenemos una solución factible sin variables artificiales en la base, con lo que podemos proceder con la segunda fase. Para ello, como hemos indicado antes, basta tomar la misma tabla suprimiendo las columnas asociadas a las variables artificiales.

En cuanto al cálculo de los costes reducidos, aunque ya tenemos invertida la matriz $B$ y podríamos hacerlo ``a capón'', no es recomendable, pues multiplicar matrices es computacionalemente costoso.

En lugar de eso, lo que podemos hacer es añadir una fila más a la tabla del Símplex del problema artificial, donde ir calculando (mediante los pivotajes) los costes reducidos asociados a la función objetivo $z:=c^tx$ del problema original (nótese que no hay ningún problema).

De esta forma, basta con consultar la tabla asociada a $B$ del problema artificial para rellenar la tabla inicial del problema original, que ya puede ser resuelto usando el algoritmo del Símplex conocido. Organizemos toda esta literatura.
\begin{algorithm}[H]
	\begin{algorithmic}[1]
		\STATE\COMMENT{Primera fase}
			\STATE Construir el problema artificial
			\STATE Resolver el problema artificial (algoritmo \ref{fund_alg_simplex}) repedidas veces.
			\STATE Aplicar el test de factibilidad (lema \ref{simp_lem_testFacti})
			\IF{es factible}
				\STATE\COMMENT{Segunda fase}
				\STATE Rellenar la primera tabla con los trucos vistos.
				\STATE Resolver el problema original (algoritmo \ref{fund_alg_simplex}) repedidas veces.
			\ELSE
				\RETURN Infactible
			\ENDIF
	\end{algorithmic}
	\caption{Algoritmo de las dos fases.}\label{simp_alg_dosFases}
\end{algorithm}
\subsection{Método de las penalizaciones (Big $M$)}
Una alternativa al método de las dos fases es el llamado \tbi[método! de las penalizaciones]{método de las penalizaciones} o de la $M$ grande. Al igual que el método de las dos fases, solo es aplicable a problemas con $b\geq 0$, lo cual, como discutimos anteriormente, no supone ninguna pérdida de generalidad.

El método de las dos fases también asocia al problema que queremos resolver otro problema, al que llamaremos \tbi[problema!penalizado]{problema penalizado}, cuyas restricciones son exactamente las mismas que las del problema artificial. Lo único que los diferencia es la función objetivo, que en este caso es
\begin{equation*}
	u:\equiv z+Mw
\end{equation*}
donde $z$ es la función objetivo del problema original y $w$ la función objetivo del problema artificial de las dos fases. Por su parte, $M$ es una constante positiva lo suficientemente grande (especificaremos después).

Antes de continuar presentemos un par de observaciones elementales.
\begin{obs}[Relaciones de factibilidad]
	Evidentemente, toda solución factible del problema original puede ``extenderse'' (poniendo las variables artificiales a cero) de modo que se convierte en una solución factible del problema penalizado. Este proceso puede realizarse a la inversa, en el sentido de que toda solución factible del problema penalizado que tenga a $0$ todas sus variables artificiales puede ``acortarse'' (fusilando las componentes asociadas a las variables artificiales) de manera que pasa a ser una solución factible del problema original.
\end{obs}
\begin{obs}[Sutilezas]
	Evidentemente, el problema penalizado es factible, esto ocurre por el mismo motivo que lo es el problema artificial. No obstante, y al contrario de lo que sucedía con el problema artificial, el problema penalizado puede estar no acotado. En efecto, basta que el problema original sea no acotado.
\end{obs}
Dicho esto, si aplicamos el algoritmo del Símplex al problema penalizado, este puede acabar de dos maneras
\begin{enumerate}
	\item El último punto extremo analizado tiene alguna componente artificial positiva.
	\item El último punto extremo tiene todas las componentes asociadas a variables artificiales nulas.
\end{enumerate}
Demostremos que si se da el primer caso, entonces el problema original es infactible.
\begin{prop}[Test de infactibilidad]
	Si el último punto extremo tiene alguna componente positiva, el problema original es infactible.
\end{prop}
\begin{proof}
	Distinguimos dos casos. Según el algoritmo detecte la no acotación o la optimalidad del problema penalizado.
	\begin{itemize}
		\item En caso de detectar la optimalidad (digamos para un punto $((x^*)^t|x_a^t)^t$), es claro que el problema es infactible, ya que de existir una solución factible $\overline{x}$ del problema original (sin pérdida de generalidad un punto extremo) se tendría que $c^t\overline{x}\leq c^tx^*+Mw((x^*)^t|x_a)^t$, siempre y cuando $M$ sea adecuadamente grande (especificaremos luego).
		\item En caso de detectarse la no acotación tendríamos que hay un cierto $k$ para el cual $\overline{u_k}<0$ y además $y_k\leq 0$ (si hubiera varios escogemos es que tenga coste reducido menor). Si desarrollamos la cuenta tenemos que 
		\begin{equation*}
			\overline{u_j}=u_j-\sum_{\text{no artif.}\in B}c_iy_{ij}-M\sum_{\text{artif.}\in B}y_{sj}
		\end{equation*}
		En el caso de $\overline{u_k}$ todos los escalares $y_{sk}$ correspondientes al último sumatorio son nulos, esto se debe a que, de haber alguno negativo tendríamos que $\overline{u_k}>0$ suponiendo que $M$ sea convenientemente grande.
		
		En el caso de otros $\overline{u_j}$ el último sumatorio no puede ser positivo, ya que, de serlo, al ser $M$ lo suficientemente grande tendríamos que $\overline{u_j}<\overline{u_k}$, contra la minimalidad de $\overline{u_k}$.
		
		La ecuación \eqref{simp_eq_general1} particularizada para las componentes asociadas a la base y que además están asociadas a variables artificiales nos dice que
		\begin{equation*}
			x_s=\overline{x_s}-\sum_{j\not\in B}y_{sj}x_j
		\end{equation*}
		sumando todas estas ecuaciones obtenemos
		\begin{equation*}
			\sum_{\text{artif.}\in B}x_s=\sum_{\text{artif.}\in B}\overline{x_s}-\sum_{j\not\in B}\left(\sum_{\text{artif.}\in B}y_{sj}\right)x_j
		\end{equation*}
		si el problema original tuviera alguna solución factible, su ampliación sería solución del problema penalizado, por ende, deberá cumplir la ecuación anterior, que en su caso particular es
		\begin{equation*}
			\sum_{\text{artif.}\in B}\overline{x_s}=\sum_{j\not\in B}\left(\sum_{\text{artif.}\in B}y_{sj}\right)x_j
		\end{equation*}
		por hipótesis el miembro de la izquierda es estrictamente positivo, mientras que el de la derecha es no negativo, lo cual es absurdo.  
		\qedhere
	\end{itemize}
\end{proof}
Una vez hecho esto, es trivial demostrar (se deja al lector) que si se da el segundo caso se dan los siguientes subcasos
\begin{itemize}
	\item Si se detecta optimalidad, la solución ``recortada'' es solución óptima del problema original.
	\item Análogamente con la no acotación.
\end{itemize}
Para casi finalizar, veamos cómo son los costes reducidos asociados a la función objetivo $u$, para lo cual, echaremos una cuenta rápida, pero antes, un poco de notación. En primer lugar extendemos la definición de $c_j$ por $0$ para índices $j\in\{n+1,\dots,n+m\}$. En segundo lugar, definimos $d_j$ como $0$ para $j\in\{1,\dots,n\}$ y como $1$ para $j\in\{n+1,\dots,n+m\}$. De esta forma
\begin{equation*}
	u^tx=\sum_{j=1}^{n+m}u_jx_j=\sum_{j=1}^{n+m}c_jx_j+M\sum_{j=1}^{n+m}d_jx_j=\sum_{j=1}^{n+m}(c_j+d_jM)x_j
\end{equation*}
con lo que finalmente se concluye
\begin{multline*}
	\overline{u_j}=u_j-u_B^ty_j=(c_j+Md_j)-(c_B^t+Md_B^t)y_j=\\=(c_j-c_B^ty_j)+M(d_j-d_B^ty_j)=\overline{c_j}+M\overline{d_j}
\end{multline*}
Dicho esto, las tablas del Símplex se representarán de la manera habitual con una pequeña salvedad, dividiremos (por razones de comodidad) tanto la función objetivo como el vector de costes reducidos en dos filas, una para la parte asociada a $z$ y otra para la parte asociada a $Mw$ (nótese que los pivotajes se pueden seguir haciendo exactamente de la misma forma).

Finalizamos este apartado con dos pequeñas observaciones.
\begin{obs}[Implementación]
	Nótese que a efectos de implementación no necesitamos saber cuánto vale $M$ (mucho menos si somos laxos a la hora de aplicar la regla de Dantzig), lo único que tenemos que tener en cuenta es que es ``dominante'', en el sentido de que al saber si un coste reducido es positivo o negativo basta mirar el factor que acompaña a $M$, ya que lo demás lo consideraremos despreciable.
\end{obs}
\begin{obs}[Valor de $M$]
	Aunque para la implementación no necesitamos saber el valor de $M$, si que necesitamos saber que todo problema posee un valor finito de $M$ tal que hace ciertas las demostraciones que hemos venido realizando.
	
	Demostrar esto es sencillo, basta ir releyendo el apartado, a lo largo del cual vamos imponiendo exigencias a la $M$, a partir de estas exigencias podemos ir sacando cotas inferiores (se deja el trabajo al lector).
\end{obs}
\section{Prevención de bucles}
El algoritmo \ref{fund_alg_simplex} se basa fundamentalmente en el teorema \ref{fund_teo_mejoraSimplex}, el cual deja la puerta abierta a que se produzcan los temido \tbi[pivotaje!degenerado]{pivotajes degenerados}, es decir, cambios de base que no mejoran el valor de la función objetivo.

El propio teorema \ref{fund_teo_mejoraSimplex} nos da una forma de saber cuándo un pivotaje es degenerado, pues basta fijarnos en la tabla y ver que estamos intentando sacar la columna $l$--ésima de la base, teniendo esta el valor $\overline{x_l}=0$.

Los pivotajes degenerados no cambian el punto extremo, analizan el mismo punto extremo de nuevo pero respecto de otra base distinta. Esto también nos lo dice el teorema \ref{fund_teo_mejoraSimplex}, ya que el nuevo punto extremo a analizar viene dado explícitamente por $\overline{x}+\frac{\overline{x_l}}{y_{lk}} d$, luego si $\overline{x_l}=0$ el punto no cambia.

De esta forma se deduce que si el algoritmo no se encuentra con pivotajes degenerados, necesariamente termina ya que no puede examinar una base anteriormente examinada, ya que el valor del siguiente punto extremo a examinar es estrictamente menor que el anterior.

Por ende, el peligro de los bucles únicamente podría aparecer con los pivotajes degenerados, y de hecho aparece, el ejemplo más famoso se debe a Martin Beale. En esta sección desarrollaremos técnicas para prevenir estos bucles.
\subsection{Regla lexicográfica}
En este apartado exponemos un criterio de salida de la base, al que llamamos \tbi[regla!de la razón léxico mínima]{regla de la razón léxico--mínima} o símplemente \tbi[regla!lexicográfica]{regla lexicográfica}. Lo que hace útil a este criterio (descubierto en $1955$) es que, como veremos al finalizar la sección, es un criterio que evita la aparición de bucles, de forma que garantiza la convergencia del algoritmo a una solución óptima (cuando el problema la posea).

Antes de comenzar introduzcamos las siguientes definiciones.
\begin{defi}[Lexicográficamente positivo]
	Un vector $x\in\R^n$ se dice \tbi[lexicográficamente!positivo]{lexicográficamente positivo} si es no nulo y su primera componente no nula es positiva.
	
	Esto lo denotamos por $x>_L0$.
\end{defi}
\begin{defi}[Orden lexicográfico]
	Dados dos vectores $x,y\in\R^n$ decimos que $x$ es \tbi[lexicográficamente!mayor]{lexicográficamente mayor} que $y$ si $x-y$ es lexicográficamente positivo. Sintéticamente
	\begin{equation*}
		x>_Ly\sii x-y>_L0
	\end{equation*}
	Esta relación define un orden total estricto en $\R^n\setminus\{0\}$.
\end{defi}
Hechas estas consideraciones previas, supongamos que en cierta iteración del algoritmo se decide que la $k$--ésima columna entra en la base, mientras que a la hora de decidir la variable que sale, se producen empates con la regla de la razón mínima.

En tal caso consideramos la matriz $V_B:=(\overline{x_B}|B^{-1})$ donde $B$ es la base ``vieja'' que vamos a sustituir por la base $B'$ (aún por calcular).

Denotaremos por $V_B^i$ a la fila $i$--ésima de la matriz $V_B$.
\begin{obs}[Cálculo de $V_B$]
	Nótese que el cálculo de la matriz $V_B$ no supone ningún coste adicional para el ordenador en el caso de que la base inicial sea la identidad, cosa que siempre pasará si utilizamos el método de las dos fases o de las penalizaciones.

	Esto se debe a que la tabla del símplex (salvo el vector de costes y la función objetivo) viene dada por la matriz
	\begin{equation*}
		B^{-1}(A|b)=B^{-1}(N|I_m|b)=(B^{-1}N|B^{-1}|\overline{x_B})
	\end{equation*}
	con lo que tenemos $B^{-1}$ en las columnas correspondientes a la base inicial, con lo que automáticamente tenemos $V_B$.
	
	Este es un buen motivo para, cuando nos encontremos en la segunda fase del método de las dos fases, no tachar las columnas sobrantes, sino tenerlas ahí (marcadas para no pivotar por ellas) con objeto de tener $B^{-1}$ siempre a mano por si se producen ciclos.
\end{obs}
Dicho esto, la \tb{regla lexicográfica} dicta que saldrá de la base la columna $l$--ésima si y solo si se verifica
\begin{equation}
\label{simp_eq_lexico}
	\begin{array}{cc}
		y_{lk}>0 \qquad&\qquad \displaystyle{\frac{V_B^i}{y_{ik}}>_L\frac{V_B^l}{y_{lk}}}
	\end{array}
\end{equation}
para todo $i\in\{1,\dots,m\}\setminus\{l\}$ con $y_{ik}>0$.

Antes de continuar presentemos dos observaciones interesantes.
\begin{obs}[Generalización]
	Nótese que la regla lexicográfica no es más que una generalización de la regla de la razón mínima, en la que en lugar de hacer una comparación lexicográfica de los vectores $V_B^i\frac{1}{y_{ik}}$ y $V_B^l\frac{1}{y_{lk}}$, únicamente se comparaban sus primeras componentes.
	
	Esto sugiere una implementación natural de la comparación lexicográfica, que únicamente necesita comparar $V_B^i\frac{1}{y_{ik}}$ y $V_B^l\frac{1}{y_{lk}}$ componente a componente hasta que la comparación arroje un resultado no nulo.
\end{obs}
\begin{obs}[Imposibilidad de empates]
	Es interesante observar también que la regla lexicográfica no permite los empates, ya que, en caso de que varios vectores empataran se dería la situación
	\begin{equation*}
		\frac{V_B^i}{y_{ik}}>_L\frac{V_B^l}{y_{lk}}\sii \frac{y_{lk}}{y_{ik}}V_B^i=V_B^l
	\end{equation*}
	Luego $B^{-1}$ tendría dos filas proporcionales (lo cual va contra su invertivilidad).
\end{obs}
Para finalizar, comprobemos que, efectivamente, la regla lexicográfica es una regla de prevención de ciclos, para lo cual necesitamos introducir una definición.
\begin{defi}[Valor vectorial lexicográfico]
	Definimos el \tbi{valor vectorial lexicográfico} asociado a la base $B$ como el vector $v_B:=\sum_{i=1}^{m}c_iV_B^i$.
\end{defi}
La idea de la demostración será comparar los valores lexicográficos de una base $B$ y el de la base $B'$ resultante de aplicar el cambio de base según la regla lexicográfica, viendo que $v_{B'}<_Lv_B$, siendo imposible que el algoritmo analice dos veces una misma base.

Antes de pasar a demostrar esto, observemos la relación que hay entre $V_{B'}^i$ y $V_B^i$. Símplemente fijándonos en las fórmulas del pivotaje tenemos que
\begin{equation}
\label{simp_eq_cambioLexico}
	\begin{array}{cc}
		\displaystyle{V_{B'}^k=\frac{V_B^l}{y_{lk}}}\qquad&\qquad \displaystyle{V_{B'}^i=V_B^i-\frac{y_{ik}}{y_{lk}}V_B^l}
	\end{array}
\end{equation}
\begin{obs}[Positividad]
	Nótese además que si la primera base considerada en el algoritmo es la identidad, es claro que $V_{I_m}^i>_L0$, pero no solo eso, la aplicación de la regla lexicográfica en este supuesto asegura que $V_B^i>_L0$ para toda base considerada.
	
	Para probar esto basta echar un vistazo a las ecuaciones \eqref{simp_eq_lexico} y \eqref{simp_eq_cambioLexico} (se deja al lector).
\end{obs}
Dicho esto, ya estamos listos para demostrar lo que queremos.
\begin{prop}[Buena definición]
	La regla lexicográfica es una regla de prevención de ciclos.
\end{prop}
\begin{proof}
	Basta desarrollar la siguiente cuenta
	\begin{multline*}
		v_{B'}:=\sum_{\substack{i=1\\i\not=l}}^{m}c_iV_{B'}^i+c_kV_{B'}^k=\sum_{\substack{i=1\\i\not=l}}^{m}c_i\left(V_B^i-\frac{y_{ik}}{y_{lk}}V_B^l\right)+c_k\frac{V_B^l}{y_{lk}}=\\=\sum_{\substack{i=1\\i\not=l}}^{m}c_iV_B^i+\frac{V_B^l}{y_{lk}}\left(c_k-\sum_{\substack{i=1\\i\not=l}}^{m}c_iy_{ik}\right)=\sum_{\substack{i=1\\i\not=l}}^{m}c_iV_B^i+\overline{c_k}\frac{V_B^l}{y_{lk}}<_L\sum_{i=1}^{m}c_iV_B^i=:v_B\qedhere
	\end{multline*}
\end{proof}
\subsection{Regla de Bland}
Presentamos a continuación otra regla de prevención de ciclos, debida a Robert G. Bland (quien la publicó en $1977$), y que, como es evidente, es conocida como \tbi[regla!de Bland]{regla de Bland}. Esta regla es muy sencilla y constituye un criterio de entrada y salida a la base.

Lo que dice la regla es lo siguiente: de todas las columnas candidatas a entrar en la base escogemos la de menor índice. Análogamente, de todas las columnas candidatas a salir de la base (que empatan con la regla de la razón mínima), escogemos también la de menor índice.  

Probemos pues que la regla de Bland es una regla de prevención de ciclos.
\begin{prop}[Buena definición]
	El algoritmo del Símplex implementado con la regla de Bland siempre termina.
\end{prop}
\begin{proof}
	Supongamos que se produce un ciclo. Es decir, el algoritmo examina las bases $B_0,\dots,B_k$ y tras esto se vuelve a exmaminar la base $B_0$.
	
	Dicho esto llamaremos \tbi[variable!volátil]{volátiles} a las columnas que pertenecen a alguna base del ciclo, pero no a todas. Consideremos la columna volátil de mayor índice, a la que llamaremos $a_t$.
	
	Por ser $a_t$ volátil, habrá alguna base $B$ para la cual $a_t$ sea seleccionada para entrar en la base. Es claro que se debe cumplir que $\overline{c_t}<0$, ya que si no, no sería candidata a entrar en la base. Además, por la implementación de la regla de Bland, las demás columnas no asociadas a la base deben cumplir que $\overline{a_j}\geq 0$.
	
	Asimismo, por la volatilidad de $a_t$, habrá alguna base $B'$ para la cual $a_t$ sea seleccionada para salir de la base, siendo sustituida por la columna $a_s$. En estas circunstancias se debe cumplir que $y'_{ts}>0$, además, $\overline{c_s'}<0$ y $s<t$ (por la regla de Bland).
	
	El punto extremo asociado a la base posterior a $B'$ es $x'=\overline{x}+\alpha d$ (ver teorema \ref{fund_teo_mejoraSimplex}), siendo $d=((-y_s|0)+e_s)^t$. Claramente $d$ es una dirección extrema, por tanto, se verifica que $Ad=0$, es decir, $d$ está en el complemento ortogonal del espacio de filas de $A$.
	
	Sabiendo que $\overline{c}=c-c_B^tB^{-1}A$ y $\overline{c'}=c-c_{B'}^tB'^{-1}A$ tenemos que 
	\begin{equation*}
		c^*:=\overline{c}-\overline{c'}\stackrel{!}{=}(c_{B'}^tB'^{-1}-c_B^tB^{-1})A
	\end{equation*}
	luego $c^*$ está en el espacio de filas de $A$, lo que quiere decir que $d$ es ortogonal a $c^*$. En otras palabras $(c^*)^td=0$. No obstante además tenemos que
	\begin{equation*}
		(c^*)^td=c^*_sd_s+c^*_td_t+\sum_{\text{no volátiles en }B'}c^*_jd_j+\sum_{\substack{\text{volátiles en }B'\\j\not=t}}c^*_jd_j
	\end{equation*}
	El sumatorio asociado a las columnas no volátiles se anula ya que $c_j^*=0$, los costes reducidos siempre se anulan por estar siempre en la base. Por su lado, el sumatorio asociado a las columnas volátiles es no negativo ya que $d_j\geq 0$ (en caso contrario estaríamos quebrantando la regla de Bland) y $c_j^*=\overline{c_j}\geq 0$ (como vimos antes). Además como $d_s=1$, $\overline{c_s}\geq 0$ y $-\overline{c_s'}>0$ tenemos que el primer sumando es positivo. Finalmente, como tanto $d_t$ como $c_t^*$ son negativos se tiene que el segundo sumando es positivo.
	
	En definiva $(c^*)^td>0$, lo cual es absurdo, con lo que queda demostrado que los ciclos son imposibles.
\end{proof}
\begin{obs}[Desventajas]
	La regla de Bland es maravillosa por su sencillez, no obstante, en la práctica puede llegar a ser peligrosa, pues suele conducir a realizar más pivotajes de los que serían necesarios usando otros criterios, como el lexicográfico.
\end{obs}