%Para este capítulo se usará la abreviatura "fund".
\chapter{Fundamentos teóricos}
\label{fund}
La disciplina de la programación lineal cogió fuerza en la segunda guerra mundial, pues era necesario optimizar los escasos recursos de los que se disponían para realizar operaciones militares. En un principio fue impulsada por el ejército británico, con un grupo de investigación dirigido por \ti{\tb{Blackett}}, quien más adelante ganaría el premio Nobel de física por otros motivos.

Cabe destacar la gran importancia que tuvo la programación lineal en países como Albania, debido, entre otras cosas, a su utilidad a la hora de optimizar los procesos de producción de carbón. 
\section{Planteamiento del problema}
De ahora en adelante, consideraremos fijadas las bases canónicas tanto de $\R^n$ como de $\R$, a no ser que se especifique lo contrario.

Antes de plantear formalmente el problema de la programación lineal, introduzcamos unas definiciones.

\begin{defi}[Vector no negativo]
	Un vector $x=(x_1,\dots,x_n)\in\R^n$ se dice \tbi[vector!no negativo]{no negativo} si todas sus componentes son, como su propio nombre indica, no negativas. Es decir, $x_i\geq 0$ para todo $i\in\{1,\dots,n\}$. Si un vector $x$ es no negativo escribiremos $x\geq 0$.
\end{defi}
Al hilo de esta definición, vamos a generalizar ese concepto que lleva presente en nuestras vidas desde que aquel docente de primaria nos dibujó dos rectas perpendiculares en la pizarra, los cuadrantes.

\begin{defi}[$2^n$--ante positivo]
	Definimos el \tbi{$2^n$--ante positivo} de $\R^n$ como el conjunto de los vectores no negativos de $\R^n$.
	
	Se deja como ejercicio al lector justificar el nombre de $2^n$--ante.
\end{defi}
Dicho esto, ya podemos formular el problema de la programación lineal.
\begin{prob}[Formulación general]
	\label{fund_prob_formulacionGeneral}
	La disciplina de la \tbi[programación!lineal]{programación lineal} estudia procedimientos (implementables en ordenador) para calcular los extremos absolutos de una aplicación lineal $f:\R^n\to\R$ cuando restringimos su dominio a una variedad afín de $\R^n$ cortada con el $2^n$--ante positivo.
	
	\begin{figure}[h!]
		\centering
		\includegraphics[scale = 0.75]{img/problemaGeneral}
		\caption{Ilustración del problema de programación lineal.}
		\label{fund_img_problemaGeneral}
	\end{figure}
	En el caso de la ilustración \ref{fund_img_problemaGeneral} suponemos que $f$ es una aplicación lineal definida sobre todo el plano a la cual restringimos el dominio a la variedad afín $\mc{L}$ cortada con el cuadrante positivo (línea azul). Nuestro deber es encontrar los puntos (si los hay) sobre la línea azul en los cuales $f$ alcanza un máximo o un mínimo absoluto.
\end{prob} 
En esta sección nos dedicaremos simplemente a formular de diversas maneras el problema de la programación lineal, observando que todas son equivalentes entre sí. Comencemos pues, sin más dilación nuestro viaje por lo desconocido, un viaje del que probablemente no regresaremos.

A continuación vemos una sencilla caracterización de las aplicaciones lineales $f:\R^n\to\R$ a partir de su expresión analítica.
\begin{obs}[Expresión analítica]
	Consideremos una aplicación lineal $f:\R^n\to\R$.
	
	Por ser $f$ una aplicación lineal, tendrá una única matriz asociada $C$. Como salta a la vista, $C$ será una matriz fila con $n$ columnas.
	
	Entonces, dado un vector $x\in\R^n$ se tiene que
	\begin{equation}
	\label{fund_eq_lineales}
	f(x)=f(x_1,\dots,x_n)= CX=\begin{pmatrix}
	c_1 & \cdots & c_n
	\end{pmatrix}\begin{pmatrix}
	x_1\\ \cdots \\x_n
	\end{pmatrix}=c_1x_1+\cdots+c_nx_n
	\end{equation}
	De esta forma se concluye que toda aplicación lineal $f:\R^n\to\R$ tiene una expresión analítica de la forma de la ecuación \eqref{fund_eq_lineales}. Recíprocamente, toda función $g:\R^n\to\R$ con una expresión analítica como la de \eqref{fund_eq_lineales} es una ecuación lineal, para demostrarlo, basta con leer como los árabes la ecuación \eqref{fund_eq_lineales}.
\end{obs}
Antes de continuar es importante hacer un inciso acerca de la notación.
\begin{obs}[Notación matricial]
	Siempre que escribamos matrices fila o columna lo haremos con letras minúsculas. Asimismo, siempre que estemos escribiendo una matriz fila añadiremos el símbolo de trasposición, para dejarlo claro implícitamente. De esta manera, a las funciones lineales las denotaremos por
	\begin{equation*}
		f(x_1\dots,x_n)=c^tx
	\end{equation*}
	donde $c^t$ es la matriz asociada a $f$ y $x$ es el vector columna con las coordenadas de $x$ respecto de la base canónica (nótese el pequeño abuso de notación).
\end{obs}
\subsection{Forma estándar y formas canónicas}

Sea $f(x_1,\dots,x_n)=c_1x_1+\cdots+c_nx_n=c^tx$ una función lineal  a la que a partir de ahora llamaremos \tbi[función!objetivo]{función objetivo}. A la matriz $c$ se la denomina \tbi[vector!de coeficientes de la función objetivo]{vector de coeficientes de la función objetivo}, mientras que $x$ recibe el nombre de \tbi[vector!de variables de decisión]{vector de variables de decisión}.

Consideremos asimismo la variedad afín de $\R^n$ dada por el conjunto de soluciones al sistema no necesariamente homogéneo $Ax=b$. Escrito de forma desarrollada
\begin{equation*}
\begin{array}{cc}
\left\{\begin{array}{cccc}
a_{11}x_1&+\cdots+&a_{1n}x_n&=b_1\\
\vdots &\ddots &\vdots &\vdots\\
a_{1m}x_1&+\cdots+&a_{mn}x_n&=b_m
\end{array}\right.\qquad&\qquad\begin{pmatrix}
a_{11} & \cdots & a_{1n}\\
\vdots & \ddots & \vdots\\
a_{m1} & \cdots & a_{mn}
\end{pmatrix}\begin{pmatrix}
x_1\\
\vdots\\
x_n
\end{pmatrix}=\begin{pmatrix}
b_1\\
\vdots\\
b_m
\end{pmatrix}
\end{array}
\end{equation*}
A la matriz $A\in\mathfrak{M}_{m\times n}(\R)$ usualmente la llamaremos \tbi[matriz!de coeficientes de las restricciones]{matriz de coeficientes de las restricciones}, además, a cada una de las ecuaciones del sistema las llamaremos \tbi{restricciones}. Asimismo, el vector $b$ será conocido como \tbi[vector!de términos independientes]{vector de términos independientes}.

Expongamos ahora la llamada ``\ti{formulación estándar}'' del problema de la programación lineal.

\begin{prob}[Forma estándar]
El problema de programación lineal en \tbi[forma!estándar]{forma estándar} consiste en hallar los vectores $x\geq 0$ tales que sean solución de $Ax=b$ y además sean mínimos absolutos de la función $f$. Usualmente se escribe de forma sintética (aunque no lo usaremos mucho)
\begin{equation*}
\begin{array}{c}
\min c^tx\\
\text{Sujeto a:}\qquad Ax=b,\qquad x\geq 0
\end{array}
\end{equation*}
Nótese que la formulación del problema en forma estándar es exactamente la misma que la formulación general \ref{fund_prob_formulacionGeneral} con la única diferencia de que ya no buscamos extremos absolutos en general, sino únicamente mínimos. 
\end{prob}
La formulación estándar del problema de programación lineal puede parecer muy inflexible, en el sentido de que, a simple vista, no parece ser demasiado útil para modelizar problemas reales. Es esta aparente rigidez la que lleva a plantearse otras formulaciones más laxas del problema de programación lineal. Las llamadas ``\ti{formulaciones canónicas}'' que exponemos a continuación.
\begin{prob}[Primera forma canónica]
	Esta nueva formulación a la que llamamos \tbi[forma!canónica primera]{primera forma canónica} plantea el problema de hallar los vectores $x\geq 0$ tales que satisfagan la ecuación $Ax\leq b$ y que además sean máximos absolutos de la función objetivo $f$. Escrito de forma compacta
	\begin{equation*}
	\begin{array}{c}
	\max c^tx\\
	\text{Sujeto a:}\qquad Ax\leq b,\qquad x\geq 0
	\end{array}
	\end{equation*}
\end{prob}
\begin{prob}[Segunda forma canónica]
	El problema de programación lineal en \tbi[forma!canónica segunda]{segunda forma canónica} consiste en encontrar los vectores $x\geq 0$ tales que satisfagan la ecuación $Ax\geq b$ y que además sean mínimos absolutos de la función objetivo $f$. En resumen
	\begin{equation*}
	\begin{array}{c}
	\min c^tx\\
	\text{Sujeto a:}\qquad Ax\geq b,\qquad x\geq 0
	\end{array}
	\end{equation*}
\end{prob}
Una vez vistas todas las formulaciones, hagamos una pequeña reflexión. Sería la apoteosis del tedio tener que demostrar resultados teóricos para cada una de las formulaciones. No obstante, no hay que dejar nunca de confiar en los dioses olímpicos, pues a veces son benévolos con nosotros. Esta benevolencia consiste en que todas los formulaciones son equivalentes en el sentido que veremos a continuación.
\subsubsection{Maximización y minimización}
Si se nos plantea el problema de hallar el máximo absoluto de una función objetivo $f$ podemos transformar este problema en uno equivalente de forma que el objetivo de el nuevo problema sea hallar el mínimo de otra función objetivo $g$.

La utilidad de este hecho es que podemos transformar un problema de programación lineal en primera forma canónica (que surgen de forma natural a la hora de modelizar problemas reales) en un problema más parecido a la formulación estándar.
\begin{lem}[Maximización--minimización]
	\label{fund_lem_maxmin}
	Sea $A$ un conjunto arbitrario y una función $f:A\to\R$ que alcanza el máximo y el mínimo. Entonces se cumple
	\begin{equation*}
		\max f=-\min (-f)
	\end{equation*}
\end{lem}
\begin{proof}
	Sea $\xi$ el máximo de $f$, es decir, el único número de $f(A)$ que verifica que $f(x)\leq \xi$ para todo $x\in A$. Consideremos ahora la función $g\equiv -f$, que, por las propiedades de los números reales alcanza el mínimo, al que llamaremos $\eta$.
	
	Evidentemente $\eta$ verifica que $g(x)\geq \eta$ para todo $x\in A$, o equivalentemente $-g(x)\leq -\eta$ para todo $x\in A$. Pero $-g(x)=f(x)$, luego $f(x)\leq -\eta$ para todo $x\in A$. Por unicidad del máximo $-\eta =\xi$, como queríamos demostrar.
\end{proof}
\begin{obs}[Aplicación]
	Si tenemos un problema de programación lineal formulado en términos de maximización bastará con cambiar la función objetivo $f$ por la función objetivo $-f$ para obtener un problema equivalente en términos de minimización.
	
	Para obtener las soluciones del primer problema en términos del segundo bastará con cambiar el valor por la función objetivo de las soluciones del segundo problema por su opuesto.
\end{obs}
\subsubsection{Igualdades y desigualdades}
Una vez visto el apartado anterior, vamos a aprender a trasformar restricciones de tipo desigualdad a restricciones de tipo igualdad, con lo cual ya podremos transformar cualquier problema planteado en alguna forma canónica a un problema equivalente planteado en forma estándar.
\begin{lem}[Variables de holgura]
	Sea una restricción de la forma
	\begin{equation}
	\label{fund_eq_rest1}
		a_{i1}x_1+\dots+a_{in}x_n\stackrel{(\geq)}{\leq} b_i
	\end{equation}
	entonces la restricción
	\begin{equation}
	\label{fund_eq_rest2}
		a_{i1}x_1+\dots+a_{in}x_n\stackrel{(-)}{+}x_i^h=b_i
	\end{equation}
	es equivalente, en el sentido de que hay una biyección ``natural'' entre los conjuntos de vectores \tb{no negativos} que verifican cada una de las restricciones.
	
	A la variable $x_i^h$ añadida en la restricción \eqref{fund_eq_rest2} se la denomina \tbi[variable!de holgura]{variable de holgura}.
\end{lem}
\begin{proof}
	Sean los conjuntos $\mc{S}$ y $\mc{S}'$ de vectores que verifican las restricciones \eqref{fund_eq_rest1} y \eqref{fund_eq_rest2} respectivamente. Consideremos la aplicación
	\begin{equation*}
		\begin{array}{cc}
		\varphi:&\mc{S}\to\mc{S}'\\
		& (x_1,\dots,x_n)\mapsto (x_1,\dots,x_n,z)
		\end{array}
	\end{equation*}
	donde $z=b_i-a_{i1}x_1+\dots+a_{in}x_n$. Veamos que es una biyección.
	
	La inyectividad es clara por definición, si dos vectores compartieran imagen todas sus componentes coinciden.
	
	En cuanto a la sobreyectividad, dado un vector $(x_1,\dots,x_n,x_{n+1})$ que verifica la restricción \eqref{fund_eq_rest2}, es claro que el vector resultante de eliminar la última componente, $(x_1,\dots,x_n)$, cumple la restricción \eqref{fund_eq_rest1}.
	
	La demostración para el caso $\geq$ es totalmente simétrica.
\end{proof}
Veamos que utilidad tiene lo que acabamos de demostrar.
\begin{obs}[Aplicación]
	Si a la hora de modelizar un problema nos aparecen restricciones de tipo $\geq$ o $\leq$ basta con sustituirlas por restricciones equivalentes añadiendo variables de holgura.
	
	Además, debemos cambiar la función objetivo $f$, extendiéndola de la siguiente manera
	\begin{equation*}
		\begin{array}{cc}
		\widehat{f}:&\R^{n+m}\to \R\\
		& (x_1,\dots,x_n,x_{n+1},\dots,x_{n+m})\mapsto f(x_1,\dots,x_n)
		\end{array}
	\end{equation*}
	donde $n$ es el número total de restricciones originales y $m$ el número de restricciones de tipo desigualdad.
	 
	Para obtener las soluciones del problema original en términos de este nuevo problema equivalente basta con quitar las componentes correspondientes a las variables de holgura.
\end{obs}
Con lo que sabemos hasta el momento podemos transformar cualquier problema escrito en cualquiera de las formas canónicas en un problema en forma estándar.
\subsubsection{Positividad y negatividad}
Otro caso que se presenta de manera usual es que al modelizar un problema concreto sea necesario permitir que algunas componentes de los vectores candidatos a ser solución sean negativas. Esta es una situación que no contempla la formulación estándar, sin embargo, con un simple truco podemos meter esta clase de problemas dentro de nuestro marco de actuación.
\begin{obs}[Truco]
	Si tenemos un problema en el que la componente $i$--ésima de los vectores puede ser negativa tomamos todas las restricciones
	\begin{equation*}
	a_{j1}x_1+\dots+a_{ji}x_i+\dots+a_{jn}x_n=b_j
	\end{equation*}
	que involucren a la componente $x_i$ y las sustituimos por las restricciones
	\begin{equation*}
	a_{j1}x_1+\dots+a_{ji}(x_i^+-x_i^-)+\dots+a_{jn}x_n=b_j
	\end{equation*}
	con $x_i^+$ y $x_i^-$ no negativos.
	
	Es claro que si un vector $(x_1,\dots,x_i,\dots,x_n)$ (donde $x_i$ es negativo) verifica las restricciones originales, entonces el vector $(x_1,\dots,0,-x_i,\dots,x_n)$ (por ejemplo) verifica las restricciones nuevas. Análogamente si $x_i$ es positivo. Asimismo, si el vector $(x_1,\dots,x_i^+,x_i^-,\dots,x_n)$ cumple las restricciones nuevas, el vector $(x_1,\dots,x_i^+-x_i^-,\dots,x_n)$ cumplirá las viejas, pudiendo concluir así que las restricciones viejas y nuevas son equivalentes en un sentido un poco más laxo que en el apartado anterior, ya que únicamente se tiene la sobreyectividad (pero esto es irrelevante).
	
	De esta forma, si cambiamos la función objetivo $f$ por la función
	\begin{equation*}
		\begin{array}{cc}
		\overline{f}:&\R^{n+1}\to \R\\
		& (x_1\dots,x_i^+,x_i^-,\dots,x_n)\mapsto f(x_1,\dots,x_i^+-x_i^-,\dots,x_n)
		\end{array}
	\end{equation*}
	obtenemos un problema equivalente.
	
	Para obtener la solución del problema original en términos de este nuevo problema basta considerar el vector $(x_1,\dots,x_i^+-x_i^-,\dots,x_n)$ donde $(x_1\dots,x_i^+,x_i^-,\dots,x_n)$ es el vector solución del problema con las restricciones nuevas.
\end{obs}
Hechas todas estas disquisiciones iniciales cabe mencionar que a la hora de modelizar problemas, siempre que tanto la función objetivo como las restricciones sean lineales, podremos realizar transformaciones (las vistas en esta sección) para que el problema quede planteado en forma estándar, pudiendo ser así resuelto mediante el uso de toda la artillería teórica que veremos a continuación.
\subsection{Terminología y casuística}
Dado un problema de programación lineal en forma estándar que tiene a $A$ como matriz de coeficientes de las restricciones y a $b$ como vector de términos independientes. Exponemos las siguientes definiciones.
\begin{defi}[Soluciones]
	Llamamos \tbi{solución} del problema $P$ a cualquier vector $x\in\R^n$ (no necesariamente no negativo) que verifique $Ax=b$.
	
	Sacando un poco de punta al asunto, diremos que si $x\in\R^n$ es solución del problema y además $x\geq 0$ entonces $x$ es una \tbi[solución!factible]{solución factible}.
	
	Llamaremos \tbi[región!factible]{región factible} al conjunto de todas las soluciones factibles del problema. Normalmente la denotaremos con la letra $\mc{S}$.
\end{defi}
Una vez resuelto el problema se pueden dar las siguientes situaciones
\begin{itemize}
	\item La región factible posee al menos un punto en el cual la función objetivo alcanza el mínimo. En esta situación diremos que el problema posee \tbi[solución!óptima]{solución óptima}.
	\item La región factible del problema es el conjunto vacío. En este caso se dirá que el problema es \tbi[problema!infactible]{infactible}.
	\item La función objetivo alcanza valores tan bajos como queramos en la región factible. En este caso se dice que el problema posee una solución \tbi[solución!no acotada]{no acotada}.
\end{itemize}
A lo largo del capítulo veremos por qué no se puede dar ninguna otra situación.

Teniendo en cuenta estas pequeñas observaciones podemos empezar a estudiar cómo son las regiones factibles, lo cual nos será muy útil para resolver el problema de programación lineal.
\section{Programación lineal y conjuntos convexos}
En esta sección demostraremos que las regiones factibles de un problema de programación lineal en forma estándar son siempre conjuntos convexos. 

Asimismo, estudiaremos en profundidad las propiedades de estos conjuntos, explotándolas para obtener un procedimiento robusto con el cual resolver estos problemas.
\subsection{Conjuntos convexos. Definición y propiedades}
Comenzamos definiendo el concepto de conjunto convexo.
\begin{defi}[Convexos]
	Sea $\mc{S}$ un subconjunto de $\R^n$, se dice \tbi[conjunto!convexo]{convexo} si para cada par de puntos $x,y\in \mc{S}$ el segmento que tiene por extremos a $x$ e $y$ está contenido en $\mc{S}$. Es decir
	\begin{equation*}
		\lambda x + (1-\lambda)y\in\mc{S}
	\end{equation*}
	para todo $\lambda\in[0,1]$. Nótese que el conjunto vacío se considera convexo.
\end{defi}
La región factible de un problema de programación lineal en forma estándar es un conjunto convexo.
\begin{lem}[Región convexa]
	El conjunto de los vectores no negativos $x\in\R^n$ que verifican la condición $Ax=b$ para cierta matriz $A$ y cierto vector $b$ es un conjunto convexo.
\end{lem}
\begin{proof}
	Si la región factible $\mc{S}$ es el conjunto vacío ya hemos terminado. En otro caso, basta considerar dos vectores $x$ e $y$ de la región factible y ver que $\alpha x + (1-\alpha) y\in \mc{S}$ para todo $\alpha\in[0,1]$, pero esto es inmediato ya que
	\begin{equation*}
		A(\alpha x + (1-\alpha) y)=\alpha Ax + (1-\alpha)Ay=\alpha b + (1-\alpha)b = b \qedhere
	\end{equation*}
\end{proof}

Una cuestión importante a destacar es que la intersección arbitraria de conjuntos convexos es convexo, tal y como muestra el siguiente lema.
\begin{lem}[Intersección]
	Sea $\mc{F}$ una familia de conjuntos convexos. La intersección de la familia es un conjunto convexo.
\end{lem}
\begin{proof}
	Sean $x$ e $y$ dos puntos de la intersección de la familia, luego $x$ e $y$ están en todos los conjuntos de la familia. Como estos conjuntos son convexos, el segmento que une $x$ e $y$ estará contenido en todos los conjuntos de la familia simultáneamente, luego también en la intersección de la familia.
\end{proof}
Un concepto por el que merece la pena pasar es el de ``envoltura convexa'' de un conjunto $\mc{S}$ de $\R^n$, que viene a ser el menor conjunto convexo que contiene a $\mc{S}$.

Esta idea es especialmente recurrente en las matemáticas, por ejemplo, en álgebra lineal teníamos el subespacio generado por un subconjunto, en teoría de grupos el subgrupo generado por un subconjunto, en topología tenemos la adherencia de un conjunto,\dots La lista es interminable.
\begin{obs}[Justificación]
	Es fácil darse cuenta de que, como la intersección arbitraria de convexos es convexa, el menor conjunto convexo que contiene a uno dado (al que llamaremos $\mc{S}$) puede ser construido de la siguiente manera
	\begin{equation*}
	\text{Conv}(\mc{S}):=\bigcap_{\mc{F}\supset \mc{S}}\mc{F}
	\end{equation*}
	En efecto, es un conjunto que contiene a $\mc{S}$, ya que todos los conjuntos de la familia a intersecar contienen a $\mc{S}$, además, es el menor de ellos, ya que, de haber uno más pequeño, pertenecería a la familia que se está intersecando, lo cual es absurdo (¡compruébese!).
	
	De esta forma queda justificada la existencia del menor conjunto convexo que contiene a $\mc{S}$, no obstante, la construcción que hemos realizado nos da escasa información acerca de los elementos de la envoltura convexa.
\end{obs}
\begin{obs}[Curiosidad]
	A modo de curiosidad comentamos que el problema de calcular la envoltura convexa de un conjunto finito de $n$ puntos en el plano o el espacio es uno de los problemas centrales de la \tbi[geometría!computacional]{geometría computacional} por sus múltiples aplicaciones.
\end{obs}
Veamos a continuación una caracterización útil de la envoltura convexa de un conjunto, para la cual necesitamos introducir una definición.
\begin{defi}[Combinaciones convexas]
	Sea $\{z_1,\dots,z_r\}$ un conjunto finito de puntos. Diremos que $z$ es \tbi[combinación!lineal convexa]{combinación lineal convexa} de dichos puntos si
	\begin{equation*}
		z=\lambda_1 z_1+\dots+\lambda_r z_r
	\end{equation*}
	donde $\lambda_i>0$ para $i\in\{1,\dots,r\}$ y se cumple que $\sum_{i=1}^{r}\lambda_i=1$.
\end{defi}
Que no asuste la longitud de la demostración de la proposición pues realmente es muy sencilla.
\begin{prop}[Caracterización]
	Dado un conjunto $\mc{S}$, su envoltura convexa es el conjunto de puntos que son una combinación lineal convexa de puntos de $\mc{S}$.
\end{prop}
\begin{proof}
	Llamemos $\mc{H}(\mc{S})$ al conjunto de puntos que son combinación lineal convexa de puntos de $\mc{S}$. Veamos en primer lugar que $\mc{H}(\mc{S})$ contiene a $\mc{S}$. Esto es claro, ya que todo punto $x$ de $\mc{S}$ es combinación lineal convexa de puntos de $\mc{S}$, en efecto, $x=1\cdot x$.
	
	Veamos ahora que $\mc{H}(\mc{S})$ es convexo. Esto lo haremos simplemente usando la definición. Sean $x$ e $y$ dos puntos de $\mc{H}(\mc{S})$, veamos que $\alpha x + (1-\alpha)y\in \mc{H}(\mc{S})$ para todo $\alpha\in[0,1]$.
	
	Por definición de $\mc{H}(\mc{S})$
	\begin{equation*}
	\begin{array}{cc}
	x=\sum_{i=1}^{r}\lambda_ix_i \qquad&\qquad 
	y=\sum_{j=1}^{s}\mu_jy_j
	\end{array}
	\end{equation*}
	por tanto, si sustituimos esto en $\alpha x + (1-\alpha)y$ obtenemos
	\begin{equation*}
		\alpha x + (1-\alpha)y=\sum_{i=1}^{r}(\alpha\lambda_i)x_i+\sum_{j=1}^{s}[(1-\alpha)\mu_j]y_j
	\end{equation*}
	Haciendo los siguientes cambios de nombre
	\begin{equation*}
	\begin{array}{cc}
	\xi_k:=\begin{cases}
	\alpha\lambda_k & \text{si } k\in\{1,\dots,r\}\\
	(1-\alpha)\mu_{k-r} & \text{si } k\in\{r+1,\dots,r+s\}
	\end{cases}
	\qquad
	&
	\qquad
	z_k:=\begin{cases}
	x_k & \text{si } k\in\{1,\dots,r\}\\
	y_{k-r} & \text{si } k\in\{r+1,\dots,r+s\}
	\end{cases}
	\end{array}
	\end{equation*}
	nos queda que
	\begin{equation*}
		\alpha x + (1-\alpha)y=\sum_{k=1}^{r+s}\xi_kz_k
	\end{equation*}
	luego por definición de $\mc{H}(\mc{S})$ solo queda comprobar que $\sum_{k=1}^{r+s}\xi_k=1$, pero esto es evidente ya que
	\begin{equation*}
		\sum_{k=1}^{r+s}\xi_k=\alpha\sum_{i=1}^r\lambda_i+(1-\alpha)\sum_{j=1}^{s}\mu_j=\alpha+(1-\alpha)=1
	\end{equation*}
	Queda por ver pues que $\mc{H}(\mc{S})$ es el menor conjunto convexo que contiene a $\mc{S}$. Para ello, consideramos $\mc{C}$ un conjunto convexo arbitrario que contiene a $\mc{S}$. Demostremos que $\mc{H}(\mc{S})\subset C$ por inducción sobre la ``longitud'' de la combinación lineal convexa de sus elementos.
	
	El caso base es evidente, ya que los elementos de $\mc{H}(\mc{S})$ que son combinaciones lineales convexas de longitud unitaria son también elementos de $\mc{S}$ y por tanto de $\mc{C}$. Supongamos cierto que los elementos de $\mc{H}(\mc{S})$ que son combinaciones lineales convexas de longitud $n$ viven también en $\mc{C}$, y demostremos que lo mismo sucederá para los elementos que sean combinaciones de longitud $n+1$.
	
	Consideremos $z\in\mc{H}(\mc{S})$ un elemento que es combinación de longitud $n+1$, es decir
	\begin{equation*}
	\begin{array}{ccc}
		z=\sum_{i=1}^{n+1}\lambda_iz_i & \text{con} & \sum_{i=1}^{n+1}\lambda_i=1
	\end{array}
	\end{equation*}
	Vamos a apañar la expresión de $z$ para poder aplicar la hipótesis de inducción
	\begin{equation*}
		z=\sum_{i=1}^{n}\lambda_iz_i + \lambda_{n+1}z_{n+1}
	\end{equation*}
	Es claro que $z_{n+1}\in\mc{C}$ por ser una combinación de longitud uno. Sin embargo, el primer sumando no es una combinación lineal convexa ya que $\sum_{i=1}^{n}\lambda_i=1-\lambda_{n+1}\not=1$. Para arreglar esto vamos a multiplicar y a  dividir el primer sumando por $1-\lambda_{n+1}$
	\begin{equation*}
		z=(1-\lambda_{n+1})\sum_{i=1}^{n}\frac{\lambda_i}{1-\lambda_{n+1}}z_i+\lambda_{n+1}z_{n+1}
	\end{equation*}
	De esta forma el primer sumando (excluyendo el factor que lo multiplica) es una combinación lineal convexa de longitud $n$, ya que $\frac{1}{1\lambda_{n+1}}\sum_{i=1}^{n}\lambda_i=\frac{1-\lambda_{n+1}}{1-\lambda_{n+1}}=1$.
	
	Por hipótesis de inducción, tanto el primer sumando como el segundo (excluyendo los factores que los multiplican) son elementos de $\mc{C}$, y como $\mc{C}$ es convexo $z\in\mc{C}$, como queríamos.
\end{proof}
\subsection{Puntos extremos}
De ahora en adelante será de utilidad tener en mente un polígono regular cuando se nos hable de conjuntos convexos, pues nos dará una idea bastante intuitiva.

Para estrenar esta nueva buena costumbre se presenta la siguiente definición.
\begin{defi}[Punto extremo]
	Sea $\mc{C}$ un conjunto convexo. Diremos que $\overline{x}\in\mc{C}$ es un \tbi[punto!extremo]{punto extremo} de $\mc{C}$ si se verifica que para cualquier par de puntos $x_1,x_2\in\mc{C}$, $\overline{x}$ \tb{no} está en el segmento generado por dichos puntos (excluyendo los extremos). Es decir
	\begin{equation*}
		\overline{x}=\lambda x_1+(1-\lambda)x_2\sii\begin{cases}
		\lambda\in\{0,1\}\\
		x_1=x_2
		\end{cases}
	\end{equation*}
\end{defi}
La ilustración \ref{fund_img_puntosExtremos} marca en negro los puntos extremos de un polígono regular, para coger una idea intuitiva. A lo largo de este capítulo veremos la importancia que tienen estos puntos en la resolución del problema de programación lineal.
\begin{figure}[h!]
	\centering
	\includegraphics[scale = 0.75]{img/puntosExtremos}
	\caption{Ilustración de los puntos extremos de un heptágono regular.}
	\label{fund_img_puntosExtremos}
\end{figure}
El teorema \ref{fund_teo_caracterizacionExtremos} nos da una caracterización de los puntos extremos de una región factible que, aunque muy potente, resulta poco intuitiva. Antes de enunciarlo recordemos un viejo resultado de álgebra lineal.
\begin{obs}[Rango y submatrices]
	\label{fund_obs_rango}
	Es conocido desde tiempos inmemoriales que $A\in\mf{M}_{m\times n}(\K)$ con $m\leq n$ y $\mathrm{rg}(A)=m$ entonces habrá al menos una submatriz cuadrada $B\in\mf{M}_{m}(\K)$ de $A$ tal que $\mathrm{rg}(B)=m$. De esta forma, reordenando las columnas de $A$ si fuera necesario, podemos hacer la siguiente división por bloques de $A$.
	\begin{equation*}
		A=(B|N)
	\end{equation*}
	Nótese que la igualdad se da solo cuando ya hemos reordenado $A$.
\end{obs}
A partir de ahora trabajaremos con matrices en las condiciones de la observación \ref{fund_obs_rango} a no ser que se especifique lo contrario. Nótese que esto no supone ninguna pérdida de generalidad en lo que se refiere al estudio de los problemas de programación lineal ya que si se nos presentara una matriz con más filas que columnas habrá restricciones redundantes.
\begin{defi}[Bases]
	Dado un problema en forma estándar que tiene a $A$ por matriz de coeficientes de las restricciones, llamaremos \tbi{base} de $A$ a toda submatriz suya cuadrada y de rango máximo.
\end{defi}
\begin{theo}[Caracterización]
	\label{fund_teo_caracterizacionExtremos}
	Dada una región factible $\mc{S}$, $\overline{x}$ es un punto extremo de $\mc{S}$ si y solo si $\overline{x}\geq 0$ y hay alguna reordenación de las columnas de $A$ de forma que
	\begin{equation*}
	\begin{array}{ccc}
	A=(B|N) \qquad&\qquad \overline{x}=\begin{pmatrix}
	\overline{x_B}\\
	\overline{x_N}
	\end{pmatrix}\text{ con } \overline{x_N}=0
	\end{array}
	\end{equation*}
	donde $B$ es una base de $A$ y $\overline{x_B}$ y $\overline{x_N}$ son los ``trozos'' de $\overline{x}$ acordes con la descomposición en bloques de $A$, es decir
	\begin{equation*}
		A\overline{x}=(B|N)\begin{pmatrix}
		\overline{x_B}\\
		\overline{x_N}
		\end{pmatrix}=B\overline{x_B}+N\overline{x_N}
	\end{equation*}
\end{theo}
\begin{proof}
	Veamos las dos implicaciones
	\begin{itemize}
		\item[\bla] Supongamos que $\overline{x}$ no es punto extremo, entonces habrá dos puntos distintos $x_1,x_2\in\mc{S}$ tales que $\overline{x}=\lambda x_1+(1-\lambda)x_2$ con $\lambda\in(0,1)$. Desplegando la descomposición por bloques tendríamos que
		\begin{equation*}
			(\overline{x_B}|\overline{x_N})^t=\lambda(x_B^1|x_N^1)^t+(1-\lambda)(x_B^2|x_N^2)^t
		\end{equation*}
		Como por hipótesis $\overline{x_N}=0$ y tanto $\lambda$ como $1-\lambda$ son estrictamente positivos se tiene que $x_N^1=x_N^2=0$, ya que $x_1$ y $x_2$ deben ser no negativos.
		
		Como $x_1,x_2\in\mc{S}$, entonces $Ax_i=b$ para $i\in\{1,2\}$, por tanto
		\begin{equation*}
			(B|N)(x_B^i|x_N^i)^t=(B|N)(x_B^i|0)^t=Bx_B^i=b\ra x_B^i=B^{-1}b
		\end{equation*}
		Con lo que se concluye que $x_1=x_2$, lo cual es absurdo.
		\item[\bra] Sea $\overline{x}$ un punto extremo. Reordenemos el vector de forma que todas sus componentes nulas queden al final, reordenando solidariamente las columnas de la matriz $A$.
		
		Podemos suponer pues que el vector $\overline{x}$ tiene $p\not=0$ componentes no nulas (si $p=0$ el resultado es evidente). Vamos a demostrar que las $p$ primeras columnas de $A$ (ya reordenada) son linealmente independientes. 
		
		A partir de ahora denotaremos a la matriz $A$ por sus columnas, es decir $A=(a_1\cdots a_n)$ donde $a_i$ representa la $i$--ésima columna de $A$.
		
		Procedamos por reducción al absurdo, es decir, supongamos que hay ciertos números reales no todos nulos $\lambda_i$ tales que $\sum_{i=1}^{p}\lambda_ia_i=0$.
		
		Definimos el vector $\lambda:=(\lambda_1,\dots,\lambda_p,0,\dots,0)\in\R^n$. A continuación consideramos los vectores \tb{no negativos} $x_1:=\overline{x}+\alpha\lambda$ y $x_2:=\overline{x}-\alpha\lambda$ tomando un $\alpha$ lo suficientemente pequeño para que ambos vectores sean precisamente no negativos, lo cual siempre puede hacerse al ser $\overline{x}$ no negativo.
		
		Los vectores $x_1$ y $x_2$ tienen la particularidad de que con ellos podemos obtener $\overline{x}$ como combinación lineal convexa. En efecto $\overline{x}=\frac{1}{2}x_1+\frac{1}{2}x_2$. Si resultara que $x_1$ y $x_2$ son puntos de la región factible estaríamos entrando en contradicción con la ``extremalidad'' de $\overline{x}$, y eso es exactamente lo que vamos a hacer. En efecto, con $i\in\{1,2\}$ tenemos
		\begin{equation*}
			Ax_i=A\overline{x}\pm \alpha A\lambda\stackrel{!}{=}A\overline{x}=b\ra x_i\in\mc{S}
		\end{equation*}
		donde la igualdad de la exclamación se debe a la composición de $\lambda$ y la dependencia lineal de las $p$ primeras columnas de $A$ (por hipótesis). Con esto llegamos a un absurdo y concluimos que las $p$ primeras columnas de $A$ son linealmente independientes.
		
		Además, de paso podemos concluir que $p\leq m$, ya que $m$ es el rango máximo de $A$. Si además $p=m$ ya tenemos una base $B=(a_1\cdots a_p)$ que cumple lo que queremos (¡compruébese!). En el caso de que $p<m$ la observación \ref{fund_obs_rango} nos dice que siempre podemos escoger columnas adicionales de $A$ para completar una base y, tras una nueva reordenación de $A$ si fuera necesario tendríamos lo que necesitamos.\qedhere
	\end{itemize}
\end{proof}
\begin{cor}[Parte básica]
	Dado un punto extremo $\overline{x}=(\overline{x_B}|0)^t$ de una región factible $\mc{S}$ se cumple que $\overline{x_B}=B^{-1}b\geq 0$.
\end{cor}
\begin{proof}
	Basta con echar las cuentas. Como $\overline{x}\in\mc{S}$ es claro que $A\overline{x}=b$, luego
	\begin{equation*}
		A\overline{x}=(B|N)(\overline{x_B}|0)^t=B\overline{x_B}=b
	\end{equation*}
	y despejando se tiene que $\overline{x_B}=B^{-1}b$.
\end{proof}
Al hilo de este teorema surge la siguiente definición.
\begin{defi}[Soluciones básicas]
	A los puntos extremos de una región factible se les suele llamar \tbi[solución!básica factible]{soluciones básicas factibles}. Esto es debido a que, por el teorema \ref{fund_teo_caracterizacionExtremos}, están ``asociados'' a una base de la matriz asociada a la región factible del problema.
\end{defi}
\begin{obs}[Procedimiento constructivo]
	Nótese que si nos dan un punto extremo de una región factible y nos piden hallar la base asociada a dicho punto extremo no tenemos más que seguir la demostración de la implicación a la derecha del teorema \ref{fund_teo_caracterizacionExtremos} para encontrarla, es decir, tomar las columnas de $A$ asociadas a las componentes no nulas del punto extremo y, en su caso, completar la base.
	
	Esto nos lleva a deducir de forma evidente que cada punto extremo puede estar asociado a más de una base (salvo reordenaciones).
\end{obs}
Una de las razones por la cual es estudio de los puntos extremos es importante es porque toda región factible de un problema de programación lineal en forma estándar tiene al menos un punto extremo, y además únicamente tiene una cantidad finita de ellos.
\begin{obs}[Finitud]
	\label{fund_obs_finitudExtremos}
	Una región factible tiene un número finito de puntos extremos, esto es debido a que $A$ tiene más bases que puntos extremos (hay una sobreyección que parte del conjunto de las bases y llega al conjunto de puntos extremos). Además, cada matriz tiene a lo sumo $\binom{n}{m}$ bases (salvo reordenaciones).
\end{obs}
\begin{theo}[Existencia]
	\label{fund_teo_existenciaExtremos}
	Toda región factible no vacía posee al menos un punto extremo.
\end{theo}
\begin{proof}
	Sea $\overline{x}$ un punto arbitrario de la región factible. Reordenemos sus componentes de forma que las componentes nulas queden al final, asimismo reordenemos $A$ de forma coherente con la reordenación de $\overline{x}$. Supongamos que $\overline{x}$ posee $p$ componentes no nulas (las $p$ primeras tras la reordenación).
	
	Ahora consideramos las columnas de $A$ correspondientes a las $p$ componentes no nulas de $\overline{x}$. Si estas columnas son linealmente independientes, por el teorema \ref{fund_teo_caracterizacionExtremos} habríamos terminado.
	
	Si, por el contrario, las columnas (que serán las $p$ primeras tras la reordenación) son linealmente dependientes, entonces existirán ciertos números reales $\lambda_i$ no todos nulos tales que $\sum_{i=1}^{p}\lambda_ia_i=0$, donde $a_i$ representa a la $i$--ésima columna de $A$.
	
	Lo que vamos a hacer es cambiar ligeramente el punto $\overline{x}$ de forma que siga siendo de la región factible y conserve un subconjunto estrictamente menor de las componentes no nulas de $\overline{x}$.
	
	En primer lugar cabe mencionar que siempre podemos suponer que existe al menos un $r\in\{1,\dots,p\}$ de forma que $\lambda_p >0$, ya que en caso de que todos los números de la combinación lineal fueran negativos podemos considerar la combinación lineal alternativa $\sum_{i=1}^{p}-\lambda_ia_i=0$, en la cual todos los escalares son positivos.
	
	Definimos ahora el vector $\lambda:=(\lambda_1,\dots,\lambda_p,0,\dots,0)\in\R^n$ y consideramos el puntos $\overline{x}':=\overline{x}-\alpha\lambda$ para cierto $\alpha>0$ que definiremos más adelante.
	
	El punto $\overline{x}'$ verifica la condición $Ax=b$, en efecto
	\begin{equation*}
		A\overline{x}'=A\overline{x}-\alpha A\lambda=A\overline{x}=b
	\end{equation*}
	Luego únicamente queda elegir un $\alpha$ adecuado para que $\overline{x}'\geq 0$. Para ello elegiremos
	\begin{equation*}
		\alpha:=\min\left\{\frac{\overline{x}_j}{\lambda_j}\midc \lambda_j>0\right\}=\frac{\overline{x}_l}{\lambda_l}
	\end{equation*}
	Veamos que $\overline{x}'$ es no negativo. Para ello distinguiremos casos componente a componente.
	\begin{itemize}
		\item Si $\lambda_j > 0$ tenemos que $\overline{x}_j'=\overline{x}_j-\frac{\overline{x}_l}{\lambda_l}\lambda_j=\frac{\overline{x}_j}{\lambda_j}\lambda_j-\frac{\overline{x}_l}{\lambda_l}\lambda_j=\lambda_j\left(\frac{\overline{x}_j}{\lambda_j}-\frac{\overline{x}_l}{\lambda_l}\right)$ y este número es no negativo por definición de $\alpha$.
		\item Si $\lambda_j\leq 0$ evidentemente $\overline{x}_j'=\overline{x}_j-\alpha\lambda_j$ es no negativo por ser la suma de números no negativos.
	\end{itemize}
	Cabe destacar que si $\overline{x}_j=0$ entonces $\overline{x}_j'=0$, además $\overline{x}_l>0$ mientras que $\overline{x}_l'=0$, luego $\overline{x}'$ tiene a lo sumo $p-1$ componentes no nulas, con lo que, al volver al principio de la demostración el conjunto de columnas que queremos que sean linealmente independientes se ha visto reducido en una columna como poco. Por tanto, tras un número finito de pasos encontraremos un punto extremo. Esto proporciona un algoritmo para encontrar un punto extremo de una región factible.
\end{proof}
Culminamos este teorema con una pequeña observación.
\begin{obs}[Extremalidad del $0$]
	Si una región factible contiene al vector $0$, este es siempre un punto extremo, basta con aplicar el teorema \ref{fund_teo_existenciaExtremos} para comprobarlo (el conjunto vacío es linealmente independiente). 
\end{obs}
El siguiente teorema termina de mostrar la brutal importancia de los puntos extremos de las regiones factibles en los problemas de programación lineal, y es que, si un problema tiene solución óptima, esta se alcanza en un punto extremo o solución básica factible.
\begin{theo}[Optimalidad]
	\label{fund_teo_optimExtremos}
	Si un problema de programación lineal en forma estándar tiene solución óptima, esta se alcanza en un punto extremo.
\end{theo}
\begin{proof}
	Procedemos de análogamente a como hicimos en la demostración del teorema \ref{fund_teo_existenciaExtremos}.
	
	Consideremos $\overline{x}$ un punto donde se alcanza la solución óptima al problema en cuestión. Reordenemos las coordenadas de $\overline{x}$ dejando las componentes nulas al final, reordenando también $A$ solidariamente.
	
	Suponiendo que $\overline{x}$ tiene $p$ componentes no nulas y que las columnas de $A$ correspondientes a dichas columnas son linealmente dependientes (en caso contrario hemos terminado), entonces habrá ciertos números reales no todos nulos (de hecho podemos suponer que alguno será positivo) tales que $\sum_{i=1}^{n}\lambda_ia_i=0$. Definimos el vector $\lambda\in\R^n$ de la manera usual y consideramos los puntos
	\begin{equation*}
		\begin{array}{cc}
		x_1=\overline{x}+\alpha_1\lambda\qquad&\qquad x_2=\overline{x}-\alpha_2\lambda
		\end{array}
	\end{equation*}
	siendo $\alpha_1$ y $\alpha_2$ coeficientes positivos lo suficientemente pequeños para que $x_1$ y $x_2$ sean no negativos.
	
	Es evidente que tanto $x_1$ como $x_2$ cumplen la condición $Ax=b$, esto se debe a la dependencia lineal de las $p$ primeras columnas y a la definición de $\lambda$, como ya hemos hecho en otras ocasiones.
	
	Evaluemos los puntos $x_1$ y $x_2$ por la función objetivo
	\begin{equation*}
		\begin{array}{c}
		c^tx_1=c^t\overline{x}+\alpha_1c^t\lambda\\
		c^tx_2=c^t\overline{x}-\alpha_2c^t\lambda
		\end{array}
	\end{equation*}
	Por la optimalidad de $\overline{x}$ se tiene que
	\begin{gather}
		\label{fund_eq_teoOptimExtrem1}c^t\overline{x}\leq c^t\overline{x}+\alpha_1c^t\lambda\\
		\label{fund_eq_teoOptimExtrem2}c^t\overline{x}\leq c^t\overline{x}-\alpha_2c^t\lambda
	\end{gather}
	De la ecuación \eqref{fund_eq_teoOptimExtrem1} se deduce que $c^t\lambda\geq 0$ y de la ecuación \eqref{fund_eq_teoOptimExtrem2} se desprende que $c^t\lambda\leq 0$, luego $c^t\lambda=0$, por tanto tanto $x_1$ como $x_2$ son puntos en los que también se alcanza la solución óptima. Centrémonos en $x_2$. Si tomamos $\alpha_2$ como
	\begin{equation*}
		\alpha_2:=\min\left\{\frac{\overline{x}_j}{\lambda_j}\midc \lambda_j>0\right\}
	\end{equation*}
	sabemos que $x_2$ sigue perteneciendo a la región factible (como demostramos en el teorema \ref{fund_teo_existenciaExtremos}) luego es una elección válida. Además, por las mismas razones que en el teorema \ref{fund_teo_existenciaExtremos}, el vector $x_2$ tiene a lo sumo $p-1$ componentes positivas, luego, tras un número finito de pasos encontraremos un punto extremo en el cual se alcance la solución óptima.
\end{proof}
\subsection{Direcciones extremas}
El siguiente es el último concepto relativo a convexidad que veremos, se trata del concepto de ``dirección''. En palabras llanas podríamos decir que una ``dirección'' de un conjunto convexo es una flecha tal que si partimos de cualquier punto del conjunto y la seguimos nunca saldremos del conjunto. la siguiente definición formaliza este concepto.
\begin{defi}[Dirección]
	Dado un conjunto convexo $\mc{C}$ y un vector $d\in\R^n$ diremos que $d$ es una \tbi{dirección} de $\mc{C}$ si para todo punto $x\in\mc{C}$ se verifica que
	\begin{equation*}
		x+\lambda d\in \mc{C}
	\end{equation*}
	para todo $\lambda$ no negativo. Asimismo, dadas dos direcciones $d_1$ y $d_2$ diremos que son \tbi[dirección!equivalente]{equivalentes} si son vectores proporcionales.
\end{defi}
\begin{obs}[No acotación]
	Nótese que si un conjunto convexo es acotado no puede tener direcciones (compruébese). Luego cuando hablemos de direcciones será recomendable tener en mente conjuntos como el que representa la ilustración \ref{fund_img_direcciones} (que continúa infinitamente hacia arriba y hacia la derecha).
\end{obs}
Veamos a continuación un concepto análogo al de los puntos extremos pero en el ámbito de las direcciones.
\begin{defi}[Dirección extrema]
	Sea $d$ una dirección. Diremos que $d$ es una \tbi[dirección!extrema]{dirección extrema} si no puede escribirse como combinación lineal positiva de dos direcciones no equivalentes. De forma sintética
	\begin{equation*}
		d=\lambda d_1+\mu d_2\text{ con }\lambda,\mu>0\sii d_1\sim d_2
	\end{equation*}
	donde $\sim$ denota la equivalencia de direcciones.
\end{defi}
\begin{figure}[h!]
	\centering
	\includegraphics[scale = 0.75]{img/direcciones}
	\caption{Ilustración de una dirección y una dirección extrema}
	\label{fund_img_direcciones}
\end{figure}
Veamos una caracterización muy sencilla de las direcciones para regiones factibles.
\begin{lem}[Caracterización débil]
	Dada una región factible $\mc{S}$, $d$ es una dirección de $\mc{S}$ si y solo si $d$ es un vector no negativo y no nulo que verifica que $Ad=0$.
\end{lem}
\begin{proof}Veamos ambas implicaciones.
	\begin{itemize}
		\item[\bra] Sea $d$ una dirección, por tanto, dado $x\in\mc{S}$ y $\lambda\geq 0$ se cumple que $x+\lambda d\in\mc{S}$, luego $A(x+\lambda d)=b$, por tanto
		\begin{equation*}
			A(x+\lambda d)=Ax+\lambda Ad=b+\lambda A d = b\sii A d = 0
		\end{equation*}
		Queda comprobar que $d$ es un vector no negativo, para ello supongamos que tiene alguna componente negativa, digamos la $i$--ésima. Entonces por hipótesis $x_j+\lambda d_j\geq 0$ para todo $\lambda\geq 0$, sin embargo, tomando un $\lambda$ lo suficientemente grande llegamos a un absurdo.
		\item[\bla] Es claro que $x+\lambda d$ es un vector no negativo para todo $x\in\mc{S}$ (por hipótesis). Comprobemos que está en la región factible. En efecto
		\begin{equation*}
			A(x+\lambda d)=Ax+\lambda Ad=Ax=b \qedhere
		\end{equation*}
	\end{itemize}
\end{proof}
El lema anterior nos abre las puertas a una caracterización de las direcciones extremas mucho muy potente y extremadamente parecida al teorema \ref{fund_teo_caracterizacionExtremos}, es decir, muy poco intuitiva. Conviene reflexionar sobre el enunciado antes de lanzarse a la demostración.
\begin{theo}[Caracterización fuerte]
	\label{fund_teo_caracterizacionDireccionesExtremas}
	Dada una región factible $\mc{S}$, $d$ es una dirección extrema de $\mc{S}$ si y solo si, tras una conveniente reordenación, hay una base $B$ de $A$ tal que
	\begin{equation*}
		\begin{array}{cc}
		A=(B|N)\qquad&\qquad d=\alpha\begin{pmatrix}
		-B^{-1}a_k\\
		e_t
		\end{pmatrix}
		\end{array}
	\end{equation*}
	donde $a_k$ es una columna de $N$, o sea, la $k=m+t$--ésima columna de $A$.
	
	Además, $\alpha>0$, $B^{-1}a_k\leq 0$ y $e_t$ es el $t$--ésimo vector de la base canónica de $\R^{n-m}$.
\end{theo}
\begin{proof}Veamos ambas implicaciones.
	\begin{itemize}
		\item[\bla] Sea $d$ un vector que cumple las propiedades del enunciado, luego no negativo y no nulo. Veamos en primer lugar que es una dirección de la región factible. En efecto, usando la única caracterización que tenemos
		\begin{equation*}
			Ad=(B|N)(-B^{-1}a_k|e_t)^t=-BB^{-1}a_k+Ne_t=-a_k+a_k=0
		\end{equation*}
		Para comprobar la extremalidad de $d$, supongamos que puede escribirse como combinación lineal positiva de dos direcciones $d_1$ y $d_2$ y veamos que estas direcciones son equivalentes.
		
		Como $d=\lambda_1 d_1+\lambda_2 d_2$, si nos fijamos únicamente en las $n-m$ últimas coordenadas se tiene que $e_t=\lambda_1d_{1N}+\lambda_2d_{2N}$, luego, por ser los vectores $d_1$ y $d_2$ no negativos, sus $n-m$ últimas componentes deben ser múltiplos de $e_t$. Es decir $d_{iN}=\mu_ie_t$ con $i\in\{1,2\}$.
		
		Fijémonos ahora en las $m$ primeras componentes. Al ser los vectores $d_i$ con $i\in\{1,2\}$ direcciones se cumplirá que $Ad_i=0$. Desarrollando esta cuenta tenemos
		\begin{equation*}
			Ad_i=(B|N)(d_{iB}|\mu_i e_t)^t=Bd_{iB}+\mu_iNe_t=Bd_{iB}+\mu_ia_k=0
		\end{equation*}
		despejando obtenemos que $d_{iB}=-\mu_iB^{-1}a_k$ con lo que ambas direcciones son equivalentes.
		\item[\bra] Sea $d$ una dirección extrema de $\mc{S}$, por tanto un vector no negativo y no nulo. Podemos suponer que $d$ tiene $p+1$ componentes positivas con $p\in\{0,\dots,n-1\}$. En primer lugar, procedemos a reordenar el vector de forma que sus $p$ primeras componentes sean positivas, colocando la última componente positiva en la posición $k>m$ (cualquiera donde quepa).
		
		Veamos que las columnas correspondientes a las $p$ primeras componentes del vector $d$ reordenado son linealmente independientes. En caso contrario existirá una combinación lineal no trivial tal que $\sum_{i=1}^{p}\lambda_ia_i=0$. Dicho esto, vamos a tratar de construir dos direcciones de $\mc{S}$ en función de las cuales $d$ pueda escribirse como combinación lineal positiva, llegando a una contradicción con la extremalidad de $d$.
		
		Definiendo el vector $\lambda:=(\lambda_1,\dots,\lambda_p,0,\dots,0)$ y los vectores
		\begin{equation*}
			\begin{array}{cc}
			d_1=d+\delta\lambda\qquad&\qquad d_2=d-\delta\lambda
			\end{array}
		\end{equation*} siendo $\delta$ suficientemente pequeño para que $d_1$ y $d_2$ sean no negativos. Nótese que $d=\frac{1}{2}d_1+\frac{1}{2}d_2$, luego si $d_1$ y $d_2$ son direcciones tendríamos una contradicción. Y, en efecto lo son, basta echar las cuentas
		\begin{equation*}
			Ad_i=A(d\pm\delta\lambda)=Ad\pm\delta A\lambda=0
		\end{equation*}
		De esta forma se llega a la conclusión de que necesariamente $p\leq m$, lo cual es importante, pues nos dice que si un vector tiene más de $m$ componentes positivas este no puede ser dirección extrema.
		
		En definitiva, si $p=m$ ya tenemos una base $B$ de $A$, en caso contrario siempre podemos coger columnas adicionales para completar la base (observación \ref{fund_obs_rango}). Veamos pues que se cumple lo que se tiene que cumplir, para lo cual basta con, de nuevo, hacer las cuentas.
		\begin{equation*}
			0=Ad=(B|N)(d_B|d_N)^t=Bd_B+Nd_N=Bd_B+a_kd_k
		\end{equation*}
		despejando obtenemos que $d_B=-d_kB^{-1}a_k$ y evidentemente $d_N=d_ke_t$, luego tomando $\alpha:=d_k$ se tiene el resultado.\qedhere
	\end{itemize}
\end{proof}
\begin{obs}[Finitud]
	Sobre la finitud de las direcciones extremas de una región factible sigue siendo válida con muy pocas modificaciones (que se dejan al lector) la observación \ref{fund_obs_finitudExtremos}.
\end{obs}
Terminado nuestro viaje por el mundo de la convexidad, podemos internarnos ya en el meollo del asunto, es decir, en desarrollar material teórico que nos permita desarrollar un algoritmo para optimizar funciones lineales condicionadas a variedades afines en el $2^n$--ante positivo.
\section{Teoremas fundamentales}
Empezamos esta sección enunciando un teorema que no demostraremos, no porque sea especialmente complicado, sino porque es especialmente largo y su demostración no aporta demasiado a nuestros objetivos. Hablamos del famoso ``teorema de representación''.
\begin{theo}[Teorema de representación]
	Sea $\mc{S}$ una región factible con $p$ puntos extremos $\{x_1,\dots,x_p\}$ y $l$ direcciones extremas $\{d_1,\dots,d_l\}$. Un punto $x$ está en la región factible si y solo si $x$ es ``combinación esotérica'' de puntos y direcciones extremas, es decir
	\begin{equation*}
		x=\sum_{i=1}^{p}\lambda_ix_i+\sum_{j=1}^{l}\mu_jd_j
	\end{equation*}
	con $\lambda_i,\mu_j\geq 0$ y $\sum_{i=1}^{p}\lambda_i=1$.
\end{theo}
El siguiente teorema es un test de optimalidad para regiones factibles que poseen direcciones extremas.
\begin{theo}[Test de optimalidad]
	\label{fund_teo_testOptim}
	Dado un problema de programación lineal con región factible no acotada y direcciones extremas $\{d_1,\dots,d_s\}$, el problema tiene solución optima si y solo si se cumple que $c^td_j\geq 0$ para todo $j\in\{1,\dots,s\}$.
\end{theo}
\begin{proof}Demostremos ambas implicaciones echando mano del teorema de representación.
	\begin{itemize}
		\item[\bra] Por reducción al absurdo, si la función objetivo alcanzara la solución óptima y hubiera una dirección extrema $d_r$ tal que $c^td_r<0$ podemos considerar el punto de la región factible (por el teorema de representación)
		\begin{equation*}
			x_{\mu_r}=\sum_{i=1}^{p}\lambda_ix_i+\mu_rd_r
		\end{equation*}
		Veamos cómo evoluciona el valor de la función objetivo para valores grandes de $\mu_r$
		\begin{equation*}
			\lim\limits_{\mu_r\to\infty}c^tx_{\mu_r}=\lim\limits_{\mu_r\to\infty}\sum_{i=1}^{p}\lambda_ic^tx_i+\mu_rc^td_r=\lim\limits_{\mu_r\to\infty}\mu_rc^td_r=-\infty
		\end{equation*}
		luego es un problema con solución no acotada (absurdo por hipótesis).
		\item[\bla]Dado un punto $x$ de la región factible por el teorema de representación se verifica que la función objetivo evaluada en $x$ es\begin{equation*}
			c^tx=\sum_{i=1}^{p}\lambda_ic^tx_i+\sum_{j=1}^{s}\mu_jc^td_j
		\end{equation*} el resultado que arroja el segundo sumatorio es positivo, luego si consiguiéramos fusilarlo mejoraríamos el valor de la función objetivo. Para ello basta con coger el punto con representación $x':=\sum_{i=1}^{p}\lambda_ix_i$. De esta forma
		\begin{equation*}
			c^tx\geq c^tx'=\sum_{i=1}^{p}\lambda_ic^tx_i
		\end{equation*} más aun, si consideramos el punto extremo $x^*$ tal que $c^tx^*:=\min\{c^tx_i\midc 1\leq i\leq p\}$ tenemos la cadena de desigualdades \begin{equation*}
		c^tx\geq c^tx'\geq \sum_{i=1}^{p}\lambda_ic^tx^*=c^tx^*
	\end{equation*} de esta forma se tiene que $x^*$ es solución óptima del problema, ya que la función objetivo evaluada en ese punto es menor o igual a la de cualquier otro.\qedhere
	\end{itemize}
\end{proof}
\begin{obs}[No acotación]
	La hipótesis de no acotación del teorema \ref{fund_teo_testOptim} no es necesaria para la demostración de la implicación a la izquierda.
	
	La demostración de la implicación a la derecha exige la existencia de al menos una dirección extrema, y por tanto la no acotación de la región factible. Esta implicación constituye un criterio de no acotación de la función objetivo en regiones factibles no acotadas.
\end{obs}
\begin{obs}[Algoritmo de representación]
	Con todo lo que tenemos hasta ahora ya tenemos un algoritmo (algoritmo \ref{fund_alg_represent}) para resolver problemas de programación lineal. Basta con calcular los puntos y direcciones extremas de la región factible, lo cual puede hacerse usando los teoremas \ref{fund_teo_caracterizacionExtremos} y \ref{fund_teo_caracterizacionDireccionesExtremas} y seguir la demostración del test de optimalidad (teorema \ref{fund_teo_testOptim}).
	
	La eficiencia de este algoritmo es pésima pues obliga a calcular todos los puntos extremos, con lo que la eficiencia el algoritmo sería $\mc{O}\binom{n}{m}$ siendo $n$ el número de variables de decisión y $m$ el número de restricciones.
\end{obs}
\begin{algorithm}[H]
	\begin{algorithmic}[1]
		\STATE Calcular los puntos y direcciones extremas.
		\IF{no hay puntos extremos}
			\RETURN problema infactible.
		\ELSIF{no hay direcciones extremas}
			\RETURN el mejor punto extremo.
		\ELSE\STATE\COMMENT{Region no acotada.}
			\STATE Comprobar las hipótesis del teorema \ref{fund_teo_testOptim}.
			\IF{hay solución óptima}
				\RETURN el mejor punto extremo.
			\ELSE
				\RETURN $x_{\mu_r}$, dirección en la que las soluciones decrecen.
			\ENDIF
		\ENDIF
	\end{algorithmic}
	\caption{Algoritmo de representación.}
	\label{fund_alg_represent}
\end{algorithm}
La pésima eficiencia del algoritmo basado en el teorema de representación incita seguir desarrollando soporte teórico para un algoritmo mejor.

El siguiente resultado actúa a modo de recopilación de todo lo visto hasta ahora y se presenta más que nada por su recurrente aparición en la literatura como un teorema con nombre propio.
\begin{theo}[Teorema fundamental de la programación lineal]
	\label{fund_teo_fund}
	Dado un problema de programación lineal en forma estándar se cumplen los siguientes asertos
	\begin{enumerate}
		\item Si el problema es factible, posee al menos una solución básica factible.
		\item Si el problema tiene solución óptima, posee al menos una solución básica factible óptima.
		\item Si el problema no tiene solución óptima, o bien es infactible o bien tiene solución no acotada.
	\end{enumerate}
\end{theo}
\begin{proof}
	El primer aserto se corresponde con el contenido del teorema \ref{fund_teo_existenciaExtremos}. La segunda afirmación se demuestra en el teorema \ref{fund_teo_optimExtremos}.
	
	El último apartado es el que tiene algo más de chicha. Distinguimos tres casos según la naturaleza de la región factible. Si la región factible es vacía el problema es, por definición, infactible. En el caso de que la región factible sea acotada, es también compacta. 
	
	Esto se debe a que es la imagen inversa de $\{b\}$ (cerrado) vía la función lineal (continua) que tiene a $A$ por matriz asociada, luego la región factible es cerrada y acotada, por tanto, la función objetivo alcanza el mínimo en la región factible y el problema tiene solución óptima (se sale de nuestros casos de estudio).
	
	Si la región factible es no acotada basta con aplicar el teorema \ref{fund_teo_testOptim}.
\end{proof}
A continuación vamos a presentar un nuevo test de optimalidad de una solución factible que no requiere del cálculo de todas las direcciones extremas. Antes de enunciarlo presentamos la siguiente definición.
\begin{defi}[Costes reducidos]
	Definimos el \tbi[vector!de costes reducidos]{vector de costes reducidos} asociado a una base $B$ como el vector $\overline{c}\in\R^n$ cuyas componentes son
	\begin{equation*}
		\overline{c_j}:=c_j-c_B^tB^{-1}a_j
	\end{equation*}
	justificaremos el nombre y la razón de ser de esta cruel definición tras la demostración del teorema \ref{fund_teo_testOptimSimplex}, que consideramos bastante importante.
\end{defi}
\begin{theo}[Test de optimalidad]
	\label{fund_teo_testOptimSimplex}
	Si $\overline{x}$ es un punto extremo de $\mc{S}$ asociado a la base $B$ y el vector de costes reducidos asociado a $B$ es no negativo entonces $\overline{x}$ es una solución óptima.
\end{theo}
\begin{proof}
	Sea $x$ un punto arbitrario de $\mc{S}$. Vamos a demostrar que $c^tx\geq c^t\overline{x}$, con lo cual quedaría demostrado el resultado.
	
	Como $x\in \mc{S}$ se verifica que $Ax=b$. Expresando esto en términos de la base $B$ tenemos que
	\begin{equation}
	\label{fund_eq_problema}
		Ax=(B|N)(x_B|x_N)^t=Bx_B+Nx_N=b\sii x_B=B^{-1}b-B^{-1}Nx_N
	\end{equation}
	Pasemos $x$ por la función objetivo
	\begin{multline}
		\label{fund_eq_funcionObj}
		c^tx=(c_{B}^t|c_N^t)(x_B|x_N)^t=c_B^tx_B+c_N^tx_N=\\
		=c_B^t(B^{-1}b-B^{-1}Nx_N)+c_N^tx_N=c_B^tB^{-1}b+(c_N^t-c_B^tB^{-1}N)x_N=\\
		=c_B^tB^{-1}b+\left(c_N^tx_N-c_B^t\sum_{m+1}^{n}(B^{-1}a_j)x_j\right)=c_B^tB^{-1}b+\sum_{j=m+1}^{n}(c_jx_j-c_B^tB^{-1}a_jx_j)=\\
		=c_B^tB^{-1}b+\sum_{j=m+1}^{n}(c_j-c_B^tB^{-1}a_j)x_j=c_B^tB^{-1}b+\sum_{j=m+1}^{n}\overline{c_j}x_j
	\end{multline}
	Como el último sumatorio arroja por hipótesis un resultado positivo tenemos que
	\begin{equation*}
		c^tx\geq c_B^tB^{-1}b
	\end{equation*}
	pero resulta que $c_B^tB^{-1}b=c^t\overline{x}$ por el teorema \ref{fund_teo_caracterizacionExtremos} (compruébese). 
\end{proof}
\begin{obs}[Justificación del nombre]
	Una interpretación que le podemos dar a las componentes del vector de costes reducidos es la siguiente.
	\begin{equation*}
		\overline{c_j}=c^te_j-c^t\overline{x}=c^t(e_j-\overline{x})
	\end{equation*}
	donde $e_j$ es el $j$--ésimo vector de la base canónica de $\R^n$ y $\overline{x}$ es una solución básica factible (punto extremo).
	
	Lo que quiere decir esta interpretación es que $\overline{c_j}$ es la diferencia entre valor que arroja la función objetivo cuando se la evalúa en un punto extremo y el valor que obtenemos si la evaluamos sobre un vector de la base canónica.
	
	La comprobación de la validez de esta interpretación se hace echando mano de las identidades obtenidas en la demostración del teorema \ref{fund_teo_testOptimSimplex}.
	
	Nótese que si $j\leq m$ entonces $\overline{c_j}=0$ (compruébese).
\end{obs}
\begin{theo}[Test de no acotación]
	\label{fund_teo_testNoAcotacionSimplex}
	Dada una región factible no vacía $\mc{S}$ y un punto extremos $\overline{x}$ asociado a la base $B$ de manera que hay algún coste reducido $\overline{c_k}<0$ y se verifica que $B^{-1}a_k\leq 0$, entonces el problema tiene solución no acotada.
\end{theo}
\begin{proof}
	Nótese que $k\in\{m+1,\dots,n\}$ ya que los costes reducidos asociados a las componentes asociadas a $B$ son nulos.
	
	Como por hipótesis $B^{-1}a_k\leq 0$, luego es claro que la región factible posee una dirección extrema, la dada por $d:=(-B^{-1}a_k|e_t)^t$ donde $k=m+t$.
	
	De esta forma sabemos que el punto $x_\alpha:=\overline{x}+\alpha d\in\mc{S}$ para todo $\alpha\geq 0$. Veamos cómo se comporta $c^tx_\alpha$ para valores grandes de $\alpha$
	\begin{equation*}
		c^tx_\alpha=c^t(\overline{x}+\alpha d)=c^t\overline{x}+\alpha c^td\stackrel{!}{=}c_b^tB^{-1}b+\alpha c^td
	\end{equation*}
	por otra parte tenemos que
	\begin{equation*}
		c^td =(c_B^t|c_N^t)(-B^{-1}a_k|e_t)^t=-c_B^tB^{-1}a_k+c_N^te_t=-c_B^tB^{-1}a_k+c_k=\overline{c_k}
	\end{equation*}
	de manera que $c^tx_\alpha=c_B^tB^{-1}b+\alpha\overline{c_k}$. Al ser $\overline{c_k}$ negativo claramente $\lim\limits_{\alpha\to\infty}c^tx_\alpha=-\infty$
\end{proof}
\begin{obs}[Dirección de decrecimiento]
	De la demostración del teorema \ref{fund_teo_testNoAcotacionSimplex} se deduce que la función objetivo decrece indefinidamente a lo largo de la recta afín de ecuación paramétrica $x_\alpha$. A esto usualmente lo llamaremos \tbi[dirección! de decrecimiento de las soluciones]{dirección de decrecimiento} de las soluciones.
	
	Algo similar se desprende de la demostración del teorema \ref{fund_teo_testOptim} con $x_{\mu_r}$.
\end{obs}
\begin{defi}[Vector auxiliar]
	\label{fund_defi_vectorAux}
	Dada una matriz $A$ y una base $B$ de $A$, llamamos \tbi[vector!auxiliar]{vector auxiliar} asociado a $a_k$ al vector
	\begin{equation*}
		y_k:=B^{-1}a_k\in\R^m
	\end{equation*}
	La única intención de esta definición es compactar un poco la notación para las direcciones extremas. Cabe destacar que no es necesario aprenderse la fórmula, simplemente hay que entender que $y_k$ es el vector $a_k$ escrito en coordenadas de la base $B$.
	
	Denotaremos por $y_{ik}$ a la $i$--ésima componente de $y_k$.
\end{defi}
Presentamos a continuación el que probablemente sea el teorema más importante del curso, cuya demostración es debida a Clara Rodríguez.
\begin{theo}[Mejora de la solución]
	\label{fund_teo_mejoraSimplex}
	Sea una región factible $\mc{S}$ no vacía y un punto extremo $\overline{x}$ asociado a una base $B$ de manera que hay algún coste reducido $\overline{c_k}<0$ y se verifica que $y_k$ tiene al menos una componente positiva.
	
	Entonces se tiene que el vector $x'=\overline{x}+\alpha d$ es un punto extremo que mejora la función objetivo respecto a $\overline{x}$, es decir $c^tx'\leq c^t\overline{x}$. Donde
	\begin{equation*}
	\begin{array}{cc}
	\alpha:=\min\left\{\frac{\overline{x_i}}{y_{ik}}\midc y_{ik}>0\right\}=:\frac{\overline{x_l}}{y_{lk}} \qquad&\qquad d:=(-y_k|e_t)^t\text{ con } k=m+t
	\end{array}
	\end{equation*}
	Además $c^tx'< c^t\overline{x}$ si y solo si $\overline{x_l}>0$.
\end{theo}
\begin{proof}
	Veamos en primer lugar que $x'$ pertenece a la región factible. Es evidente que $Ax'=b$ (se deja al lector hacer la cuenta), luego solo queda comprobar que $x'\geq 0$. Es claro que las componentes de $x'$ que no están asociadas a la base $B$ son todas positivas ya que
	\begin{equation*}
		x'=(x_B'|x_N')^t=(B^{-1}b|0)^t+\alpha(-y_k|e_t)^t\sii x_N'=\alpha e_t\geq 0
	\end{equation*}
	por tanto, solo nos debemos preocupar de las componentes asociadas a la base. Veamos que son todas no negativas
	\begin{equation*}
		x_B'=\overline{x_B}-\alpha y_k\geq 0\sii \overline{x_i}-\alpha y_{ik}\geq 0\text{ con } i\in\{1,\dots,m\}
	\end{equation*}
	esa última condición se verifica si y solo si $\alpha\leq \frac{\overline{x_i}}{y_{ik}}$ para los $y_{ik}>0$. Y hemos cogido $\alpha$ expresamente para que esto se cumpla, luego $x'$ está en la región factible.
	
	Veamos ahora que $x'$ es un punto extremo, para lo cual basta con demostrar que el conjunto de vectores $(B\setminus a_l)\cup a_k$ es linealmente independiente. Para mostrar consideramos el conjunto de vectores $B\cup a_k$, que es un sistema de generadores de $\R^m$.
	
	Resulta que $a_l$ es combinación lineal de los vectores de $(B\setminus a_l)\cup a_k$ ya que
	\begin{equation*}
		a_k=\sum_{i=1}^{m}y_{ik}a_i=\sum_{\substack{i=1\\i\not=l}}^{m}y_{ik}a_i+y_{lk}a_l
	\end{equation*}
	como $y_{lk}>0$ podemos despejar $a_l$. En efecto
	\begin{equation*}
		a_l=\frac{a_k}{y_{lk}}-\sum_{\substack{i=1\\i\not=l}}^{m}\frac{y_{ik}}{y_{lk}}a_i
	\end{equation*}
	de donde se desprende que el conjunto $(B\setminus a_l)\cup a_k$ sigue siendo un sistema de generadores de $\R^m$ que además tiene $m$ elementos, luego es una base.
	
	Veamos ahora que $x'$ es una solución mejor que $\overline{x}$, basta echar las cuentas
	\begin{equation*}
		c^t(\overline{x}+\alpha d)=c^t\overline{x}+\frac{\overline{x_l}}{y_{lk}}c^td\stackrel{!}{=}c_B^tB^{-1}b+\frac{\overline{x_l}}{y_{lk}}\overline{c_k}\leq c_B^tB^{-1}b=c^t\overline{x}
	\end{equation*}
	El ``además'' es evidente. Con lo que concluye la prueba.
\end{proof}
Con la ayuda estos últimos teoremas podemos desarrollar un algoritmo (mejor que el basado en el teorema de representación) que nos permite hallar la solución a un problema de programación lineal. Este algoritmo es conocido como ``algoritmo del Símplex'' (algoritmo \ref{fund_alg_simplex}), desarrollado por el matemático estadounidense \ti{\tb{George Dantzig}} en $1947$.
\begin{obs}[Justificación del nombre]
	El nombre ``símplex'' que recibe el algoritmo se debe a que las regiones factibles de los problemas de programación lineal en forma estándar tienen estructura geométrica de \tb{\ti{poliedros}} o \tb{\ti{símplices}}, o, en inglés \tb{\ti{simplex}}.
\end{obs}
\begin{algorithm}[H]
	\begin{algorithmic}[1]
		\REQUIRE Punto extremo $\overline{x}$ de la región factible $\mc{S}$.
		\ENSURE Punto extremo $x'$ que mejora o iguala a $\overline{x}$.
		\STATE Aplicar el teorema \ref{fund_teo_testOptimSimplex} para detectar la optimalidad de $\overline{x}$.
		\IF {$\overline{x}$ es solución óptima}
		\RETURN $\overline{x}$.
		\ELSE 
		\STATE Aplicar el teorema \ref{fund_teo_testNoAcotacionSimplex} para detectar la no acotación.
			\IF{el problema es no acotado}
			\RETURN $x_\alpha$, dirección de decrecimiento de las soluciones.
			\ELSE
			\STATE Calcular una solución mejor usando el teorema \ref{fund_teo_mejoraSimplex}.
			\RETURN $x'$, la solución mejorada.
			\ENDIF
		\ENDIF	
	\end{algorithmic}
	\caption{Primera aproximacíon al algoritmo del Símplex.}\label{fund_alg_simplex}
\end{algorithm}
\begin{obs}[Acertijos en la oscuridad]
	El algoritmo \ref{fund_alg_simplex} deja ciertas lagunas. Por ejemplo, no está claro que la aplicación reiterada del algoritmo termine devolviéndonos una solución óptima (podría quedarse estancado).
	
	Además, tampoco se especifica cómo se calcula el primer punto extremo, ni se establece ninguna regla acerca de cómo calcular los costes reducidos necesarios para aplicar los teoremas.
	
	Este último punto puede dar lugar (con una implementación inocente) a un algoritmo extremadamente ineficiente.
	
	Otro asunto a discutir sería el de, en caso de haber varios costes reducidos negativos, cuál elegir para aplicar el teorema \ref{fund_teo_mejoraSimplex}.
\end{obs}
Todas estas cuestiones se discutirán en el capítulo siguiente.