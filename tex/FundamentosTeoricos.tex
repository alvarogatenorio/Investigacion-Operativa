%Para este capítulo se usará la abreviatura "fund".
\chapter{Fundamentos teóricos}
\label{fund}
La disciplina de la programación lineal cogió fuerza en la segunda guerra mundial, pues era necesario optimizar los escasos recursos de los que se disponían para realizar operaciones militares. En un principio fue impulsada por el ejército británico, con un grupo de investigación dirigido por \ti{\tb{Blackett}}, quien más adelante ganaría el premio Nobel de física.

Cabe destacar la gran importancia que tuvo la programación lineal en países como Albania, debido, entre otras cosas, a su utilidad a la hora de optimizar los procesos de producción de carbón. 
\section{Planteamiento del problema}
De ahora en adelante consideraremos fijadas las bases canónicas tanto de $\R^n$ como de $\R$, a no ser que se especifique lo contrario.

Antes de plantear formalmente el problema de la programación lineal, introduzcamos unas definiciones.

\begin{defi}[Vector no negativo]
	Un vector $x=(x_1,\dots,x_n)\in\R^n$ se dice \tbi[vector!no negativo]{no negativo} si todas sus componentes son, como su propio nombre indica, no negativas. Es decir, $x_i\geq 0$ para todo $i\in\{1,\dots,n\}$. Si un vector $x$ es no negativo escribiremos $x\geq 0$.
\end{defi}
Al hilo de esta definición, vamos a generalizar ese concepto que lleva presente en nuestras vidas desde que aquel docente de primaria nos dibujó dos rectas perpendiculares en la pizarra, los cuadrantes.

\begin{defi}[$2^n$--ante positivo]
	Definimos el \tbi{$2^n$--ante positivo} de $\R^n$ como el conjunto de los vectores no negativos de $\R^n$.
	
	Se deja como ejercicio al lector justificar el nombre de $2^n$--ante.
\end{defi}
Dicho esto, ya podemos formular el problema de la programación lineal.
\begin{prob}[Formulación general]
	\label{fund_prob_formulacionGeneral}
	La disciplina de la \tbi[programación!lineal]{programación lineal} estudia procedimientos (implementables en ordenador) para calcular los extremos absolutos de una aplicación lineal $f:\R^n\to\R$ cuando restringimos su dominio a una variedad afín de $\R^n$ cortada con el $2^n$--ante positivo.
	
	\begin{figure}[h!]
		\centering
		\label{fund_img_problemaGeneral}
		\includegraphics[scale = 0.75]{img/problemaGeneral}
		\caption{Ilustración del problema de programación lineal.}
	\end{figure}
	En el caso de la ilustración \ref{fund_img_problemaGeneral} suponemos que $f$ es una aplicación lineal definida sobre todo el plano a la cual restringimos el dominio a la variedad afín $\mc{L}$ cortada con el cuadrante positivo (línea azul). Nuestro deber es encontrar los puntos (si los hay) sobre la línea azul en los cuales $f$ alcanza un máximo o un mínimo absoluto.
\end{prob} 
En esta sección nos dedicaremos simplemente a formular de diversas maneras el problema de la programación lineal, observando que todas son equivalentes entre sí. Comencemos pues, sin más dilación nuestro viaje por lo desconocido, un viaje del que probablemente no regresaremos.

A continuación vemos una sencilla caracterización de las aplicaciones lineales $f:\R^n\to\R$ a partir de su expresión analítica.
\begin{obs}[Expresión analítica]
	Consideremos una aplicación lineal $f:\R^n\to\R$.
	
	Por ser $f$ una aplicación lineal, tendrá una única matriz asociada $C$. Como salta a la vista, $C$ será una matriz fila con $n$ columnas.
	
	Entonces, dado un vector $x\in\R^n$ se tiene que
	\begin{equation}
	\label{fund_eq_lineales}
	f(x)=f(x_1,\dots,x_n)= CX=\begin{pmatrix}
	c_1 & \cdots & c_n
	\end{pmatrix}\begin{pmatrix}
	x_1\\ \cdots \\x_n
	\end{pmatrix}=c_1x_1+\cdots+c_nx_n
	\end{equation}
	De esta forma se concluye que toda aplicación lineal $f:\R^n\to\R$ tiene una expresión analítica de la forma de la ecuación \eqref{fund_eq_lineales}. Recíprocamente, toda función $g:\R^n\to\R$ con una expresión analítica como la de \eqref{fund_eq_lineales} es una ecuación lineal, para demostrarlo, basta con leer como los árabes la ecuación \eqref{fund_eq_lineales}.
\end{obs}
Antes de continuar es importante hacer un inciso acerca de la notación.
\begin{obs}[Notación matricial]
	Siempre que escribamos matrices fila o columna lo haremos con letras minúsculas. Asimismo, siempre que estemos escribiendo una matriz fila añadiremos el símbolo de trasposición, para dejarlo claro implícitamente. De esta manera, a las funciones lineales las denotaremos por
	\begin{equation*}
		f(x_1\dots,x_n)=c^tx
	\end{equation*}
	donde $c^t$ es la matriz asociada a $f$ y $x$ es el vector columna con las coordenadas de $x$ respecto de la base canónica (nótese el pequeño abuso de notación).
\end{obs}
\subsection{Forma estándar y formas canónicas}

Sea $f(x_1,\dots,x_n)=c_1x_1+\cdots+c_nx_n=c^tx$ una función lineal  a la que a partir de ahora llamaremos \tbi[función!objetivo]{función objetivo}. A la matriz $c$ se la denomina \tbi[vector!de coeficientes de la función objetivo]{vector de coeficientes de la función objetivo}, mientras que $x$ recibe el nombre de \tbi[vector!de variables de decisión]{vector de variables de decisión}.

Consideremos asimismo la variedad afín de $\R^n$ dada por el conjunto de soluciones al sistema no necesariamente homogéneo $Ax=b$. Escrito de forma desarrollada
\begin{equation*}
\begin{array}{cc}
\left\{\begin{array}{cccc}
a_{11}x_1&+\cdots+&a_{1n}x_n&=b_1\\
\vdots &\ddots &\vdots &\vdots\\
a_{1m}x_1&+\cdots+&a_{mn}x_n&=b_m
\end{array}\right.\qquad&\qquad\begin{pmatrix}
a_{11} & \cdots & a_{1n}\\
\vdots & \ddots & \vdots\\
a_{m1} & \cdots & a_{mn}
\end{pmatrix}\begin{pmatrix}
x_1\\
\vdots\\
x_n
\end{pmatrix}=\begin{pmatrix}
b_1\\
\vdots\\
b_m
\end{pmatrix}
\end{array}
\end{equation*}
A la matriz $A\in\mathfrak{M}_{m\times n}(\R)$ usualmente la llamaremos \tbi[matriz!de coeficientes de las restricciones]{matriz de coeficientes de las restricciones}, además, a cada una de las ecuaciones del sistema las llamaremos \tbi{restricciones}. Asimismo el vector $b$ será conocido como \tbi[vector!de términos independientes]{vector de términos independientes}.

Expongamos ahora la llamada ``\ti{formulación estándar}'' del problema de la programación lineal.

\begin{prob}[Forma estándar]
El problema de programación lineal en \tbi[forma!estándar]{forma estándar} consiste en hallar los vectores $x\geq 0$ tales que sean solución de $Ax=b$ y además sean mínimos absolutos de la función $f$. Usualmente se escribe de forma sintética (aunque no lo usaremos mucho)
\begin{equation*}
\begin{array}{c}
\min c^tx\\
\text{Sujeto a:}\qquad Ax=b,\qquad x\geq 0
\end{array}
\end{equation*}
Nótese que la formulación del problema en forma estándar es exactamente la misma que la formulación general \ref{fund_prob_formulacionGeneral} con la única diferencia de que ya no buscamos extremos absolutos en general, sino únicamente mínimos. 
\end{prob}
La formulación estándar del problema de programación lineal puede parecer muy inflexible, en el sentido de que, a simple vista, no parece ser demasiado útil para modelizar problemas reales. Es esta aparente rigidez la que lleva a plantearse otras formulaciones más laxas del problema de programación lineal. Las llamadas ``\ti{formulaciones canónicas}'' que exponemos a continuación.
\begin{prob}[Primera forma canónica]
	Esta nueva formulación a la que llamamos \tbi[forma!canónica primera]{primera forma canónica} plantea el problema de hallar los vectores $x\geq 0$ tales que satisfagan la ecuación $Ax\leq b$ y que además sean máximos absolutos de la función objetivo $f$. Escrito de forma compacta
	\begin{equation*}
	\begin{array}{c}
	\max c^tx\\
	\text{Sujeto a:}\qquad Ax\leq b,\qquad x\geq 0
	\end{array}
	\end{equation*}
\end{prob}
\begin{prob}[Segunda forma canónica]
	El problema de programación lineal en \tbi[forma!canónica segunda]{segunda forma canónica} consiste en encontrar los vectores $x\geq 0$ tales que satisfagan la ecuación $Ax\geq b$ y que además sean mínimos absolutos de la función objetivo $f$. En resumen
	\begin{equation*}
	\begin{array}{c}
	\min c^tx\\
	\text{Sujeto a:}\qquad Ax\geq b,\qquad x\geq 0
	\end{array}
	\end{equation*}
\end{prob}
Una vez vistas todas las formulaciones hagamos una pequeña reflexión. Sería la apoteosis del tedio tener que demostrar resultados teóricos para cada una de las formulaciones. No obstante, no hay que dejar nunca de confiar en los dioses olímpicos, pues a veces son benévolos con nosotros. Esta benevolencia consiste en que todas los formulaciones son equivalentes en el sentido que veremos a continuación.
\subsubsection{Maximización y minimización}
Si se nos plantea el problema de hallar el máximo absoluto de una función objetivo $f$ podemos transformar este problema en uno equivalente de forma que el objetivo de el nuevo problema sea hallar el mínimo de otra función objetivo $g$.

La utilidad de este hecho es que podemos transformar un problema de programación lineal en primera forma canónica (que surgen de forma natural a la hora de modelizar problemas reales) en un problema más parecido a la formulación estándar.
\begin{lem}[Maximización--minimización]
	Sea $A$ un conjunto arbitrario y una función $f:A\to\R$ que alcanza el máximo y el mínimo. Entonces se cumple
	\begin{equation*}
		\max f=-\min (-f)
	\end{equation*}
\end{lem}
\begin{proof}
	Sea $\xi$ el máximo de $f$, es decir, el único número de $f(A)$ que verifica que $f(x)\leq \xi$ para todo $x\in A$. Consideremos ahora la función $g\equiv -f$, que, por las propiedades de los números reales alcanza el mínimo, al que llamaremos $\eta$.
	
	Evidentemente $\eta$ verifica que $g(x)\geq \eta$ para todo $x\in A$, o equivalentemente $-g(x)\leq -\eta$ para todo $x\in A$. Pero $-g(x)=f(x)$, luego $f(x)\leq -\eta$ para todo $x\in A$. Por unicidad del máximo $-\eta =\xi$, como queríamos demostrar.
\end{proof}
\begin{obs}[Aplicación]
	Si tenemos un problema de programación lineal formulado en términos de maximización bastará con cambiar la función objetivo $f$ por la función objetivo $-f$ para obtener un problema equivalente en términos de minimización cuyas soluciones deben ser cambiadas de signo.
\end{obs}
\subsubsection{Igualdades y desigualdades}
Una vez visto el apartado anterior, vamos a aprender a trasformar restricciones de tipo desigualdad a restricciones de tipo igualdad, con lo cual ya podremos transformar cualquier problema planteado en alguna forma canónica a un problema equivalente planteado en forma estándar.
\begin{lem}[Variables de holgura]
	Sea una restricción de la forma
	\begin{equation}
	\label{fund_eq_rest1}
		a_{i1}x_1+\dots+a_{in}x_n\stackrel{(\geq)}{\leq} b_i
	\end{equation}
	entonces la restricción
	\begin{equation}
	\label{fund_eq_rest2}
		a_{i1}x_1+\dots+a_{in}x_n\stackrel{(-)}{+}x_i^h=b_i
	\end{equation}
	es equivalente, en el sentido de que hay una biyección ``natural'' entre los conjuntos de vectores \tb{no negativos} que verifican cada una de las restricciones.
	
	A la variable $x_i^h$ añadida en la restricción \eqref{fund_eq_rest2} se la denomina \tbi[variable!de holgura]{variable de holgura}.
\end{lem}
\begin{proof}
	Sean los conjuntos $\mc{S}$ y $\mc{S}'$ de vectores que verifican las restricciones \eqref{fund_eq_rest1} y \eqref{fund_eq_rest2} respectivamente. Consideremos la aplicación
	\begin{equation*}
		\begin{array}{cc}
		\varphi:&\mc{S}\to\mc{S}'\\
		& (x_1,\dots,x_n)\mapsto (x_1,\dots,x_n,z)
		\end{array}
	\end{equation*}
	donde $z=b_i-a_{i1}x_1+\dots+a_{in}x_n$. Veamos que es una biyección.
	
	La inyectividad es clara por definición, si dos vectores compartieran imagen todas sus componentes coinciden.
	
	En cuanto a la sobreyectividad, dado un vector $(x_1,\dots,x_n,x_{n+1})$ que verifica la restricción \eqref{fund_eq_rest2}, es claro que el vector resultante de eliminar la última componente, $(x_1,\dots,x_n)$, cumple la restricción \eqref{fund_eq_rest1}.
	
	La demostración para el caso $\geq$ es totalmente simétrica.
\end{proof}
Veamos que utilidad tiene lo que acabamos de demostrar.
\begin{obs}[Aplicación]
	Si a la hora de modelizar un problema nos aparecen restricciones de tipo $\geq$ o $\leq$ basta con sustituirlas por restricciones equivalentes añadiendo variables de holgura.
	
	Además, debemos cambiar la función objetivo $f$, extendiéndola de la siguiente manera
	\begin{equation*}
		\begin{array}{cc}
		\widehat{f}:&\R^{n+m}\to \R\\
		& (x_1,\dots,x_n,x_{n+1},\dots,x_{n+m})\mapsto f(x_1,\dots,x_n)
		\end{array}
	\end{equation*}
	donde $n$ es el número total de restricciones originales y $m$ el número de restricciones de tipo $\geq$ o $\leq$. De esta forma obtenemos un problema equivalente cuya solución será el vector solución del nuevo problema quitando las componentes correspondientes a variables de holgura (¡compruébese!).
\end{obs}
Con lo que sabemos hasta el momento podemos transformar cualquier problema escrito en cualquiera de las formas canónicas en un problema en forma estándar.
\subsubsection{Positividad y negatividad}
Otro caso que se presenta de manera usual es que al modelizar un problema concreto sea necesario permitir que algunas componentes de los vectores candidatos a ser solución sean negativas. Esta es una situación que no contempla la formulación estándar, sin embargo, con un simple truco podemos meter esta clase de problemas dentro de nuestro marco de actuación.
\begin{obs}[Truco]
	Si tenemos un problema en el que la componente $i$--ésima de los vectores puede ser negativa tomamos todas las restricciones
	\begin{equation*}
	a_{j1}x_1+\dots+a_{ji}x_i+\dots+a_{jn}x_n=b_j
	\end{equation*}
	que involucren a la componente $x_i$ y las sustituimos por las restricciones
	\begin{equation*}
	a_{j1}x_1+\dots+a_{ji}(x_i^+-x_i^-)+\dots+a_{jn}x_n=b_j
	\end{equation*}
	con $x_i^+$ y $x_i^-$ no negativos.
	
	Es claro que si un vector $(x_1,\dots,x_i,\dots,x_n)$ (donde $x_i$ es negativo) verifica las restricciones originales, entonces el vector $(x_1,\dots,0,-x_i,\dots,x_n)$ (por ejemplo) verifica las restricciones nuevas. Análogamente si $x_i$ es positivo. Asimismo, si el vector $(x_1,\dots,x_i^+,x_i^-,\dots,x_n)$ cumple las restricciones nuevas, el vector $(x_1,\dots,x_i^+-x_i^-,\dots,x_n)$ cumplirá las viejas, pudiendo concluir así que las restricciones viejas y nuevas son equivalentes en un sentido un poco más laxo que en el apartado anterior, ya que únicamente se tiene la sobreyectividad (pero esto es irrelevante).
	
	De esta forma, si cambiamos la función objetivo $f$ por la función
	\begin{equation*}
		\begin{array}{cc}
		\overline{f}:&\R^{n+1}\to \R\\
		& (x_1\dots,x_i^+,x_i^-,\dots,x_n)\mapsto f(x_1,\dots,x_i^+-x_i^-,\dots,x_n)
		\end{array}
	\end{equation*}
	obtenemos un problema equivalente cuya solución será el vector $(x_1,\dots,x_i^+-x_i^-,\dots,x_n)$ donde $(x_1\dots,x_i^+,x_i^-,\dots,x_n)$ es el vector solución del problema con las restricciones nuevas.
\end{obs}
Hechas todas estas disquisiciones iniciales cabe mencionar que a la hora de modelizar problemas, siempre que tanto la función objetivo como las restricciones sean lineales, podremos realizar transformaciones (las vistas en esta sección) para que el problema quede planteado en forma estándar, pudiendo ser así resuelto mediante el uso de toda la artillería teórica que veremos a continuación.
\subsection{Terminología y casuística}
Dado un problema de programación lineal en forma estándar que tiene a $A$ como matriz de coeficientes de las restricciones y a $b$ como vector de términos independientes. Exponemos las siguientes definiciones.
\begin{defi}[Soluciones]
	Llamamos \tbi{solución} del problema $P$ a cualquier vector $x\in\R^n$ (no necesariamente no negativo) que verifique $Ax=b$.
	
	Sacando un poco de punta al asunto, diremos que si $x\in\R^n$ es solución del problema y además $x\geq 0$ entonces $x$ es una \tbi[solución!factible]{solución factible}.
	
	Llamaremos \tbi[región!factible]{región factible} al conjunto de todas las soluciones factibles del problema. Normalmente la denotaremos con la letra $\mc{S}$.
\end{defi}
Una vez resuelto el problema se pueden dar las siguientes situaciones
\begin{itemize}
	\item La región factible posee al menos un punto en el cual la función objetivo alcanza el mínimo. En esta situación diremos que el problema posee \tbi[solución!óptima]{solución óptima}.
	\item La región factible del problema es el conjunto vacío. En este caso se dirá que el problema es \tbi[problema!infactible]{infactible}.
	\item La función objetivo alcanza valores tan bajos como queramos en la región factible. En este caso se dice que el problema posee una solución \tbi[solución!no acotada]{no acotada}.
\end{itemize}
Teniendo en cuenta estas pequeñas observaciones podemos empezar a estudiar cómo son las regiones factibles, lo cual nos será muy útil para resolver el problema de programación lineal.
\section{Programación lineal y conjuntos convexos}
En esta sección demostraremos que las regiones factibles de un problema de programación lineal en forma estándar son siempre conjuntos convexos. 

Asimismo, estudiaremos en profundidad las propiedades de estos conjuntos, explotándolas para obtener un procedimiento robusto con el cual resolver estos problemas.
\subsection{Conjuntos convexos. Definición y propiedades}
Comenzamos definiendo el concepto de conjunto convexo.
\begin{defi}[Convexos]
	Sea $\mc{S}$ un subconjunto de $\R^n$, se dice \tbi[conjunto!convexo]{convexo} si para cada par de puntos $x,y\in \mc{S}$ el segmento que tiene por extremos a $x$ e $y$ está contenido en $\mc{S}$. Es decir
	\begin{equation*}
		\lambda x + (1-\lambda)y\in\mc{S}
	\end{equation*}
	para todo $\lambda\in[0,1]$. Nótese que el conjunto vacío se considera convexo.
\end{defi}
La región factible de un problema de programación lineal en forma estándar es un conjunto convexo.
\begin{lem}[Región convexa]
	El conjunto de los vectores no negativos $x\in\R^n$ que verifican la condición $Ax=b$ para cierta matriz $A$ y cierto vector $b$ es un conjunto convexo.
\end{lem}
\begin{proof}
	Si la región factible $\mc{S}$ es el conjunto vacío ya hemos terminado. En otro caso, basta considerar dos vectores $x$ e $y$ de la región factible y ver que $\alpha x + (1-\alpha) y\in \mc{S}$ para todo $\alpha\in[0,1]$, pero esto es inmediato ya que
	\begin{equation*}
		A(\alpha x + (1-\alpha) y)=\alpha Ax + (1-\alpha)Ay=\alpha b + (1-\alpha)b = b \qedhere
	\end{equation*}
\end{proof}

Una cuestión importante a destacar es que la intersección arbitraria de conjuntos convexos es convexo, tal y como muestra el siguiente lema.
\begin{lem}[Intersección]
	Sea $\mc{F}$ una familia de conjuntos convexos. La intersección de la familia es un conjunto convexo.
\end{lem}
\begin{proof}
	Sean $x$ e $y$ dos puntos de la intersección de la familia, luego $x$ e $y$ están en todos los conjuntos de la familia. Como estos conjuntos son convexos, el segmento que une $x$ e $y$ estará contenido en todos los conjuntos de la familia simultáneamente, luego también en la intersección de la familia.
\end{proof}
Un concepto por el que merece la pena pasar es el de ``envoltura convexa'' de un conjunto $\mc{S}$ de $\R^n$, que viene a ser el menor conjunto convexo que contiene a $\mc{S}$.

Esta idea es especialmente recurrente en las matemáticas, por ejemplo, en álgebra lineal teníamos el subespacio generado por un subconjunto, en teoría de grupos el subgrupo generado por un subconjunto, en topología tenemos la adherencia de un conjunto,\dots La lista es interminable.
\begin{obs}[Justificación]
	Es fácil darse cuenta de que, como la intersección arbitraria de convexos es convexa, el menor conjunto convexo que contiene a uno dado (al que llamaremos $\mc{S}$) puede ser construido de la siguiente manera
	\begin{equation*}
	\text{Conv}(\mc{S}):=\bigcap_{\mc{F}\supset \mc{S}}\mc{F}
	\end{equation*}
	En efecto, es un conjunto que contiene a $\mc{S}$, ya que todos los conjuntos de la familia a intersecar contienen a $\mc{S}$, además, es el menor de ellos, ya que, de haber uno más pequeño, pertenecería a la familia que se está intersecando, lo cual es absurdo (¡compruébese!).
	
	De esta forma queda justificada la existencia del menor conjunto convexo que contiene a $\mc{S}$, no obstante, la construcción que hemos realizado nos da escasa información acerca de los elementos de la envoltura convexa.
\end{obs}
\begin{obs}[Curiosidad]
	A modo de curiosidad comentamos que el problema de calcular la envoltura convexa de un conjunto finito de $n$ puntos en el plano o el espacio es uno de los problemas centrales de la \tbi[geometría!computacional]{geometría computacional} por sus múltiples aplicaciones.
\end{obs}
Veamos a continuación una caracterización útil de la envoltura convexa de un conjunto, para la cual necesitamos introducir una definición.
\begin{defi}[Combinaciones convexas]
	Sea $\{z_1,\dots,z_r\}$ un conjunto finito de puntos. Diremos que $z$ es \tbi[combinación!lineal convexa]{combinación lineal convexa} de dichos puntos si
	\begin{equation*}
		z=\lambda_1 z_1+\dots+\lambda_r z_r
	\end{equation*}
	donde $\lambda_i>0$ para $i\in\{1,\dots,r\}$ y se cumple que $\sum_{i=1}^{r}\lambda_i=1$.
\end{defi}
Que no asuste la longitud de la demostración de la proposición pues realmente es muy sencilla.
\begin{prop}[Caracterización]
	Dado un conjunto $\mc{S}$, su envoltura convexa es el conjunto de puntos que son una combinación lineal convexa de puntos de $\mc{S}$.
\end{prop}
\begin{proof}
	Llamemos $\mc{H}(\mc{S})$ al conjunto de puntos que son combinación lineal convexa de puntos de $\mc{S}$. Veamos en primer lugar que $\mc{H}(\mc{S})$ contiene a $\mc{S}$. Esto es claro, ya que todo punto $x$ de $\mc{S}$ es combinación lineal convexa de puntos de $\mc{S}$, en efecto, $x=1\cdot x$.
	
	Veamos ahora que $\mc{H}(\mc{S})$ es convexo. Esto lo haremos simplemente usando la definición. Sean $x$ e $y$ dos puntos de $\mc{H}(\mc{S})$, veamos que $\alpha x + (1-\alpha)y\in \mc{H}(\mc{S})$ para todo $\alpha\in[0,1]$.
	
	Por definición de $\mc{H}(\mc{S})$
	\begin{equation*}
	\begin{array}{cc}
	x=\sum_{i=1}^{r}\lambda_ix_i \qquad&\qquad 
	y=\sum_{j=1}^{s}\mu_jy_j
	\end{array}
	\end{equation*}
	por tanto, si sustituimos esto en $\alpha x + (1-\alpha)y$ obtenemos
	\begin{equation*}
		\alpha x + (1-\alpha)y=\sum_{i=1}^{r}(\alpha\lambda_i)x_i+\sum_{j=1}^{s}[(1-\alpha)\mu_j]y_j
	\end{equation*}
	Haciendo los siguientes cambios de nombre
	\begin{equation*}
	\begin{array}{cc}
	\xi_k:=\begin{cases}
	\alpha\lambda_k & \text{si } k\in\{1,\dots,r\}\\
	(1-\alpha)\mu_{k-r} & \text{si } k\in\{r+1,\dots,r+s\}
	\end{cases}
	\qquad
	&
	\qquad
	z_k:=\begin{cases}
	x_k & \text{si } k\in\{1,\dots,r\}\\
	y_{k-r} & \text{si } k\in\{r+1,\dots,r+s\}
	\end{cases}
	\end{array}
	\end{equation*}
	nos queda que
	\begin{equation*}
		\alpha x + (1-\alpha)y=\sum_{k=1}^{r+s}\xi_kz_k
	\end{equation*}
	luego por definición de $\mc{H}(\mc{S})$ solo queda comprobar que $\sum_{k=1}^{r+s}\xi_k=1$, pero esto es evidente ya que
	\begin{equation*}
		\sum_{k=1}^{r+s}\xi_k=\alpha\sum_{i=1}^r\lambda_i+(1-\alpha)\sum_{j=1}^{s}\mu_j=\alpha+(1-\alpha)=1
	\end{equation*}
	Queda por ver pues que $\mc{H}(\mc{S})$ es el menor conjunto convexo que contiene a $\mc{S}$. Para ello, consideramos $\mc{C}$ un conjunto convexo arbitrario que contiene a $\mc{S}$. Demostremos que $\mc{H}(\mc{S})\subset C$ por inducción sobre la ``longitud'' de la combinación lineal convexa de sus elementos.
	
	El caso base es evidente, ya que los elementos de $\mc{H}(\mc{S})$ que son combinaciones lineales convexas de longitud unitaria son también elementos de $\mc{S}$ y por tanto de $\mc{C}$. Supongamos cierto que los elementos de $\mc{H}(\mc{S})$ que son combinaciones lineales convexas de longitud $n$ viven también en $\mc{C}$, y demostremos que lo mismo sucederá para los elementos que sean combinaciones de longitud $n+1$.
	
	Consideremos $z\in\mc{H}(\mc{S})$ un elemento que es combinación de longitud $n+1$, es decir
	\begin{equation*}
	\begin{array}{ccc}
		z=\sum_{i=1}^{n+1}\lambda_iz_i & \text{con} & \sum_{i=1}^{n+1}\lambda_i=1
	\end{array}
	\end{equation*}
	Vamos a apañar la expresión de $z$ para poder aplicar la hipótesis de inducción
	\begin{equation*}
		z=\sum_{i=1}^{n}\lambda_iz_i + \lambda_{n+1}z_{n+1}
	\end{equation*}
	Es claro que $z_{n+1}\in\mc{C}$ por ser una combinación de longitud uno. Sin embargo, el primer sumando no es una combinación lineal convexa ya que $\sum_{i=1}^{n}\lambda_i=1-\lambda_{n+1}\not=1$. Para arreglar esto vamos a multiplicar y a  dividir el primer sumando por $1-\lambda_{n+1}$
	\begin{equation*}
		z=(1-\lambda_{n+1})\sum_{i=1}^{n}\frac{\lambda_i}{1-\lambda_{n+1}}z_i+\lambda_{n+1}z_{n+1}
	\end{equation*}
	De esta forma el primer sumando (excluyendo el factor que lo multiplica) es una combinación lineal convexa de longitud $n$, ya que $\frac{1}{1\lambda_{n+1}}\sum_{i=1}^{n}\lambda_i=\frac{1-\lambda_{n+1}}{1-\lambda_{n+1}}=1$.
	
	Por hipótesis de inducción, tanto el primer sumando como el segundo (excluyendo los factores que los multiplican) son elementos de $\mc{C}$, y como $\mc{C}$ es convexo $z\in\mc{C}$, como queríamos.
\end{proof}
\subsection{Puntos extremos}
De ahora en adelante será de utilidad tener en mente un polígono regular cuando se nos hable de conjuntos convexos, pues nos dará una idea bastante intuitiva.

Para estrenar esta nueva buena costumbre se presenta la siguiente definición.
\begin{defi}[Punto extremo]
	Sea $\mc{C}$ un conjunto convexo. Diremos que $\overline{x}\in\mc{C}$ es un \tbi[punto!extremo]{punto extremo} de $\mc{C}$ si se verifica que para cualquier par de puntos $x_1,x_2\in\mc{C}$, $\overline{x}$ \tb{no} está en el segmento generado por dichos puntos (excluyendo los extremos). Es decir
	\begin{equation*}
		\overline{x}=\lambda x_1+(1-\lambda)x_2\sii\begin{cases}
		\lambda\in\{0,1\}\\
		x_1=x_2
		\end{cases}
	\end{equation*}
\end{defi}
La ilustración \ref{fund_img_puntosExtremos} marga en negro los puntos extremos de un polígono regular, para coger una idea intuitiva. A lo largo de este capítulo veremos la importancia que tienen estos puntos en la resolución del problema de programación lineal.
\begin{figure}[h!]
	\centering
	\label{fund_img_puntosExtremos}
	\includegraphics[scale = 0.75]{img/puntosExtremos}
	\caption{Ilustración de los puntos extremos de un heptágono regular.}
\end{figure}
El teorema \ref{fund_teo_caracterizacionExtremos} nos da una caracterización de los puntos extremos de una región factible que, aunque muy potente, resulta poco intuitiva. Antes de enunciarlo recordemos un viejo resultado de álgebra lineal.
\begin{obs}[Rango y submatrices]
	\label{fund_obs_rango}
	Es conocido desde tiempos inmemoriales que $A\in\mf{M}_{m\times n}(\K)$ con $m\leq n$ y $\mathrm{rg}(A)=m$ entonces habrá al menos una submatriz cuadrada $B\in\mf{M}_{m}(\K)$ de $A$ tal que $\mathrm{rg}(B)=m$. De esta forma, reordenando las columnas de $A$ si fuera necesario, podemos hacer la siguiente división por bloques de $A$.
	\begin{equation*}
		A=(B|N)
	\end{equation*}
	Nótese que la igualdad se da solo cuando ya hemos reordenado $A$.
\end{obs}
A partir de ahora trabajaremos con matrices en las condiciones de la observación \ref{fund_obs_rango} a no ser que se especifique lo contrario. Nótese que esto no supone ninguna pérdida de generalidad en lo que se refiere al estudio de los problemas de programación lineal ya que si se nos presentara una matriz con más filas que columnas habrá restricciones redundantes.
\begin{defi}[Bases]
	Dado un problema en forma estándar que tiene a $A$ por matriz de coeficientes de las restricciones, llamaremos \tbi{base} de $A$ a toda submatriz suya cuadrada y de rango máximo.
\end{defi}
\begin{theo}[Caracterización]
	\label{fund_teo_caracterizacionExtremos}
	Dada una región factible $\mc{S}$, $\overline{x}$ es un punto extremo de $\mc{S}$ si y solo si hay alguna reordenación de las columnas de $A$ de forma que
	\begin{equation*}
	\begin{array}{ccc}
	A=(B|N) \qquad&\qquad \overline{x}=\begin{pmatrix}
	\overline{x_B}\\
	\overline{x_N}
	\end{pmatrix}\text{ con } \overline{x_N}=0
	\end{array}
	\end{equation*}
	donde $B$ es una base de $A$ y $\overline{x_B}$ y $\overline{x_N}$ son los ``trozos'' de $\overline{x}$ acordes con la descomposición en bloques de $A$, es decir
	\begin{equation*}
		A\overline{x}=(B|N)\begin{pmatrix}
		\overline{x_B}\\
		\overline{x_N}
		\end{pmatrix}=B\overline{x_B}+N\overline{x_N}
	\end{equation*}
\end{theo}
\begin{proof}
	Veamos las dos implicaciones
	\begin{itemize}
		\item[\bla] Supongamos que $\overline{x}$ no es punto extremo, entonces habrá dos puntos distintos $x_1,x_2\in\mc{S}$ tales que $\overline{x}=\lambda x_1+(1-\lambda)x_2$ con $\lambda\in(0,1)$. Desplegando la descomposición por bloques tendríamos que
		\begin{equation*}
			(\overline{x_B}|\overline{x_N})^t=\lambda(x_B^1|x_N^1)^t+(1-\lambda)(x_B^2|x_N^2)^t
		\end{equation*}
		Como por hipótesis $\overline{x_N}=0$ y tanto $\lambda$ como $1-\lambda$ son estrictamente positivos se tiene que $x_N^1=x_N^2=0$, ya que $x_1$ y $x_2$ deben ser no negativos.
		
		Como $x_1,x_2\in\mc{S}$, entonces $Ax_i=b$ para $i\in\{1,2\}$, por tanto
		\begin{equation*}
			(B|N)(x_B^i|x_N^i)^t=(B|N)(x_B^i|0)^t=Bx_B^i=b\ra x_B^i=B^{-1}b
		\end{equation*}
		Con lo que se concluye que $x_1=x_2$, lo cual es absurdo.
		\item[\bra] Sea $\overline{x}$ un punto extremo. Reordenemos el vector de forma que todas sus componentes nulas queden al final, reordenando solidariamente las columnas de la matriz $A$.
		
		Podemos suponer pues que el vector $\overline{x}$ tiene $p$ componentes no nulas. Vamos a demostrar que las $p$ primeras columnas de $A$ (ya reordenada) son linealmente independientes. 
		
		A partir de ahora denotaremos a la matriz $A$ por sus columnas, es decir $A=(a_1\cdots a_n)$ donde $a_i$ representa la $i$--ésima columna de $A$.
		
		Procedamos por reducción al absurdo, es decir, supongamos que hay ciertos números reales no todos nulos $\lambda_i$ tales que $\sum_{i=1}^{p}\lambda_ia_i=0$.
		
		Definimos el vector $\lambda:=(\lambda_1,\dots,\lambda_p,0,\dots,0)\in\R^n$. A continuación consideramos los vectores \tb{no negativos} $x_1:=\overline{x}+\alpha\lambda$ y $x_2:=\overline{x}-\alpha\lambda$ tomando un $\alpha$ lo suficientemente pequeño para que ambos vectores sean precisamente no negativos, lo cual siempre puede hacerse al ser $\overline{x}$ no negativo.
		
		Los vectores $x_1$ y $x_2$ tienen la particularidad de que con ellos podemos obtener $\overline{x}$ como combinación lineal convexa. En efecto $\overline{x}=\frac{1}{2}x_1+\frac{1}{2}x_2$. Si resultara que $x_1$ y $x_2$ son puntos de la región factible estaríamos entrando en contradicción con la ``extremalidad'' de $\overline{x}$, y eso es exactamente lo que vamos a hacer. En efecto, con $i\in\{1,2\}$ tenemos
		\begin{equation*}
			Ax_i=A\overline{x}\pm \alpha A\lambda\stackrel{!}{=}A\overline{x}=b\ra x_i\in\mc{S}
		\end{equation*}
		donde la igualdad de la exclamación se debe a la composición de $\lambda$ y la dependencia lineal de las $p$ primeras columnas de $A$ (por hipótesis). Con esto llegamos a un absurdo y concluimos que las $p$ primeras columnas de $A$ son linealmente independientes.
		
		Además, de paso podemos concluir que $p\leq m$, ya que $m$ es el rango máximo de $A$. Si además $p=m$ ya tenemos una base $B=(a_1\cdots a_p)$ que cumple lo que queremos (¡compruébese!). En el caso de que $p<m$ la observación \ref{fund_obs_rango} nos dice que siempre podemos escoger columnas adicionales de $A$ para completar una base y, tras una nueva reordenación de $A$ si fuera necesario tendríamos lo que necesitamos.\qedhere
	\end{itemize}
\end{proof}
\begin{cor}[Parte básica]
	Dado un punto extremo $\overline{x}=(\overline{x_B}|0)^t$ de una región factible $\mc{S}$ se cumple que $\overline{x_B}=B^{-1}b$.
\end{cor}
\begin{proof}
	Basta con echar las cuentas. Como $\overline{x}\in\mc{S}$ es claro que $A\overline{x}=b$, luego
	\begin{equation*}
		A\overline{x}=(B|N)(\overline{x_B}|0)^t=B\overline{x_B}=b
	\end{equation*}
	y despejando se tiene que $\overline{x_B}=B^{-1}b$.
\end{proof}
Al hilo de este teorema surge la siguiente definición.
\begin{defi}[Soluciones básicas]
	A los puntos extremos de una región factible se les suele llamar \tbi[solución!básica factible]{soluciones básicas factibles}. Esto es debido a que, por el teorema \ref{fund_teo_caracterizacionExtremos}, están ``asociados'' a una base de la matriz asociada a la región factible del problema.
\end{defi}
\begin{obs}[Procedimiento constructivo]
	Nótese que si nos dan un punto extremo de una región factible y nos piden hallar la base asociada a dicho punto extremo no tenemos más que seguir la demostración de la implicación a la derecha del teorema \ref{fund_teo_caracterizacionExtremos} para encontrarla.
\end{obs}
Una de las razones por la cual es estudio de los puntos extremos es importante es porque toda región factible de un problema de programación lineal en forma estándar tiene al menos un punto extremo.
\begin{theo}[Existencia]
	Toda región factible no vacía posee al menos un punto extremo.
\end{theo}
\begin{theo}[Optimalidad]
	Si un problema de programación lineal en forma estándar tiene solución óptima, esta se alcanza en un punto extremo.
\end{theo}
\subsection{Direcciones extremas}
\section{Teoremas fundamentales}